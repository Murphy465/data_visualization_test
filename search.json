[
  {
    "objectID": "icite.html",
    "href": "icite.html",
    "title": "Lesson 4. iCite",
    "section": "",
    "text": "The Relative Citation Ratio (RCR) evaluates the influence of a scientific article by analyzing its co-citation network. An RCR of 1.0 represents the median citation rate for articles in a given field. Values above 1.0 indicate that an article is cited more frequently than average, suggesting greater impact or influence. In this lesson, we‚Äôll use the NIH Reporter to identify an active project at The Ohio State University with associated publications. We will then retrieve the RCR values for the publications using the iCite API.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 4. iCite"
    ]
  },
  {
    "objectID": "icite.html#data-skills-concepts",
    "href": "icite.html#data-skills-concepts",
    "title": "Lesson 4. iCite",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nWorking with APIs",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 4. iCite"
    ]
  },
  {
    "objectID": "icite.html#learning-objectives",
    "href": "icite.html#learning-objectives",
    "title": "Lesson 4. iCite",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nLocate API documentation and identify key components required to formulate an API request\nParse the API response and store extracted data.\n\nThis tutorial is designed to support multi-session workshops hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts visit the Python - Mastering the Basics tutorial.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 4. iCite"
    ]
  },
  {
    "objectID": "icite.html#step-1-locate-an-active-nih-project",
    "href": "icite.html#step-1-locate-an-active-nih-project",
    "title": "Lesson 4. iCite",
    "section": "Step 1: Locate an active NIH project",
    "text": "Step 1: Locate an active NIH project\n\nGo to the NIH Reporter.\nUnder Fiscal Year, select Active Projects.\nUnder Organization, search for and select Ohio State University.\n\n\n\n\nnih_reporter_1_search.png",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 4. iCite"
    ]
  },
  {
    "objectID": "icite.html#step-2-browse-active-projects",
    "href": "icite.html#step-2-browse-active-projects",
    "title": "Lesson 4. iCite",
    "section": "Step 2: Browse active projects",
    "text": "Step 2: Browse active projects\n\nAs of May 6, 2025 there were 687 active projects at The Ohio State University.\nBrowse the list and click on a project that interests you.\n\n\n\n\nnih_reporter_2_select_project.png",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 4. iCite"
    ]
  },
  {
    "objectID": "icite.html#step-3-review-project-details",
    "href": "icite.html#step-3-review-project-details",
    "title": "Lesson 4. iCite",
    "section": "Step 3: Review project details",
    "text": "Step 3: Review project details\n\nRead the project abstract to understand its focus.\nTo check for related publications:\n\nClick on Publications in the left-hand menu, or\nScroll to the bottom of the project page.\n\n\n\n\n\nnih_reporter_3_select_publications.png",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 4. iCite"
    ]
  },
  {
    "objectID": "icite.html#step-4-export-the-publications",
    "href": "icite.html#step-4-export-the-publications",
    "title": "Lesson 4. iCite",
    "section": "Step 4: Export the publications",
    "text": "Step 4: Export the publications\n\nIf the publications are listed, use the Export option to download the list for further analysis.\n\n\n\n\nnih_reporter_4_export_publications.png\n\n\n\n\n\n\nExercise 1: Read .csv\n\n\n\n\nUse Pandas to read the exported publication file into Python.\n\n\nTip\n\n\nMove the file from downloads to your project folder.\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport pandas as pd\ndf=pd.read_csv('data/Publications_10875609_6May2025_171210.csv') #Change the path to your downloaded file\n\n\n\n\n\n\n\n\nExercise 2: List PMIDs\n\n\n\n\n\nEach article indexed in PubMed has a unique PMID (PubMed Identifier). The exported file includes a PMID column. Create a list of PMIDs found in the PMID column. To prepare the data:\n\n\n\nRemove any missing (null) PMIDs\n\n\nConvert each PMID from a float to integer, then to string.\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport pandas as pd\ndf=pd.read_csv('data/Publications_10875609_6May2025_171210.csv') \npmid_list=df.PMID.dropna().tolist()\npmids=[]\nfor pmid in pmid_list:\n  pmids.append(str(int(pmid)))\npmids\n\n\n\n\n\n\n\n\nExercise 3: iCite API\n\n\n\n\n\nLaunch iCite.\n\n\nScroll to the bottom and and review:\n\n\n\nWeb Policies and Notices\n\n\nBulk Data and APIs\n\n\n\nOn the Bulk Data and APIs page, locate the Python code example for querying the iCite API.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport requests\n\n\nresponse = requests.get(\n    \"/\".join([\n        \"https://icite.od.nih.gov/api\",\n        \"pubs\",\n        \"23456789\",\n    ]),\n)\npub = response.json()\nprint(pub)\n\n\n\n\n\n\n\n\nExercise 4: Adjust code\n\n\n\n\n\nAdjust the Python code example to extract the following fields for each PMID.\n\n\n\npmid\n\n\ndoi\n\n\njournal\n\n\nrelative citation ratio\n\n\ncited by\n\n\nreferences\n\n\ntitle\n\n\nyear\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport pandas as pd\nimport requests\n\nresults=pd.DataFrame(columns=['pmid','doi','journal','rcr','cited_by','references','title','year'])\n\ndf=pd.read_csv('Publications_10875609_6May2025_171210.csv') \npmid_list=df.PMID.dropna().tolist()\npmids=[]\nfor pmid in pmid_list:\n  pmids.append(str(int(pmid)))\n\ncount=1\nfor each_pmid in pmids:\n    print(f\"Starting record {count}: {each_pmid}\")\n    response = requests.get(\n        \"/\".join([\n        \"https://icite.od.nih.gov/api\",\n        \"pubs\",\n        each_pmid,\n        ]),\n        )   \n    pub = response.json()\n    citing_pmids=pub['cited_by']\n    cited_by=[]\n    for each_citation in citing_pmids:\n        cited_by.append(str(each_citation))\n    reference_pmids=pub['references']\n    references=[]\n    for each_reference in references:\n        references.append(str(each_reference))\n\n    row = {\n            'pmid': pub['pmid'],\n            'doi': pub['doi'],\n            'journal': pub['journal'],\n            'rcr': pub['relative_citation_ratio'],\n            'cited_by': ','.join(cited_by),\n            'references': ','.join(references),\n            'title': pub['title'],\n            'year': pub['year']\n            }\n    each_row=pd.DataFrame(row, index=[0])\n    results=pd.concat([each_row, results], axis=0, ignore_index=True)\n    count += 1",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 4. iCite"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html",
    "href": "tableau_research_choose_effective_visual.html",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "",
    "text": "Choosing the right chart isn‚Äôt always straightforward ‚Äî it‚Äôs often an iterative and context-dependent process. Effective data visualization requires you to consider both the nature of the data and the needs of your audience. This lesson introduces how to create basic charts in Tableau.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#data-skills-concepts",
    "href": "tableau_research_choose_effective_visual.html#data-skills-concepts",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nTableau\nWorking with data\nAnalyzing data",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#learning-objectives",
    "href": "tableau_research_choose_effective_visual.html#learning-objectives",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nSelect the most effective chart type for your audience.\nDistinguish between discrete and continuous dates and apply this when building line graphs in Tableau.\nVisualize relationships between two variables using scatterplots.\nCreate alternatives to crosstabs or text tables for displaying numeric data.\nDiscuss the strengths and limitations of data storytelling.\n\nThis tutorial is designed to support a multi-session Tableau for Research workshop hosted by The Ohio State University Libraries Research Commons. It is intended to help the ABSOLUTE beginner, or anyone who is relatively new to Tableau to build the skills and confidence to apply Tableau to research projects.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#relate-or-join-data-sources",
    "href": "tableau_research_choose_effective_visual.html#relate-or-join-data-sources",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Relate or join data sources",
    "text": "Relate or join data sources\nRelationships combine data from multiple tables in a data source based on common fields ‚Äî without merging them into a single flat table. This approach offers several benefits, including preserving table-level detail and helping to avoid issues like data duplication or incorrect aggregations.\nTo practice creating relationships and to better understand how they affect the level of detail in your visualizations, consider downloading Tableau‚Äôs Bookshop dataset.\nJoins in Tableau physically merge tables into a single flat table at the data source level, based on a defined join condition (e.g., inner, left, right, or full outer). While useful when both tables share the same level of detail, joins can lead to duplicated rows or inflated values if the join keys aren‚Äôt clean or consistent, so careful data preparation is essential.\n\n\nTip - Copilot\n\n\n\nFor a more in-depth exploration of these concepts, ask Copilot to explain the difference between a relationship and a join in Tableau. Copilot will provide a clear breakdown, along with guidance on when to use a relationship versus a join, depending on your data structure and analysis goals.\n\n\n\n\nLet‚Äôs enhance the rock_n_roll_performers.csv dataset by relating it to rock_n_roll_studio_albums.csv.\n\nGo to the Data Source page.\nIn the Connections pane, locate rock_n_roll_studio_albums.csv.1\nDrag and drop the table onto the canvas.\nIn the relationship settings, set the relationship as:\nArtist = Artist1.\n\n\nVideo showing steps 1-4 above",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#ranking",
    "href": "tableau_research_choose_effective_visual.html#ranking",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Ranking",
    "text": "Ranking\nRanking shows an element‚Äôs position in an ordered list. Horizontal and vertical bar charts are especially effective for visualizing rank-based comparisons, making it easy to see which values stand out.\n\n\nTip: One Task, Multiple Approaches\n\n\n\n\nTableau loves options‚Äîthere‚Äôs almost always more than one way to do the same thing!!!\n\n\n\n\nüìä Bar Chart\nLet‚Äôs build a bar chart that highlights the average U.S. peak chart positions for albums by 2025 Rock & Roll Hall of Fame inductees.\n\nIn the Data Pane scroll down to find the rock_n_roll_studio_albums.csv table .\nDouble click Peak US to instantly generate a bar chart showing the total sum of US peak chart positions.\n\nNow let‚Äôs refine the chart to focus on specific artists and show average values instead:\n\nScroll up in the Data Pane to find the rock_n_roll_performers.csv table.\nDrag Artist to the Filters Shelf.\nIn the filter dialog, select the following 2025 inductees:\n\nBad Company\nChubby Checker\nJoe Cocker\nCyndi Lauper\nOutkast\nSoundgarden\nThe White Stripes\n\nDrag Artist to the Columns Shelf.\nOn the Rows Shelf, click the ‚ñº caret on SUM(Peak US) and change the aggregation form SUM to AVERAGE.\nRight-click the sheet tab and rename it BarChart1.\n\n\nVideo showing the steps 1-6 below\n\n\n\n\n\nTip: Worksheet Names File\n\n\nAlways give your worksheets clear, meaningful names‚Äîthis makes it much easier to organize and navigate your dashboards or stories later on.\n\n\n\n\nUnderstanding Tableau pills\nIn Tableau, dimensions appear as blue pills and measures as green pills. Adding a dimension (like Artist) disaggregates the data in your view. Many Tableau features‚Äîsuch as formatting, sorting, or converting data types‚Äîare accessed by clicking the ‚ñº caret on a pill.\n\n\n\n\n\nExercise 1: Bar Chart\n\n\n\n\n\nBuild a bar chart that displays the peak U.S. chart positions for each album released by your favorite artist inducted into the Rock and Roll Hall of Fame.\n\n\n\nOpen a new worksheet\n\nClick the worksheet icon next your Barchart1 tab in the lower-left corner of the Tableau workspace.\n\n\n\nRename the sheet:\n\nRight-click on Sheet2 and rename it BarChart2.\n\n\n\nBuild your bar chart.\n\nUse the album_title dimension and the Peak US measure to create a bar chart showing chart positions for each album.\n\n\n\nExperiment with layout:\n\nClick the Swap button in the toolbar to switch the orientation of the bars (horizontal ‚ÜîÔ∏é vertical).\n\n\n\nSort the bars:\n\nUse the Sort ascending button in the toolbar to arrange bars from lowest to highest chart position.\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n\n\n\n\n\nShow Me\nAnother way to create a bar chart in Tableau is by using the Show Me  button located in the upper right corner of the workspace. Before clicking Show Me, hold down the Ctrl key (or Option key on a Mac) while selecting one or more dimensions and/or measures. Tableau will then highlight the chart types that best match your selection.\nIf you pre-select artist, album_title, and Peak US, for example, Tableau will automatically suggests horizontal bar charts as a suitable visualization.\n\n\n\n\n\nExercise 2. Bar Chart\n\n\n\n\n\n\nOpen a new worksheet and rename the sheet BarChart3.\n\n\nBuild the initial view:\n\n\nDrag Release date to the Columns shelf.\n\n\nDrag rock_n_roll_studio_albums.csv (Count) to the Rows shelf.\n\nNote: Tableau automatically creates a line chart, because Release date is recognized as a date field.\n\n\n\n\n\nChange the chart type:\n\n\nOn the Marks Card, click the dropdown next to Automatic and select Bar.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#change-over-time",
    "href": "tableau_research_choose_effective_visual.html#change-over-time",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Change over time",
    "text": "Change over time\n\nüìà Line chart\nLine charts are ideal for showing trends over time. When you place Release date on the Columns chelf and rock_n_roll_studio_albums.csv (Count) on the Rows shelf, Tableau automatically generates a line chart‚Äîbecause it recognizes Release date as a date field.\nTableau classifies fields as either discrete or continuous, and this classification affects both the type of chart Tableau creates and how you interact with your data. You can identify the classification by the pill color:\n\nBlue = Discrete\nGreen = Continuous\n\nThis distinction is especially important when working with dates, as they can be treated either way.\n\nüìÖ Discrete dates\nLet‚Äôs walk through an example using discrete dates:\n\nCreate a new worksheet and rename it LineChart1.\nDrag Release date to Columns and rock_n_roll_studio_albums.csv (Count) to Rows.\n\nNotice that Release date appears as a blue pill, indicating it‚Äôs being treated as discrete.\nThe pill is labeled YEAR(Release date) and is prefixed with a +, meaning it can be expanded.\n\nExpand the date hierarchy:\n\nClick the + next to YEAR to add QUARTER.\nClick the + again to add MONTH.\nTableau automatically parses discrete dates into parts: year, quarter, month, day.\n\nClean up the view:\n\nRemove YEAR, QUARTER, and DAY from the Columns shelf by dragging them off.\n\nAdd a filter for specific years:\n\nRight-click and drag Release date to the Filters Shelf.\nIn the Filter Field window, select YEARS and click Next.\nChoose 1960-1964, then click OK.\nAgain, Tableau uses blue and green to indicate discrete versus continuous dates.\n\nAdd color by year:\n\nHold Ctrl (or Option on Mac) and drag YEAR from the Filters shelf to the Color shelf on the Marks card.\n\n\n\nNote:The bulk of albums released in January is misleading. When gathering this data, if only a year was provided for release data, the month and day of the release date was automatically set to January 1.`\n\n\nüìÖ Continuous dates\nNow let‚Äôs see how the view changes when Release date is converted from a discrete to a continuous date.\n\nDuplicate the existing sheet:\n\nRight-click on LineChart1 and select Duplicate.\nRename the new sheet LineChart2.\n\nConvert the date field:\n\nClick the ‚ñº caret on the MONTH(Release date) pill.\nChange the date from the discrete format (e.g.¬†Month (May)) to the continuous format (e.g.¬†Month (May 2015)).\n\n\nAt first glance, the chart may look broken or disjointed. However, if you hover over each line, you‚Äôll see that Tableau is correctly plotting each month across all years on a continuous timeline. The visual choppiness due to the color still being based on the discrete year.\nTo smooth out the visualization and better highlight trends:\n\nRemove the YEAR(Release date) pill from Color.\nAdd color based on values:\n\nDrag a copy of rock_n_roll_studio_albums.csv (Count) to Color shelf on the Marks card.\n\n\nThis change applies a diverging color palette based on the count values, helping to highlight highs and lows in the data. The result is a smoother, more insightful trend chart that emphasizes variation over time.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#correlation",
    "href": "tableau_research_choose_effective_visual.html#correlation",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Correlation",
    "text": "Correlation\n\n‚ñë Scatterplot\nTableau automatically displays data at the highest level-of-detail available when creating a view. This behavior is especially noticeable when using scatterplots to analyze relationships between two or more measures.\n\nTo break down or disaggregate the data point, add a lower level of detail to the view.\n\n\n\n\nExercise 3. Scatterplots\n\n\n\n\n\nCreate a scatterplot to compare the Peak US and Peak UK chart positions for albums released by your favorite artist inducted into the Rock and Roll Hall of Fame.\n\n\n\nOpen a new worksheet and rename it Scatterplot1.\n\n\nBuild the scatterplot:\n\n\nDrag Peak US to the Columns shelf.\n\n\nDrag Peak UK to the Rows shelf.\n\n\n\n\nFilter by artist:\n\n\nDrag Artist to the Filters shelf and select your favorite artist.\n\n\n\n\nAdd detail:\n\n\nDrag album_title to Detail on the Marks card.\n\n\n\n\n\n\n\n\nBy adding album_title to Detail, you increase the granularity of the view. Now, each mark (circle) represents a unique album, plotted by its Peak US and Peak UK chart positions. This allows for more detailed and meaningful comparison across individual albums.\n\n\nTip: Outliers\n\n\n\nOutliers can significantly distort the results of a scatterplot, so it‚Äôs important to handle them thoughtfully. Tableau makes it easy to exclude outlier data points: simply click on a mark to open the tooltip and select üõá Exclude, or right-click the mark and choose X Exclude.\n\n\nHowever, always be sure to document your approach to handling outliers. Transparency in your data-cleaning decisions is essential for maintaining the integrity and reproducibility of your analysis.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#formatting",
    "href": "tableau_research_choose_effective_visual.html#formatting",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Formatting",
    "text": "Formatting\nEnhance the clarity and visual appeal of your scatterplot using the Marks card:\n\nAdd a dimension to Color (e.g., album_title or release year) to visually group or differentiate data points.\nChange the mark type from Automatic to Circle for a cleaner, more focused design.\nApply a dark gray border and reduce opacity to 60% for a layered, professional appearance.\nIncrease the size of the circles to improve visibility and emphasis.\nHide null indicators to remove albums with missing US or UK chart data.\n\nThese small adjustments can significantly improve the clarity and aesthetic appeal of your visualization.\n\nVideo showing scatter plots steps 1-4 and formatting tips",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#text-table",
    "href": "tableau_research_choose_effective_visual.html#text-table",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Text table",
    "text": "Text table\nA text table is not a data visualization. When used appropriately, however, a text table can complement visual elements by providing precise values, detailed comparisons, or supporting context that enhances the overall understanding of your data.\nLet‚Äôs walk through how to create a text table that displays the the peak chart positions in every country for each album released by The Beatles.\n\nStart a new worksheet and rename it TextTable.\nFilter the data:\n\nDrag Artist to the Filters shelf and select The Beatles.\n\nSelect the relevant fields:\n\nClick on album_title\nHold Ctrl (or Cmd on Mac) and click Peak AUS\nThen hold Shift and click Peak US to select all measures between them. \n\n\nOpen Show Me and select Text Tables.\nIn the Toolbar, change the view from Standard to Entire View .\nAdjust the width of the album_title column for better readability.\n\n\nWhat Tableau does behind the scenes\n\nA blue Measure Names pill now appears on the Columns shelf.\nA green Measure Values pill is placed on Label on the Marks Card.\nEach selected measure now appears on a separate Measure Values card under the Marks Card.\n\n\n\nCustomizing the table\n\nReorder columns by dragging measures on the Measures Values card.\nRemove a measure by dragging it off the Measure Values card.\n\n\n\nTip - Set Default Properties\n\n\n\nTo save time and ensure consistency, try setting Default Properties for dimensions and measures before starting a Tableau project.\n\n\n\n\nRight-click on each measure that begins with ‚ÄúPeak‚Äù.\n\n\nScroll to Default Properties, and select Number Format.\n\n\nChoose Number Standard from the formatting options.\n\n\nThis ensures that all selected measures display in a consistent numeric format throughout your workbook, reducing the need for repetitive formatting later on.\n\n\n\n\n\n\nCallout numbers\nSometimes one large, single number can make the most powerful statement in a data visualization. Callout numbers are used to highlight key takeaways and can be easily created in Tableau with a few formatting steps.\nLet‚Äôs create a callout number to emphasize the Peak US chart position for The Beatles‚Äô album Abbey Road.\nTo emphasize the Peak US chart position for The Beatles album Abbey Road:\n\nDuplicate the existing sheet:\n\nRight-click the TextTable sheet tab and select Duplicate.\nRename the new sheet Callout\n\nSimplify the view:\n\nRemove all Measure Values starting with ‚ÄúPeak‚Äù except SUM(Peak US).\n\nFilter to a single album:\n\nIn the view, right-click Abbey Road and choose ‚úî Keep only.\n\nFormat the number:\n\nRight-click on the number (e.g., 2) and select Format to open the Formatting Pane.\nFrom the Fields ‚ñº dropdown in the Formatting Pane, select SUM(Peak US).\nUnder Font, increase the font size from 9 to 72.\nUnder Alignment, set the horizontal and vertical alignment to Center.\n\nClean up the view:\n\nDrag album_title from the Rows shelf to Detail on the Marks card.\n\nAdd a caption:\n\nRight-click in the gray space under the Marks card and select Caption.\nDouble-click the Caption area to edit it.\nDelete defaulat text and type: Peak US chart position.\nHighlight the text and select center in the edit caption window.\n\n\nWith just a few additional formatting tweaks, you‚Äôll have a clean, impactful callout number ready to be featured in your Tableau dashboard.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#overview",
    "href": "tableau_research_choose_effective_visual.html#overview",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nBETTER DATA VISUALIZATIONS\n\n\n\n\nBetter Data Visualizations: A Guide for Scholars, Researchers, and Wonks\n\n\n\n\nby Jonathan Schwabish\n\n\nNew York : Columbia University Press, 2021.\n\n\n\n\n\n\n\nSTORYTELLING WITH DATA\n\n\n\n\nStorytelling with Data: A Data Visualization Guide for Business Professionals\n\n\n\n\nby Cole Nussbaumer Knaflic\n\n\nHoboken, New Jersey: Wiley, 2015.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "tableau_research_choose_effective_visual.html#footnotes",
    "href": "tableau_research_choose_effective_visual.html#footnotes",
    "title": "Lesson 2. Choose an Effective Visual",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWant to learn how to gather discography tables from Wikipedia pages? Check out Lesson 3 of the Research Commons‚Äô Python tutorials on Websites and APIs.`‚Ü©Ô∏é",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 2. Choose an Effective Visual"
    ]
  },
  {
    "objectID": "mini.html",
    "href": "mini.html",
    "title": "Mini Fence Test",
    "section": "",
    "text": "BETTER DATA VISUALIZATIONS\n\n\n\n\n\nby Jonathan SchwabishNew York : Columbia University Press, 2021.\n\n\n\n\n\n\n\nEFFECTIVE DATA VISUALIZATION\n\n\n\n\n\nby Stephanie EvergreenThousand Oaks, California: SAGE Publications, 2020\n\n\n\n\n\n\n\nSTORYTELLING WITH DATA\n\n\n\n\n\nby Cole Nussbaumer KnaflicHoboken, New Jersey: Wiley, 2015.\n\n\n\n\n\n\n\nDATA STORY\n\n\n\n\n\nby Nancy DuarteOakton, Virginia: Ideapress Publishing, 2019."
  },
  {
    "objectID": "mini.html#overview",
    "href": "mini.html#overview",
    "title": "Mini Fence Test",
    "section": "",
    "text": "BETTER DATA VISUALIZATIONS\n\n\n\n\n\nby Jonathan SchwabishNew York : Columbia University Press, 2021.\n\n\n\n\n\n\n\nEFFECTIVE DATA VISUALIZATION\n\n\n\n\n\nby Stephanie EvergreenThousand Oaks, California: SAGE Publications, 2020\n\n\n\n\n\n\n\nSTORYTELLING WITH DATA\n\n\n\n\n\nby Cole Nussbaumer KnaflicHoboken, New Jersey: Wiley, 2015.\n\n\n\n\n\n\n\nDATA STORY\n\n\n\n\n\nby Nancy DuarteOakton, Virginia: Ideapress Publishing, 2019."
  },
  {
    "objectID": "mini.html#inspiration",
    "href": "mini.html#inspiration",
    "title": "Mini Fence Test",
    "section": "Inspiration",
    "text": "Inspiration\n\n\n\n\nBOOK OF CIRCLES\n\n\n\n\nThe Book of Circles: Visualizing Spheres of Knowledge\n\n\nby Manuel LimaNew York, New York: Princeton Architectural Press, 2017.\n\n\n\n\n\n\n\nBOOK OF TREES\n\n\n\n\nThe Book of Trees: Visualizing Branches of Knowledge\n\n\nby Manuel LimaNew York, New York: Princeton Architectural Press, 2014.\n\n\n\n\n\n\n\nBIG BOOK of DASHBOARDS\n\n\n\n\nThe Big Book of Dashboards: Visualizing Your Data Using Real-World Business Scenarios\n\n\nby Steve Wexler, Jeffrey Shaffer, Andy CotgreaveHoboken, New Jersey : Wiley, 2017."
  },
  {
    "objectID": "mini.html#theory",
    "href": "mini.html#theory",
    "title": "Mini Fence Test",
    "section": "Theory",
    "text": "Theory\n\n\n\n\nVISUAL DISPLAY OF QUANTITATIVE INFORMATION\n\n\n\n\nThe Visual Display of Quantitative Information\n\n\nby Edward R. TufteCheshire, Conn. : Graphic Press, 1983.\n\n\n\n\n\n\n\nBEAUTIFUL EVIDENCE\n\n\n\n\nBeautiful Evidence\n\n\nby Edward R. TufteCheshire, Conn. : Graphic Press, 2006."
  },
  {
    "objectID": "mini.html#practice",
    "href": "mini.html#practice",
    "title": "Mini Fence Test",
    "section": "Practice",
    "text": "Practice\n\n\n\n\nSTORYTELLING WITH DATA: LET‚ÄôS PRACTICE!\n\n\n\n\nStorytelling With Data: Let‚Äôs Practice!\n\n\nby Cole Nussbaumer KnaflicHoboken, New Jersey : Wiley, 2020.\n\n\n\n\n\n\n\nDATA VISUALIZATION SKETCHGBOOK\n\n\n\n\nThe Data Visualization Sketch Book\n\n\nby Stephanie D. H. EvergreenThousand Oaks : SAGE Publications, 2019."
  },
  {
    "objectID": "excel_old.html",
    "href": "excel_old.html",
    "title": "‚Ä¶ Excel",
    "section": "",
    "text": "Don‚Äôt discount Excel! For datasets under 1,000 rows, Excel can be an ideal tool for creating clear and effective visualizations. The resources below offer practical guidance and tips for making the most of Excel‚Äôs powerful charting and data presentation features.\n\nBooks\n\n\n\nDATA VISUALIZATION IN EXCEL\n\n\n\n\n\n\n\nJonathan Schwabish shows Excel remains a powerful tool for visualizing data in Data Visualization in Excel: A Guide for Scholars, Researchers, and Wonks. Each chapter provides detailed instructions for creating nearly 30 additional graphs beyond Excel‚Äôs default graph library. From the basic heatmaps to advanced Marimekko charts, this book will guide you in crafting more colorful, effective data visualizations for your audience.\n\n\n\n\nEFFECTIVE DATA VISUALIZATION\n\n\n\n\n\n\n\nStephanie Evergreen‚Äôs Effective Data Visualization: the Right Chart for the Right Data provides detailed instructions for constructing and then formatting Excel graphs that inspire conversations and support decision-making. A fun ninja rating scale helps you to decide how challenging it may be to create each graph and is embedded in the side-margins of each chapter. Further, an entire chapter offers ideas for visualizing qualitative and quantitative data.\nWrite MyST Markdown to create enriched documents with publication-quality features.\n\n\n\n\nSTORYTELLING WITH DATA\n\n\n\n\n\n\n\nStephanie Evergreen‚Äôs Effective Data Visualization: the Right Chart for the Right Data provides detailed instructions for constructing and then formatting Excel graphs that inspire conversations and support decision-making. A fun ninja rating scale helps you to decide how challenging it may be to create each graph and is embedded in the side-margins of each chapter. Further, an entire chapter offers ideas for visualizing qualitative and quantitative data.\nWrite MyST Markdown to create enriched documents with publication-quality features.\n\n\n\n\n\nO‚ÄôReilly Online Learning\nFor additional books and learning materials, the O‚ÄôReilly Online Learning: Academic/Public Library Edition collection provides extensive access to eBooks and videos in computer science, IT, business, and related subjects, featuring content from O‚ÄôReilly and other top publishers. This resource is provided by University Libraries and is available to all Ohio State faculty, students, and staff with a valid osu.edu email address."
  },
  {
    "objectID": "tableau_communicate.html",
    "href": "tableau_communicate.html",
    "title": "Lesson 4. Communicate Data",
    "section": "",
    "text": "Lesson 3 introduced the use of groups to modify data categories and hierarchies to organize data and enhance interactivity in visualizations. The readings explored the advantages and limitations of pie charts for illustrating part-to-whole relationships and suggested alternative visualization methods. Formatting techniques and table calculations were also covered to improve visual impact and optimize Tableau performance by limiting the scope of computation to what‚Äôs displayed in the view. Lesson 4 introduces calculated fields, parameters, maps, and dashboard design.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#data-skills-concepts",
    "href": "tableau_communicate.html#data-skills-concepts",
    "title": "Lesson 4. Communicate Data",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nTableau\nAnalyzing data\nArguing with Data",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#learning-objectives",
    "href": "tableau_communicate.html#learning-objectives",
    "title": "Lesson 4. Communicate Data",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nCreate calculated fields to clean data, assign values, add interactivity, and perform statistical analysis.\nUse parameters to replace constants, highlight data dynamically, apply filters, and more.\nBuild symbol maps and choropleth maps for geographic data visualization.\nDesign and assemble a dashboard.\n\nThis tutorial is designed to support a multi-session Tableau for Research workshop hosted by The Ohio State University Libraries Research Commons. It is intended to help the ABSOLUTE beginner, or anyone who is relatively new to Tableau to build the skills and confidence to apply Tableau to research projects.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#ethical-use-of-data",
    "href": "tableau_communicate.html#ethical-use-of-data",
    "title": "Lesson 4. Communicate Data",
    "section": "Ethical use of data",
    "text": "Ethical use of data\nHandling data responsibly‚Äîespecially when it includes personally identifiable information‚Äîrequires thoughtful ethical practices. From how data is collected and stored to how it‚Äôs shared and used, each step involves important considerations. To explore these principles further, check out this informative video from Virginia Tech University Libraries.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#data-citation",
    "href": "tableau_communicate.html#data-citation",
    "title": "Lesson 4. Communicate Data",
    "section": "Data Citation",
    "text": "Data Citation\nUsing data ethically means giving proper credit to the people or organizations that created it. Just like citing books or articles, citing data acknowledges the original source and supports transparency and reproducibility. Want to learn more? Check out ICPSR‚Äôs fun video on why citing data matters!",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#dual-axis-combination-chart",
    "href": "tableau_communicate.html#dual-axis-combination-chart",
    "title": "Lesson 4. Communicate Data",
    "section": "Dual-axis combination chart",
    "text": "Dual-axis combination chart\nA dual-axis combination chart lets you display two different data fields on the same graph‚Äîeither to save space or to highlight comparisons more effectively. You can also enhance your visual analysis by layering multiple mark types (like bars and lines) in a single view.\nLet‚Äôs dive back into our rock_n_roll_performers dataset to create a dual-axis chart that highlights our favorite albums and their peak U.S. chart positions.\n\nStart a new worksheet and rename it DualAxis\nFilter for favorite albums\n\nDrag Favorite albums group to the Filters shelf.\nSelect Favorite albums only.\n\nBuild the initial bar chart\n\nDrag album_title to Columns.\nDrag Peak US to Rows.\n\nCreate the dual axis\n\nHold Ctrl (or Option on Mac) and drag SUM(Peak US) to the right on the Rows shelf to duplicate it.\nYou‚Äôll now see three Marks Cards:\n\nAll\nSUM(Peak US)\nSUM(Peak US) (2)\n\nClick the ‚ñº caret on the second SUM(Peak US) pill and select Dual Axis.\n\nFormat as a lollipop chart\n\nOn the Marks Card for SUM(Peak US), change the mark type to Bar.\nOn the Marks Card for SUM(Peak US) (2), leave it as Circle.\nSynchronize the axes:\n\nRight-click the right Y-axis and select Synchronize Axis.\n\nAdjust bar size:\n\nOn the SUM(Peak US) Marks card, clickSize and drag the slider left to reduce bar width.\n\n\nEnhance the visual formatting\n\nChange the view to Entire View from the Toolbar.\nColor the marks:\n\nBars:\nOn SUM(Peak US) click Color and choose a light gray.\nCircles:\nOn SUM(Peak US) (2) click Color &gt; More Colors‚Ä¶, enter #ba0c2f for HTML color.\n\nAdd data labels:\n\nOn the SUM(Peak US) (2), hold Ctrl (or Option on Mac) and drag SUM(Peak US) to Label.\nClick Label and set:\n\nHorizontal Alignment: Center\nVertical Alignment: Center\nFont Color: White\n\n\nAdjust circle and bar size if needed.\n\nClean up the view\n\nRemove unnecessary lines:\n\nGo to Format &gt; Lines.\n\nSet Zero Lines and Grid Lines to None.\n\n\nRemove borders:\n\nGo to Format &gt; Borders.\n\nOn Sheet tab, set Row and Column Dividers to None.\n\n\nHide headers and field labels:\n\nRight-click the left Y-axis and uncheck Show Header.\nRight-click album_title and select Hide Field Labels for Columns.\n\nEnsure album titles are fully visible:\n\nHover over the X-axis line until the until the ‚Üï icon appears.\nDrag upward to increase axis height.\n\n\nAdd a descriptive title\n\nDouble click the worksheet title DualAxis.\nEnter:\n\nLine 1: My Favorite Albums\nLine 2: Peak US Chart Positions.\n\nAdjust font sizes and styling as needed.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#pareto-chart",
    "href": "tableau_communicate.html#pareto-chart",
    "title": "Lesson 4. Communicate Data",
    "section": "üìà Pareto chart",
    "text": "üìà Pareto chart\nPareto charts illustrate the Pareto Principle, which suggests that roughly 80% of outcomes result from 20% of causes. In Tableau, these charts are built using table calculations. Individual values are displayed as bars in descending order, while a cumulative percentage line overlays the bars to show the proportion each category contributes to the total.\n\n\n\n\nExercise 1. Pareto Chart\n\n\n\n\n\nCreate a Pareto Chart showing the cumulative percentage of album titles of all inductees into the Rock N Roll Hall of Fame\n\n\n\nStart a new worksheet and rename it ParetoChart\n\n\nAdjust the View\n\n\nIn the Toolbar, change the view from Standard to Entire View\n\n\n\n\nBuild the initial bar chart\n\n\nDrag Artist to Columns.\n\n\nRight-click , hold, and drag album_title to Rows.\n\n\nIn the Drop Field dialog, select CNTD(album_title).\n\n\nSort the bars in descending order.\n\n\n\n\nCreate the Dual Axis\n\n\nHold Ctrl (or Option on Mac) and drag CNTD(album_title) to the right on the Rows shelf to duplicate it.\n\n\nClick the ‚ñº caret on the second CNTD(album_title) pill and select Dual Axis.\n\n\n\n\nFormat as a Pareto chart.\n\n\nOn the Marks Card for CNTD(album_title):\n\n\nSet mark type to Bar.\n\n\nClick Color and choose a light gray.\n\n\nClick Size and drag the slider all the way to left to minimize bar size.\n\n\n\n\nOn the Marks Card for CNTD(album_title) (2):\n\n\nSet the mark type to Line.\n\n\nClick Color &gt; More Colors‚Ä¶ and enter #ba0c2f in the HTML color field.\n\n\n\n\n\n\nAdd a table calculation\n\n\nClick the ‚ñº caret on the second CNTD(album_title) pill and select Add Table Calculation.\n\n\nChange Calculation Type to Running Total.\n\n\nCheck ‚úî Add secondary calculation and set it to Percent of Total.\n\n\nClose the Table Calculation window.\n\n\n\n\nAdjust axis titles.\n\n\nRight-click the left Y-axis, choose Edit Axis and rename it to # of albums.\n\n\nRight-clic the right Y-axis, choose Edit Axis and rename it to % of all albums released.\n\n\n\n\nClean up the view\n\n\nRemove lines:\n\n\nGo to Format &gt; Lines.\n\n\nSet Zero Lines and Grid Lines to None.\n\n\n\n\nRemove borders:\n\n\nGo to Format &gt; Borders.\n\n\nOn Sheet tab, set Row and Column Dividers to None.\n\n\n\n\nHide headers and field labels:\n\n\nClick the ‚ñº caret on the Artist pill and uncheck Show Header.\n\n\nRight-click Artist and select Hide Field Labels for Columns.\n\n\n\n\n\n\nEnhance interactivity.\n\n\nOn the Marks Card for All:\n\n\nClick Tooltip\n\n\nEdit the tooltip to: Artist released CNTD(album_title) albums\n\n\nHover over a bar to preview and proofread the tooltip.\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#calculated-fields",
    "href": "tableau_communicate.html#calculated-fields",
    "title": "Lesson 4. Communicate Data",
    "section": "üßÆ Calculated fields",
    "text": "üßÆ Calculated fields\nCalculated fields empower you to transform your data directly within Tableau. You can use them to clean and organize data, define custom segments, add interactivity, and even perform statistical or mathematical calculations‚Äîall without modifying the original dataset.\nFor example, in the rock_n_roll_studio_albums table, you‚Äôll find album certification statuses from organizations like the RIAA, BPI, and ARIA. With calculated fields, you can standardize these certifications, compare award levels across countries, or even create new fields that group albums by global recognition‚Äîmaking your analysis more insightful and dynamic.\nLet‚Äôs take a closer look at how calculated fields work by analyzing RIAA certifications for our favorite albums.\nOver time, the Recording Industry Association of America (RIAA) has updated its certification standards to reflect changes in how music is consumed. Originally based solely on album sales, certifications now account for digital downloads and streaming activity. To keep things simple, we‚Äôll use the current criteria for album-level awards, as defined by the RIAA in February 2016:\nüéµ RIAA Certification Levels - Gold: 500,000 units - Platinum: 1,000,000 units - Multi-Platinum: 2,000,000+ units (in 1,000,000-unit increments) - Diamond: 10,000,000 units\nüíΩ What Counts as a Unit? A single unit can be any of the following:\n\nOne full digital or physical album sale\n10 track downloads from the same album\n1,500 on-demand audio or video streams from the album\n\nUsing this framework, we can create a calculated field in Tableau to categorize albums by their certification level based on total units.\n\nStart a new worksheet and rename it CalculatedFields\nBuild a text table\n\nDrag Favorite albums to the Filters Shelf and select Favorite albums.\nDrag album_title to Rows.\nDrag Certification Riaa Status to Text on the Marks Card.\nRight-click any album titles with no certification and select ‚ùå Exclude.\n\nCreate a calculated field\n\nIn the Data Pane, click the ‚ñº caret next to the search bar and choose Create Calculated Field.\nName the field Number of units.\nClick the ‚ñ∂ caret on the right side of the Calculation Editor to explore available functions.\nUse an IF statement to assign unit values based on certification: assign units to each certification category.\n\n\nIF &lt;expr&gt; THEN &lt;then&gt; [ELSEIF] &lt;expr2&gt; THEN &lt;then2&gt; ‚Ä¶] [ELSE &lt;else&gt;] END\n\n\nif [Certification Riaa Status]='Gold' then 500000\nelseif [Certification Riaa Status]='Platinum' then 1000000\nelseif [Certification Riaa Status]='Multiplatinum' then 2000000\nelseif [Certification Riaa Status]='Diamond' then 10000000\nend\n\n\nAdd the calculated field to view\n\nOn the Marks Card, replace Certification Riaa Status with the calculated field Number of units.\n\nClean up and enhance the view\n\nRemove borders and shading:\n\nGo to Format &gt; Borders.\n\nOn Sheet tab, set Row Divider to None.\n\nOn Formatting Pane, go to Shading.\n\nOn Sheet tab, set row banding to None on Pane and Header.\n\n\nHide field labels:\n\nRight-click album_title and select Hide Field Labels for Rows.\n\nEdit the text display:\n\nOn the Marks Card, click Text and click the ‚Ä¶ button next to the text box..\nEnter the word units after &lt;SUM(Number of units)&gt; for clarity.\n\n\n\n\nVideo of steps 1-5 above",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#parameters",
    "href": "tableau_communicate.html#parameters",
    "title": "Lesson 4. Communicate Data",
    "section": "Parameters",
    "text": "Parameters\nIn Tableau, parameters: - Replace constant values such as numbers, dates, or text strings in calculations, filters, or reference lines‚Äîmaking your dashboards more flexible and dynamic. - Enhance interactivity by allowing users to control what data is displayed or how it‚Äôs calculated, depending on how the parameter is integrated into the view.\nLet‚Äôs take our CalculatedFields text table and transform it into a bar chart‚Äîa perfect opportunity to explore how parameters can add interactivity and flexibility to your visualization.\n\nDuplicate the existing sheet\n\nRight-click on CalculatedFields and select Duplicate.\nRename the new sheet Parameters.\n\nTransform text table into a bar chart\n\nMove SUM(Number of units) from Text to Columns.\nSort the bars in descending order.\n\nCreate a parameter to highlight specific albums\n\nIn the Data Pane, click the ‚ñº caret next to the search bar and choose Create Parameter.\nName the parameter Highlight album.\nSet the Data type to String.\nUnder Allowable values, select List.\nFrom the Add values from ‚ñº dropdown, choose rock_n_roll_performers &gt; Certification Riaa Status.\n\n\nOnce created, the Highlight album parameter will appear at the bottom of the Data Pane. Right-click it and select Show Parameter to display it in the view.\n\n\n\nNote:\n\n\nSelecting values like Diamond, Gold, or Platinum won‚Äôt affect the chart yet. That‚Äôs because parameters must be embedded in a calculated field to have an effect‚Äîand that calculated field must be added to the view..\n\n\n\nCreate a calculated field\n\nIn the Data Pane, click the ‚ñº caret next to the search bar and choose Create Calculated Field.\nName the field Album highlighter.\nEnter the following formula:\n\n\n\n[Certification Riaa Status]=[Highlight album]\n\n# This expression returns True for albums that match the selected certification in the parameter. \n\n\nAdd the calculated field to view\n\nOn the Marks Card, drag Album highligther to Color.\nUse the Highlight album parameter to toggle between Diamond, Gold, or Platinum and watch the corresponding bars update.\n\nClean up and enhance the view\n\nRemove lines:\n\nGo to Format &gt; Lines.\n\nOn Columns tab, set Gridlines and Axis Rulers to None.\n\n\nCustomize colors:\n\nFor True\n\nClick Color &gt; Edit Colors and then click True.\nChoose a bold color, like red.\n\nFor False\n\nClick Color &gt;Edit Colors and then click False.\nChoose a neutral color, like light gray.\n\n\n\n\n\n\nTip: Rename Output File\n\n\n\nThere are countless creative ways to use parameters with calculated fields in Tableau. You can:\n\n\n\nHighlight or filter specific data\n\n\nAdd weights or thresholds\n\n\nDynamically control reference lines\n\n\nSwitch between metrics or dimensions\n\n\nAnd much more!",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#symbol-map",
    "href": "tableau_communicate.html#symbol-map",
    "title": "Lesson 4. Communicate Data",
    "section": "üó∫Ô∏è Symbol map",
    "text": "üó∫Ô∏è Symbol map\n\n\nStart a new worksheet and rename it SymbolMap\nSet the active data source\n\nOn the Data Pane, select ODNR_fish_stocking.\n\nHighlight Latitude, and Longitude in the Data Pane\nOpen Show Me‚Äîyou‚Äôll notice that Symbol Map is highlighted, since you‚Äôve selected at least one geographic field\nClick Symbol Map to generate a map\n\nAt this point, the map isn‚Äôt very informative. To make it meaningful, we need to add both a measure and a dimension.\n\n\nDrag Numberstocked to Size on the Marks Card\nDrag Location Name to Detail\n\nNow, each lake, river, reservoir, or pond where fish were stocked appears as a separate symbol. The size of each symbol reflects the number of fish stocked at that location.\n\nAnother quick way to create a symbol map:\n\nStart a new worksheet\nDouble-click on County (or any geographic dimension) Tableau will automatically generate a map of Ohio, placing a blue dot in the center of each county.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#choropleth-map",
    "href": "tableau_communicate.html#choropleth-map",
    "title": "Lesson 4. Communicate Data",
    "section": "üó∫Ô∏è Choropleth map",
    "text": "üó∫Ô∏è Choropleth map\nYou can easily convert your symbol map into a choropleth map by replacing Location Name with County on Detail and changing the mark type from Automatic to Map. This change fills each county with color instead of using dots.\nBy dragging Number Stocked (or another measure) to Color, you can visualize how that value is distributed across the state. The deeper the shade, the higher the number‚Äîmaking it easy to spot patterns and regional differences at a glance.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_communicate.html#dual-axis-map",
    "href": "tableau_communicate.html#dual-axis-map",
    "title": "Lesson 4. Communicate Data",
    "section": "üó∫Ô∏è Dual-axis map",
    "text": "üó∫Ô∏è Dual-axis map\nTo see Ohio alone, build a dual-axis map that combines features from both the symbol and the choropleth map. 1. Duplicate the SymbolMap worksheet and rename it DualAxisMap 2. Create the dual axis - Hold Ctrl (or Option on Mac) and drag AVG(Latitude) to the right on the Rows shelf to duplicate it. 3. Create a calculated field - In the Data Pane, click the ‚ñº caret next to the search bar and choose Create Calculated Field. - Name the field State. - Enter the following formula:\n                \"OH\"\n This expression creates a field for **State** and assigns the string \"OH\" to every row in the dataset.\n\nChange State to String data type (if not already) and set Geographic Role to State/Province.\n\n\nFormat to isolate state\n\nOn the Marks Card for AVG(Latitude) (2):\n\nRemove SUM(Numberstocked) and Location Name from the marks card.\nChange the marks type to Map.\nPlace State on detail.\nClick Color:\n\nColor: white\nOpacity: 0%\nBorder: dark gray\n\n\nFrom the Toolbar select Map &gt; Background Layers.\n\nSet Washout to 100%\n\n\nSet dual-axis\n\nClick the ‚ñº caret on the second AVG(Latitude) pill and select Dual Axis.\n\nClean up and enhance the view\n\nRemove borders:\n\nGo to Format &gt; Borders.\n\nOn Sheet tab, set Row and Column Dividers to None.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 4. Communicate Data"
    ]
  },
  {
    "objectID": "tableau_for_research.html",
    "href": "tableau_for_research.html",
    "title": "TABLEAU FOR RESEARCH",
    "section": "",
    "text": "Welcome to Tableau for Research! This tutorial is designed to support multi-session workshops hosted by The Ohio State University Libraries Research Commons. It is intended to help the ABSOLUTE beginner, or anyone who is relatively new to Tableau to build the skills and confidence to apply Tableau to research projects.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Introduction"
    ]
  },
  {
    "objectID": "tableau_for_research.html#download-tableau-desktop",
    "href": "tableau_for_research.html#download-tableau-desktop",
    "title": "TABLEAU FOR RESEARCH",
    "section": "Download Tableau Desktop",
    "text": "Download Tableau Desktop\nMastering data visualization takes time, patience and consistent practice. To complete the activities in this tutorial, you will need a copy of Tableau Desktop.\n\n\n\nMap and Gantt chart for survey question\n\n\nStudents can obtain free access to the Public Edition of Tableau Desktop through Tableau‚Äôs Academic Programs. Both students and instructors are elibile for a full access to Tableau Desktop through the Tableau for Teaching program when the software is used for coursework. If your role at OSU does not involve teaching, please follow the university‚Äôs instructions for obtaining Tableau Desktop Access. Copies of Tableau Desktop are also installed on workstations in the Research Commons computer lab. You can access these machines remotely using the online PC reservation system.\nFor more details, see Tableau Desktop and Tableau Desktop Public Edition Feature Comparison.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Introduction"
    ]
  },
  {
    "objectID": "tableau_for_research.html#what-is-data-literacy",
    "href": "tableau_for_research.html#what-is-data-literacy",
    "title": "TABLEAU FOR RESEARCH",
    "section": "What is data literacy?",
    "text": "What is data literacy?\n\n\n\nData literacy is a journey\n\n\nStrengthening your ability to evaluate and work with data will empower you to use data more effectively and responsibly.\n\n\n\nData literacy defined\nData literacy is ‚Äúthe ability to read, work with, analyze, and argue with data.‚Äù\n\nReading data: Understanding what data represents and how it reflects the world around us.\nWorking with data: Creating, acquiring, cleaning, and managing datasets.\nAnalyzing data: Filtering, sorting, aggregating, comparing, and analyzing data.\nArguing with data: Using data to support a narrative that is intended to communicate some message or story to a particular audience. (Bhargava and D‚ÄôIgnazio 2015)\n\nData literacy concepts and competencies are integrated throughout this tutorial to help you build strong skills in data visualization and storytelling. In addition to interpreting and evaluating data, we‚Äôll explore data handling, data synthesis, ethical use of data, and best practices for presenting data effectively.\nWhen looking at data collected by others - whether it is raw data or data published in an article - it is important to understand what the data means, how it was collected, and who collected it. You must critically evaluate each data source before you consider using data to support an argument. Approach your data with curiosity and some skepticism.\nUse the DRAMA Framework to evaluate your data. (Primeau, n.d.)\n\n\n\n\nDRAMA Framework\n\n\n \n\n\nDate\n\n\nWhen was the data last updated? Is it current? Does it reflect current trends?  \n\n\nRelevance\n\n\nWhat procedures were used to collect the data? Is the data relevant to my research project? Did sampling procedures target the right audience or population? What was the context for collecting the data? Is there a description of the data set and what data it does and does not contain?  \n\n\nAccuracy\n\n\nIs the data reliable? Valid? Were procedures for gathering the data followed consistently?  \n\n\nMotivation\n\n\nWhy was the data collected? Are there any potential biases in the data? Was any relevant data not included in the dataset? If yes, was this disclosed?  \n\n\nAuthority\n\n\nWho collected the data? an individual? a government agency? a business? or a political action committee? Are they credible?",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Introduction"
    ]
  },
  {
    "objectID": "tableau_for_research.html#what-to-expect",
    "href": "tableau_for_research.html#what-to-expect",
    "title": "TABLEAU FOR RESEARCH",
    "section": "What to expect",
    "text": "What to expect\nEach section in this tutorial introduces one or two data literacy concepts or chart selection tools, followed-by three to four hands-on lessons demonstrating how to apply these ideas in Tableau. The lessons guide you through each step, and are complemented by readings that highlight best practices for data visualization. The texts are available through The Ohio State University Libraries for faculty, students and staff, and include:\n\n\n\n\nBETTER DATA VISUALIZATIONS\n\n\n\n\nBetter Data Visualizations: A Guide for Scholars, Researchers, and Wonks\n\n\n\n\nby Jonathan Schwabish\n\n\nNew York : Columbia University Press, 2021.\n\n\n\n\n\n\n\nEFFECTIVE DATA VISUALIZATION\n\n\n\n\nEffective Data Visualization: The Right Chart for the Right Data\n\n\n\n\nby Stephanie Evergreen\n\n\nThousand Oaks, California: SAGE Publications, 2020\n\n\n\n\n\n\n\nSTORYTELLING WITH DATA\n\n\n\n\nStorytelling with Data: A Data Visualization Guide for Business Professionals\n\n\n\n\nby Cole Nussbaumer Knaflic\n\n\nHoboken, New Jersey: Wiley, 2015.\n\n\n\n\n\nSince practice is one of the most effective ways to learn data visualization, each lesson also includes activites that encourage you to apply what you‚Äôve learned to a dataset that is meaningful to you. The lesson then concludes with a reflection prompt, inviting you to consider the skills you‚Äôve developed and how you might apply these skills to a future project.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Introduction"
    ]
  },
  {
    "objectID": "excel.html",
    "href": "excel.html",
    "title": "Excel",
    "section": "",
    "text": "Don‚Äôt discount Excel! For datasets under 1,000 rows, Excel can be an ideal tool for creating clear and effective visualizations. The resources below offer practical guidance and tips for making the most of Excel‚Äôs powerful charting and data presentation features.\n\nBooks\n\n\n\n\nDATA VISUALIZATION IN EXCEL\n\n\n\n\nData Visualization in Excel: A Guide for Scholars, Researchers, and Wonks\n\n\n\n\nJonathan Schwabish shows Excel remains a powerful tool for visualizing data. Each chapter provides detailed instructions for creating nearly 30 additional graphs beyond Excel‚Äôs default graph library. From the basic heatmaps to advanced Marimekko charts, this book will guide you in crafting more colorful, effective data visualizations for your audience.\n\n\n\n\n\n\n\nEFFECTIVE DATA VISUALIZATION\n\n\n\n\nEffective Data Visualization: The Right Chart for the Right Data\n\n\n\n\nStephanie Evergreen provides detailed instructions for constructing and then formatting Excel graphs that inspire conversations and support decision-making. A fun ninja rating scale helps you to decide how challenging it may be to create each graph and is embedded in the side-margins of each chapter. Further, an entire chapter offers ideas for visualizing qualitative and quantitative data.\n\n\n\n\n\n\n\nO‚ÄôReilly Online Learning\nFor additional books and learning materials, the O‚ÄôReilly Online Learning: Academic/Public Library Edition collection provides extensive access to eBooks and videos in computer science, IT, business, and related subjects, featuring content from O‚ÄôReilly and other top publishers. This resource is provided by University Libraries and is available to all Ohio State faculty, students, and staff with a valid osu.edu email address.\n\n\nWorkshops and Events\nUniversity Libraries offers a variety of data skills workshops and events for Ohio State faculty, students and staff throughout the academic year.",
    "crumbs": [
      "Library Resources",
      "TOOLS",
      "... Excel"
    ]
  },
  {
    "objectID": "python_basics.html",
    "href": "python_basics.html",
    "title": "Mastering the Basics",
    "section": "",
    "text": "Python",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#getting-started",
    "href": "python_basics.html#getting-started",
    "title": "Mastering the Basics",
    "section": "Getting started",
    "text": "Getting started\n\nSetup Python\nA easy way to install Python and essential libraries is by dowloading Anaconda on your personal device. Anaconda is a Python distribution that simplifies package management and deployment. It comes preloaded with over 250 popular for data analysis and visualization‚Äîincluding Pandas, NumPy, and Matplotlib‚Äîand allows you to install, update, and manage packages without using the command line, making it especially user-friendly for beginners.\n\n\nSetup IDE\nIntegrated Development Environments (IDEs) combine essential coding tools‚Äîlike editors, terminals, and plugins‚Äîinto as single application to streamline your workflow. Spyder, included with the Anaconda distribution, is tailored for scientists, engineers, and data analysts. Another excellent option is Visual Studio Code, a versatile IDE that offers real-time feedback as you build and refine your code. Both tools support interactive coding, data exploration and more, making them great choices for beginners and professionals alike.",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#relative-paths",
    "href": "python_basics.html#relative-paths",
    "title": "Mastering the Basics",
    "section": "Relative paths",
    "text": "Relative paths\nIf your files are stored in the same folder as your Python script, you can use a relative path.\n\nrelative_filepath_of_text = \"path.txt\"\n\nPython interprets this path relative to the current working directory.",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#absolute-paths",
    "href": "python_basics.html#absolute-paths",
    "title": "Mastering the Basics",
    "section": "Absolute paths",
    "text": "Absolute paths\nIf your files are stored elsewhere, use an absolute path to specify the full location:\n\nabsolute_filepath_of_text = \"C:/path/to/your/directory/file.txt\"",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#opening-a-file",
    "href": "python_basics.html#opening-a-file",
    "title": "Mastering the Basics",
    "section": "Opening a file",
    "text": "Opening a file\nTo work with a file in Python, you first need to open it. This returns a file object that allows you to read from or write to the file.\nBasic Syntax:\n\nopen(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)\n\n\nfile: The path to the file (absolute or relative).\nmode: Optional. Specifies how the file is opened. Default is r (read mode).",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#common-file-modes",
    "href": "python_basics.html#common-file-modes",
    "title": "Mastering the Basics",
    "section": "Common file modes",
    "text": "Common file modes\n\n\n\n\n\n\n\nCharacter\nMeaning\n\n\n\n\n'r'\nopen for reading (default)\n\n\n'w'\nopen for writing, truncating the file first\n\n\n'x'\nopen for exclusive creation, failing if the file already exists\n\n\n'a'\nopen for writing, appending to the end of file if it exists\n\n\n'b'\nbinary mode\n\n\n't'\ntext mode (default)\n\n\n'+'\nopen for updating (reading and writing",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#encoding",
    "href": "python_basics.html#encoding",
    "title": "Mastering the Basics",
    "section": "Encoding",
    "text": "Encoding\nUse the encoding parameter when working in text mode to specify how characters are interpreted. Common encodings include: - utf-8 ‚Äî Most commonly used and recommended. - utf-16 ‚Äî Uses 2 bytes per character; useful for some international text. - utf-32 ‚Äî Uses 4 bytes per character; supports all Unicode characters directly.\n\ncarmen_ohio=open('carmen_ohio.txt', mode='r', encoding='utf-8')\nprint(carmen_ohio)\n\n&lt;_io.TextIOWrapper name='Carmen_Ohio.txt' mode='r' encoding='utf-8'&gt;",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#reading-a-file",
    "href": "python_basics.html#reading-a-file",
    "title": "Mastering the Basics",
    "section": "Reading a file",
    "text": "Reading a file\nTo read a file, open it in read mode (r) and use the .read() method.\n\ncarmen_ohio=open('carmen_ohio.txt', mode='r', encoding='utf-8').read()\nprint(carmen_ohio)\n\nOh come let's sing Ohio's praise\nAnd songs to Alma Mater raise\nWhile our hearts rebounding thrill\nWith joy which death alone can still\nSummer's heat or winter's cold\nThe seasons pass the years will roll\nTime and change will surely[a] show\nHow firm thy friendship ... OHIO!\n\n\nThese jolly days of priceless worth\nBy far the gladdest days on earth\nSoon will pass and we not know\nHow dearly we love Ohio\nWe should strive to keep thy name\nOf fair repute and spotless fame\nSo in college halls we'll grow\nAnd love thee better ... OHIO!\n\n\nThough age may dim our mem'ry's store\nWe'll think of happy days of yore\nTrue to friend and frank to foe\nAs sturdy sons of Ohio\nIf on seas of care we roll\nNeath blackened sky or barren shoal\nThoughts of thee bid darkness go\nDear Alma Mater ... OHIO!\n\n\n\n#Alternative - use a with block to open the file in read mode and use the .read() method \n\nwith open('carmen_ohio.text', mode='r', encoding='utf-8') as file:\n    content = file.read()",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#writing-to-a-file",
    "href": "python_basics.html#writing-to-a-file",
    "title": "Mastering the Basics",
    "section": "Writing to a file",
    "text": "Writing to a file\nTo write a file, open it in write mode (w). This will overwrite the file if it already exists.\nIf you don‚Äôt specify a mode, Python defaults to r (read mode), so it‚Äôs important to explicity set mode='w' when writing.\n\n\nTip: Rename Output File\n\n\nTo avoid accidentally overwriting your original file, consider renaming the output file before writing to it.\n\n\n\nopen('carmen_ohio_revised.txt', mode='w', encoding='utf-8')\n\n&lt;_io.TextIOWrapper name='carmen_ohio_revised.txt' mode='w' encoding='utf-8'&gt;\n\n\nAgain, this creates an object. To add text to a file, use the .write() method.\n\nopen('carmen_ohio_revised.txt', mode='w', encoding='utf-8').write(\"Let's firm thy friendship again!!!\")\n\nNow our file reads:\n\nopen('carmen_ohio_revised.txt', mode='r', encoding='utf-8').read()\n\n\"Let's firm thy friendship again!!!\"\n\n\n\n#Alternative - use a with block to open in write mode and use the .write() method \n\nwith open('carmen_ohio_revised.txt', mode='w', encoding='utf-8') as file:\n¬†¬†¬† file.write(\"Let's firm thy friendship again!!!\")",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#working-with-variables",
    "href": "python_basics.html#working-with-variables",
    "title": "Mastering the Basics",
    "section": "Working with variables",
    "text": "Working with variables\nVariables are created using the = operator.\n\ninstitution_name = \"The Ohio State University\"\nyear_founded = 1870\nuniversity_facts = {\"institution_name\": \"The Ohio State University\", \"year_founded\" : 1870, \"school_colors\" : \"scarlet & grey\", \"mascot\" : \"Brutus Buckeye\", \"students\" : 60046, \"endowment\": 7.9 }\nfact_list = [\"The Ohio State University\", 1870, \"scarlet & grey\",\"Brutus Buckeye\", 60046, 7.9 ]\n\n\nNaming rules\n\nCan include letters, digits, and underscores.\nMust not start with a digit.\nCannot contain spaces or special characthers (except _).\nMust not use reserved Python keywords.\n\n\n\nReserved keywords\nFalse, True, None, and, as, assert, async, await, def, del, elif, else, break, class, continue, except, finally, for, from, global, if, import, in, is, lambda, nonlocal, not, or, pass, raise, return, try, while, with, yield\nTo check a variable‚Äôs type, use the type() function.\n\ntype(university_facts)\n\ndict",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#assigning-strings-to-variables",
    "href": "python_basics.html#assigning-strings-to-variables",
    "title": "Mastering the Basics",
    "section": "Assigning strings to variables",
    "text": "Assigning strings to variables\nYou can assign a string to a variable using the = operator:\n\nbanned_book = \"Brave New World\"",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#string-indexing-and-slicing",
    "href": "python_basics.html#string-indexing-and-slicing",
    "title": "Mastering the Basics",
    "section": "String indexing and slicing",
    "text": "String indexing and slicing\nPython uses zero-based indexing, meaning the first character of a string is at position 0.\n\nAccess a single character\n\nbanned_book[0]\n\n'B'\n\n\n\n\nSlice a substring\nTo extract a portion of a string, use the following syntax:\n\nstring[start index:end index]\n\nThe start index is where the slice begins and the end index tells Python where to stop, but it does not include the character at that position.\nExample: Slice the word Brave from the string banned_book:\n\nbanned_book[0:5]\n\n'Brave'\n\n\nIf the start index is not included, Python will automatically start the range at the 0 index position.\n\nbanned_book[:5]\n\n'Brave'\n\n\n\n\nNegative Indexing\nYou can also access characters from the end of a string using negative indices:\n\nlibrary = \"Thompson Library, 1858 Neil Avenue Mall, Columbus, OH 43210\"\nzipcode = library[-5:]\n\nzipcode\n\n'43085'\n\n\n\n\n\n\nExercise 2: Slice Carmen Ohio\n\n\n\n\n\n\nFind the first 250 characters in Carmen Ohio\n\n\nFind the last 250 characters in Carmen Ohio\n\n\nFind the second sentence in Carmen Ohio\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\ncarmen_ohio=open('carmen_ohio.txt', mode='r', encoding='utf-8').read()\n\n# first 250 characters\ncarmen_first_250=carmen_ohio[:250]\nprint(carmen_first_250)\n\n# last 250 characters\ncarmen_last_250=carmen_ohio[250:]\nprint(carmen_last_250)\n\n# second sentence\ncarmen_sentence=carmen_ohio[33:62]\nprint(carmen_sentence)\n\n\n\n\n\n\nCommon string methods\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nstring.lower()\nConverts all characters to lowercase\n\n\nstring.upper()\nConverts all characters to uppercase\n\n\nstring.title()\nCapitalizes the first letter of each word\n\n\nstring.strip()\nRemoves leading and trailing white space\n\n\nstring.replace('old string', 'new string')\nReplaces all occurrences of old with new\n\n\nstring.split('delim')\nSplits the string into a list using the specified delimiter\n\n\nstring.join(list)\nJoins elements of a list into a string using the specified delimiter.\n\n\nstring.startswith('some string')\nReturns True if the string starts with the specified text\n\n\nstring.endswith('some string')\nReturns True if the string ends with the specified text\n\n\nstring.isspace()\nReturns True if the string contains only whitespace\n\n\n\nüîó For more, see Python String Methods documentation.\n\n\nExamples:\n\n\n1. Replace text in a string\nLet‚Äôs try replacing the word praise in the first sentence of carmen_ohio with wonders using the replace method.\nBasic Syntax\n\nstring.replace('old string', 'new string')\n\n\nfirst_sentence=carmen_ohio[:32]\nrevised_first_sentence=first_sentence.replace('praise','wonders')\n\nprint(\"original first sentence: \"+ first_sentence)\nprint(\"revised first sentence: \"+revised_first_sentence)\n\noriginal first sentence: Oh come let's sing Ohio's praise\nrevised first sentence: Oh come let's sing Ohio's wonders\n\n\n\n\n\n\nExercise 3: Strings\n\n\n\n\n\n\nIsolate a sentence in carmen_ohio\n\n\nPrint the isolated sentence\n\n\nCreate a new sentence and try replacing a word\n\n\nPrint the new sentence in uppercase text\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\ncarmen_ohio=open('carmen_ohio.txt', mode='r', encoding='utf-8').read()\n\n# isolate a sentence\ncarmen_sentence=carmen_ohio[:333]\n\n# print the isolated sentence\nprint(carmen_sentence)\n\n# create a new sentence and try replacing a word\nnew_sentence=carmen_sentence.replace('gladdest','happiest')\n\n# print the new sentence\nprint(new_sentence.upper())\n\n\n\n\n\n\n2. Normalize case\nString methods like .lower() and .upper() are useful for standardizing text, especially when comparing values that may differ in capitalization.\nExample:\n\nname1 = \"McMurty, Larry\"\nname2 = \"Mcmurty, Larry\"\nname3 = \"mcmurty, larry\"\n\n#Convert all names to uppercase for consistent comparison\nname1_rev = name1.upper()\nname2_rev = name2.upper()\nname3_rev = name3.upper()\n\nprint(name1_rev)\nprint(name2_rev)\nprint(name3_rev)\n\nMCMURTY, LARRY\nMCMURTY, LARRY\nMCMURTY, LARRY\n\n\n\n\n3. Splitting strings\nBy default, .split() separates strings by spaces. You can also specify a delimiter, such as newline ‚Äò‚Äô. This returns a list of lines, one for each row in the string.\n\nnewline_free_carmen=carmen_ohio.split('\\n')\nnewline_free_carmen\n\n[\"Oh come let's sing Ohio's praise\",\n 'And songs to Alma Mater raise',\n 'While our hearts rebounding thrill',\n 'With joy which death alone can still',\n \"Summer's heat or winter's cold\",\n 'The seasons pass the years will roll',\n 'Time and change will surely[a] show',\n 'How firm thy friendship ... OHIO!',\n '',\n '',\n 'These jolly days of priceless worth',\n 'By far the gladdest days on earth',\n 'Soon will pass and we not know',\n 'How dearly we love Ohio',\n 'We should strive to keep thy name',\n 'Of fair repute and spotless fame',\n \"So in college halls we'll grow\",\n 'And love thee better ... OHIO!',\n '',\n '',\n \"Though age may dim our mem'ry's store\",\n \"We'll think of happy days of yore\",\n 'True to friend and frank to foe',\n 'As sturdy sons of Ohio',\n 'If on seas of care we roll',\n 'Neath blackened sky or barren shoal',\n 'Thoughts of thee bid darkness go',\n 'Dear Alma Mater ... OHIO!']\n\n\nIf the delimiter is blank, a list of words is returned:\n\ncarmen_split=carmen_ohio.split()\ncarmen_split[0:6] #limit to first 6\n\n['Oh', 'come', \"let's\", 'sing', \"Ohio's\", 'praise']",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#example-wilbur-the-pig",
    "href": "python_basics.html#example-wilbur-the-pig",
    "title": "Mastering the Basics",
    "section": "Example: Wilbur the Pig",
    "text": "Example: Wilbur the Pig\n\n\n\ncute piglet\n\n\nLet‚Äôs say we want to:\n\nPrint Wilbur‚Äôs name if he‚Äôs a pig.\nCompliment him if he‚Äôs handsome.\nConfirm if Charlotte is his best friend.\n\n\nStep 1: Assign variables\n\nwilbur_species='pig'\nwilbur_handsome='handsome'\nwilbur_likes_charlotte=True\n\n\n\nStep 2: Create conditional statements\n\nif wilbur_species == 'pig':\n    print('Wilbur')\n    \nif wilbur_handsome=='handsome':\n    print(\"He's really cute!\") # Use double quotes to handle apostrophes\n    \nif wilbur_likes_charlotte==True:\n    print('Charlotte is Wilbur\\'s best friend') # or cancel out the apostrophe\nelse:\n    print('no')\n\nWilbur\nHe's really cute!\nCharlotte is Wilbur's best friend",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#using-if-elif-and-else",
    "href": "python_basics.html#using-if-elif-and-else",
    "title": "Mastering the Basics",
    "section": "Using if, elif, and else",
    "text": "Using if, elif, and else\n\nif checks the first condition.\nelif (short for ‚Äúelse if‚Äù) checks additional conditions if the first is false.\nelse runs if none of the previous conditions are met.\n\n\nif wilbur_likes_charlotte==True:\n     print(\"Charlotte is Wilbur's best friend\") \n\nCharlotte is Wilbur's best friend\n\n\n\nif wilbur_likes_charlotte==True:\n    print('yes')\nelse:\n    print(\"Charlotte is Wilbur's best friend\") \n\nyes",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#comparison-operators",
    "href": "python_basics.html#comparison-operators",
    "title": "Mastering the Basics",
    "section": "Comparison operators",
    "text": "Comparison operators\n\n\n\n\n\n\n\nOperator\nMeaning\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n&gt;\nGreater than\n\n\n&lt;\nLess than\n\n\n&gt;=\nGreater than or equal to\n\n\n&lt;=\nLess than or equal to\n\n\n\n\n\n\n\nExercise 4: Conditionals | Comparisons\n\n\n\n\n\n\nwilbur_age = 1\n\n\ncharlotte_age = 4\n\n\nfarmer_age = 42\n\n\n\nIs Wilbur older than Charlotte? Verify Wilbur‚Äôs age with a conditional statement.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nwilbur_age=1\ncharlotte_age=4\nfarmer_age=42\n\nprint('IS WILBUR OLDER THAN CHARLOTTE?')\n\nif wilbur_age &gt; charlotte_age:\n    print(\"Wilbur is older than Charlotte.\")\nelse:\n    print(\"Wilbur is younger than Charlotte.\")",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#logical-operators",
    "href": "python_basics.html#logical-operators",
    "title": "Mastering the Basics",
    "section": "Logical operators",
    "text": "Logical operators\nUse these to combine multiple conditions\n\n\n\n\n\n\n\nOperator\nMeaning\n\n\n\n\nand\nTrue if both conditions are true\n\n\nor\nTrue if at least one conditions is true\n\n\nnot\nTrue if the condition is not true\n\n\n\n\nExamples:\n\n\nif wilbur_species == 'pig' and wilbur_handsome == 'handsome':\n    print(\"Wilbur is a handsome pig!\")\n\nWilbur is a handsome pig!\n\n\nIs this Wilbur?\n\n\n\nguinea pig\n\n\n\ny=\"guinea pig\"\n\nif not y:\n    print('Wilbur')\nelse:\n    print(y)\n\nguinea pig",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#using-the-in-keyword",
    "href": "python_basics.html#using-the-in-keyword",
    "title": "Mastering the Basics",
    "section": "Using the in keyword",
    "text": "Using the in keyword\nThe in keyword checks if a substring exists within a string.\n\nif 'mouse' not in y:\n    print('Not a guinea pig')\n\nNot a guinea pig",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#accessing-list-items",
    "href": "python_basics.html#accessing-list-items",
    "title": "Mastering the Basics",
    "section": "Accessing list items",
    "text": "Accessing list items\nLists use zero-based indexing, just like strings. You can use slicing to extract elements:\n\nlist_name[start_index:stop_index:step]\n\n\nExamples:\n\nExtract the first item from the list characters:\n\n\ncharacters[0]\n\n'Han Solo'\n\n\n\nExtract the last 3 items from the list characters:\n\n\ncharacters[-3:]\n\n['Luke Skywalker', 'C3PO', 'R2D2']\n\n\n\nExtract characters from the string han_solo:\n\n\nhan_solo='Han Solo'\nhan_only=han_solo[0:3]\nprint(han_only)\n\nHan\n\n\n\n\n\n\nExercise 5: Extract list items\n\n\n\n\nIs there another method to extract the last 3 items from the list characters?\ncharacters = ['Han Solo', 'Luke Skywalker', 'C3PO', 'R2D2']\n\n\n\n\n\n\nSolution:\n\n\n\n\ncharacters = ['Han Solo', 'Luke Skywalker', 'C3PO', 'R2D2']\nlast3_characters_alternative = characters[1:]\nprint(last3_characters_alternative )\n\n\n\n\n\n\n\n\nExercise 6: Step through list\n\n\n\n\n\nExtract every second item from the list numbers.\n\nnumbers = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n\n\n\n\n\n\nSolution:\n\n\n\n\nnumbers = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\neven_numbers=numbers[1::2]\nprint(even_numbers)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#common-list-methods",
    "href": "python_basics.html#common-list-methods",
    "title": "Mastering the Basics",
    "section": "Common list methods",
    "text": "Common list methods\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nlist.append(another_item)\nAdds an item to end of list\n\n\nlist.extend(another_list)\nAdds all items from another list\n\n\nlist.remove(item)\nRemoves the first occurence of the item\n\n\nlist.sort(reverse=False)\nSorts the list in ascending order\n\n\nlist.reverse()\nReverses the order of the list\n\n\n\n\nExamples:\n\ncharacters = ['Han Solo', 'Luke Skywalker', 'C3PO', 'R2D2']\ncharacters.append('Rey')\nprint(characters)\n\n['Han Solo', 'Luke Skywalker', 'C3PO', 'R2D2', 'Rey']\n\n\n\ncharacters.remove('Rey')\nprint(characters)\n\n['Han Solo', 'Luke Skywalker', 'C3PO', 'R2D2']\n\n\n\ncharacters_new = ['Rey','BB-8']\ncharacters.extend(characters_new)\nprint(characters)\n\n['Han Solo', 'Luke Skywalker', 'C3PO', 'R2D2', 'Rey', 'BB-8']\n\n\n\ncharacters.sort()\nprint(characters)\n\n['BB-8', 'C3PO', 'Han Solo', 'Luke Skywalker', 'R2D2', 'Rey']\n\n\n\ncharacters.sort(reverse=True)\nprint(characters)\n\n['Rey', 'R2D2', 'Luke Skywalker', 'Han Solo', 'C3PO', 'BB-8']",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#using-loops-with-lists",
    "href": "python_basics.html#using-loops-with-lists",
    "title": "Mastering the Basics",
    "section": "Using loops with lists",
    "text": "Using loops with lists\nWhen working with lists, it‚Äôs a good practice to use plural names for your list variables. This makes your code easier to read and understand‚Äîespecially when you‚Äôre iterating through each element.\nPython‚Äôs for loop allows you to iterate over items in a sequence‚Äîsuch as a list, tuple, dictionary, or string‚Äîand execute a block of code for each item.\nBasic Syntax\n\nfor item in sequence\n     #Code to execute for each item\n\n\nExamples:\n\nLoop through the characters list.\n\n\nfor character in characters:\n    print(character)\n\nRey\nR2D2\nLuke Skywalker\nHan Solo\nC3PO\nBB-8\n\n\n\nUse slicing to print names of the last 3 characters:\n\n\nfor character in characters[-3:]:\n    print(character)\n\nHan Solo\nC3PO\nBB-8\n\n\n\nUse conditional statements to add a comment for each character:\n\n\nfor character in characters:\n    if character == 'BB-8' or character=='R2D2' or character == 'C3PO':\n        print(f\"{character} is a robot\")\n    elif character == 'Luke Skywalker':\n        print('Luke is Leia\\'s brother')\n    elif character == 'Han Solo':\n        print('Chewbacca is Han Solo\\'s friend')\n    elif character == 'Rey':\n        print(\"Rey rocks\")\n\nRey rocks\nR2D2 is a robot\nLuke is Leia's brother\nChewbacca is Han Solo's friend\nC3PO is a robot\nBB-8 is a robot\n\n\nNote the line f\"{character} is a robot\" in the first conditional statement of the loop. This is known as an f-string, short for formatted string literal.\nAn f-string is created by placing the letter f directly before the opening quotation mark of a string. This allows you to embed variables or expressions inside the string using curly braces {}. Python will automatically evaluate the expression and insert its value into the string.\n\n\n\n\nExercise 7: Loops\n\n\n\n\nPrint the name of each president of The Ohio State University.\npresidents=[\"Edward Francis Baxter Orton Sr\",\"Walter Quincy Scott\",\"William Henry Scott\",\"James Hulme Canfield\",\"William Oxley Thompson\",\n            \"George Washington Rightmire\",\"William McPherson\",\"Howard Landis Bevis\",\"Novice Gail Fawcett\",\"Harold Leroy Enarson\",\n            \"Edward Harrington Jennings\",\"E. Gordon Gee\",\"John Richard Sisson\",\"William English Kirwan\",\"Edward Harrington Jennings\",\n            \"Karen Ann Holbrook\",\"Joseph A. Alutto\",\"E. Gordon Gee\",\"Joseph A. Alutto\",\"Michael V. Drake\",\"Kristina M. Johnson\",\n            \"Peter J. Mohler\",'Walter \"Ted\" Carter, Jr.']\n\n\n\n\n\n\nSolution:\n\n\n\n\npresidents=[\"Edward Francis Baxter Orton Sr\",\"Walter Quincy Scott\",\"William Henry Scott\",\"James Hulme Canfield\",\"William Oxley Thompson\",\n            \"George Washington Rightmire\",\"William McPherson\",\"Howard Landis Bevis\",\"Novice Gail Fawcett\",\"Harold Leroy Enarson\",\n            \"Edward Harrington Jennings\",\"E. Gordon Gee\",\"John Richard Sisson\",\"William English Kirwan\",\"Edward Harrington Jennings\",\n            \"Karen Ann Holbrook\",\"Joseph A. Alutto\",\"E. Gordon Gee\",\"Joseph A. Alutto\",\"Michael V. Drake\",\"Kristina M. Johnson\",\n            \"Peter J. Mohler\",'Walter \"Ted\" Carter, Jr.']\n\nfor president in presidents:\n    print(president)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#index-or-enumerate-list-items",
    "href": "python_basics.html#index-or-enumerate-list-items",
    "title": "Mastering the Basics",
    "section": "Index or enumerate list items",
    "text": "Index or enumerate list items\nThere are several ways to determine the index position of items in a list.\n\n.index()\nOne way is to use a for loop with the built-in .index() method. This method returns the first occurence of a specified value in the list.\nHere‚Äôs an example using list of Ohio State University presidents.\n\npresidents=[\"Edward Francis Baxter Orton Sr\",\"Walter Quincy Scott\",\"William Henry Scott\",\"James Hulme Canfield\",\"William Oxley Thompson\",\n            \"George Washington Rightmire\",\"William McPherson\",\"Howard Landis Bevis\",\"Novice Gail Fawcett\",\"Harold Leroy Enarson\",\n            \"Edward Harrington Jennings\",\"E. Gordon Gee\",\"John Richard Sisson\",\"William English Kirwan\",\"Edward Harrington Jennings\",\n            \"Karen Ann Holbrook\",\"Joseph A. Alutto\",\"E. Gordon Gee\",\"Joseph A. Alutto\",\"Michael V. Drake\",\"Kristina M. Johnson\",\n            \"Peter J. Mohler\",'Walter \"Ted\" Carter, Jr.']\n\nfor president in presidents:\n    president_index_position=presidents.index(president)\n    president_index_position=president_index_position+1\n    print(str(president_index_position)+' '+president)\n\n1 Edward Francis Baxter Orton Sr\n2 Walter Quincy Scott\n3 William Henry Scott\n4 James Hulme Canfield\n5 William Oxley Thompson\n6 George Washington Rightmire\n7 William McPherson\n8 Howard Landis Bevis\n9 Novice Gail Fawcett\n10 Harold Leroy Enarson\n11 Edward Harrington Jennings\n12 E. Gordon Gee\n13 John Richard Sisson\n14 William English Kirwan\n11 Edward Harrington Jennings\n16 Karen Ann Holbrook\n17 Joseph A. Alutto\n12 E. Gordon Gee\n17 Joseph A. Alutto\n20 Michael V. Drake\n21 Kristina M. Johnson\n22 Peter J. Mohler\n23 Walter \"Ted\" Carter, Jr.\n\n\n\n\n.enumerate()\nA more common and efficient approach is to use the built-in enumerate() function within a for loop.\nThe enumerate() function returns both the index and the item as you iterate through the list, making your code more readable and avoiding potential issues with methods like .index(), which only returns the first occurrence of a value.\n\nWhy use enumerate()?\n\nIt‚Äôs more efficient than calling .index() inside a loop.\nIt handles duplicate values correctly by giving the actual position in the list.\nIt improves readability and reduces the chance of logical errors.\n\nThis example asks Python to return two variables from the presidents list, the index position and the president.\n\nfor index, president in enumerate(presidents):\n    print(index, president)\n\n0 Edward Francis Baxter Orton Sr\n1 Walter Quincy Scott\n2 William Henry Scott\n3 James Hulme Canfield\n4 William Oxley Thompson\n5 George Washington Rightmire\n6 William McPherson\n7 Howard Landis Bevis\n8 Novice Gail Fawcett\n9 Harold Leroy Enarson\n10 Edward Harrington Jennings\n11 E. Gordon Gee\n12 John Richard Sisson\n13 William English Kirwan\n14 Edward Harrington Jennings\n15 Karen Ann Holbrook\n16 Joseph A. Alutto\n17 E. Gordon Gee\n18 Joseph A. Alutto\n19 Michael V. Drake\n20 Kristina M. Johnson\n21 Peter J. Mohler\n22 Walter \"Ted\" Carter, Jr.\n\n\nThis example uses an f-string to make the returned print statement more clear.\n\nfor index, president in enumerate(presidents):\n    print(f\"{president} was the {index} president of The Ohio State University\")\n\nEdward Francis Baxter Orton Sr was the 0 president of The Ohio State University\nWalter Quincy Scott was the 1 president of The Ohio State University\nWilliam Henry Scott was the 2 president of The Ohio State University\nJames Hulme Canfield was the 3 president of The Ohio State University\nWilliam Oxley Thompson was the 4 president of The Ohio State University\nGeorge Washington Rightmire was the 5 president of The Ohio State University\nWilliam McPherson was the 6 president of The Ohio State University\nHoward Landis Bevis was the 7 president of The Ohio State University\nNovice Gail Fawcett was the 8 president of The Ohio State University\nHarold Leroy Enarson was the 9 president of The Ohio State University\nEdward Harrington Jennings was the 10 president of The Ohio State University\nE. Gordon Gee was the 11 president of The Ohio State University\nJohn Richard Sisson was the 12 president of The Ohio State University\nWilliam English Kirwan was the 13 president of The Ohio State University\nEdward Harrington Jennings was the 14 president of The Ohio State University\nKaren Ann Holbrook was the 15 president of The Ohio State University\nJoseph A. Alutto was the 16 president of The Ohio State University\nE. Gordon Gee was the 17 president of The Ohio State University\nJoseph A. Alutto was the 18 president of The Ohio State University\nMichael V. Drake was the 19 president of The Ohio State University\nKristina M. Johnson was the 20 president of The Ohio State University\nPeter J. Mohler was the 21 president of The Ohio State University\nWalter \"Ted\" Carter, Jr. was the 22 president of The Ohio State University\n\n\nKeep in mind that Python uses zero-based indexing, meaning the first item in a list has an index of 0. To make the output more user-friendly‚Äîespecially when displaying positions‚Äîyou can add 1 to each index so that counting starts at 1.\nTo enhance the script further, you can use a conditional statement to append a superscript to each number (like 1À¢·µó, 2‚Åø·µà, 3 ≥·µà, etc.), making the output more polished and readable.\n\nfor index, president in enumerate(presidents):\n    index=index+1\n    if index==1 or index==21:\n        superscript='st'\n    elif index==2 or index==22:\n        superscript='nd'\n    elif index==3 or index==23:\n        superscript='rd'\n    elif index&gt;=4 and index&lt;=20:\n        superscript='th'\n    print(f\"{president} was the {index}{superscript} president of The Ohio State University\")\n\nEdward Francis Baxter Orton Sr was the 1st president of The Ohio State University\nWalter Quincy Scott was the 2nd president of The Ohio State University\nWilliam Henry Scott was the 3rd president of The Ohio State University\nJames Hulme Canfield was the 4th president of The Ohio State University\nWilliam Oxley Thompson was the 5th president of The Ohio State University\nGeorge Washington Rightmire was the 6th president of The Ohio State University\nWilliam McPherson was the 7th president of The Ohio State University\nHoward Landis Bevis was the 8th president of The Ohio State University\nNovice Gail Fawcett was the 9th president of The Ohio State University\nHarold Leroy Enarson was the 10th president of The Ohio State University\nEdward Harrington Jennings was the 11th president of The Ohio State University\nE. Gordon Gee was the 12th president of The Ohio State University\nJohn Richard Sisson was the 13th president of The Ohio State University\nWilliam English Kirwan was the 14th president of The Ohio State University\nEdward Harrington Jennings was the 15th president of The Ohio State University\nKaren Ann Holbrook was the 16th president of The Ohio State University\nJoseph A. Alutto was the 17th president of The Ohio State University\nE. Gordon Gee was the 18th president of The Ohio State University\nJoseph A. Alutto was the 19th president of The Ohio State University\nMichael V. Drake was the 20th president of The Ohio State University\nKristina M. Johnson was the 21st president of The Ohio State University\nPeter J. Mohler was the 22nd president of The Ohio State University\nWalter \"Ted\" Carter, Jr. was the 23rd president of The Ohio State University",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#building-lists-with-loops",
    "href": "python_basics.html#building-lists-with-loops",
    "title": "Mastering the Basics",
    "section": "Building lists with loops",
    "text": "Building lists with loops\nYou can create new lists from existing ones by using a for loop along with the .append() method. This is a common and effective way to filter or transform data in Python.\n\nExample: Finding U.S. States That Border the Pacific Ocean\nSuppose you have a list of U.S. states and want to create a new list containing only those that border the Pacific Ocean. You can do this by checking each state and appending the matching ones to a new list:\n\n#FIRST WE START BY ASSIGNING AN EMPTY LIST TO A VARIABLE\npacific_states=[ ]\n\n# NEXT WE NEED A LIST OF ALL THE STATES\nunited_states=[\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\n               \"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\n               \"Maryland\",\"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\n               \"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\n               \"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\n               \"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n\n#THEN WE USE A FOR LOOP WITH A CONDITIONAL STATEMENT TO FIND THE WESTERN STATES\nfor state in united_states:\n    if state==\"Alaska\" or state==\"California\" or state==\"Hawaii\" or state==\"Oregon\" or state==\"Washington\":\n        pacific_states.append(state)\n        \nprint(pacific_states)\n\n['Alaska', 'California', 'Hawaii', 'Oregon', 'Washington']\n\n\n\n\n\n\nExercise 8: Building lists\n\n\n\n\nCreate a list containing all states bordering the great lakes.\nunited_states=[\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\n            \"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\n            \"Maryland\",\"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\n            \"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\n            \"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\n            \"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n\n\n\n\n\n\nSolution:\n\n\n\n\ngreat_lakes_states=[]\nunited_states=[\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\n               \"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\n               \"Maryland\",\"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\n               \"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\n               \"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\n               \"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n\nfor state in united_states:\n    if state == 'Illinois' or state == 'Indiana' or state == 'Michigan' or state == 'Minnesota' or state == 'New York' or state == 'Ohio' or state =='Pennsylvania' or state == 'Wisconsin':\n        great_lakes_states.append(state)\n\nprint(great_lakes_states)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#counting-items-in-a-list",
    "href": "python_basics.html#counting-items-in-a-list",
    "title": "Mastering the Basics",
    "section": "Counting items in a list",
    "text": "Counting items in a list\nKnowing how many items are in a list is especially helpful when working with data‚Äîsuch as extracting information from a website or querying a database.\nIn Python, you can determine the number of elements in a list using the built-in len() function.\n\nlen(united_states)\n\n50",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#using-counter-variables",
    "href": "python_basics.html#using-counter-variables",
    "title": "Mastering the Basics",
    "section": "Using Counter Variables",
    "text": "Using Counter Variables\nWhile Python provides built-in tools like enumerate() and the .index() method to find the position of items in a list, there are times when using a counter variable can be a simple and effective alternative.\nA counter variable allows you to manually track the position of items as you loop through a list. This can be especially useful when you need more control over how positions are counted or displayed.\n\ncount=0\nfor state in united_states:\n    count +=1\n    print('Starting record '+str(count)+' '+state)\n\nStarting record 1 Alabama\nStarting record 2 Alaska\nStarting record 3 Arizona\nStarting record 4 Arkansas\nStarting record 5 California\nStarting record 6 Colorado\nStarting record 7 Connecticut\nStarting record 8 Delaware\nStarting record 9 Florida\nStarting record 10 Georgia\nStarting record 11 Hawaii\nStarting record 12 Idaho\nStarting record 13 Illinois\nStarting record 14 Indiana\nStarting record 15 Iowa\nStarting record 16 Kansas\nStarting record 17 Kentucky\nStarting record 18 Louisiana\nStarting record 19 Maine\nStarting record 20 Maryland\nStarting record 21 Massachusetts\nStarting record 22 Michigan\nStarting record 23 Minnesota\nStarting record 24 Mississippi\nStarting record 25 Missouri\nStarting record 26 Montana\nStarting record 27 Nebraska\nStarting record 28 Nevada\nStarting record 29 New Hampshire\nStarting record 30 New Jersey\nStarting record 31 New Mexico\nStarting record 32 New York\nStarting record 33 North Carolina\nStarting record 34 North Dakota\nStarting record 35 Ohio\nStarting record 36 Oklahoma\nStarting record 37 Oregon\nStarting record 38 Pennsylvania\nStarting record 39 Rhode Island\nStarting record 40 South Carolina\nStarting record 41 South Dakota\nStarting record 42 Tennessee\nStarting record 43 Texas\nStarting record 44 Utah\nStarting record 45 Vermont\nStarting record 46 Virginia\nStarting record 47 Washington\nStarting record 48 West Virginia\nStarting record 49 Wisconsin\nStarting record 50 Wyoming",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#list-comprehensions",
    "href": "python_basics.html#list-comprehensions",
    "title": "Mastering the Basics",
    "section": "List Comprehensions",
    "text": "List Comprehensions\nList comprehensions offer a compact way to create new lists by combining a for loop and an optional if condition‚Äîall in a single line of code.\nBasic Syntax\n\n[expression for item in iterable if condition]\n\nInstead of writing this longer version:\n\npacific_states=[]\nfor state in united_states:\n    if state==\"Alaska\" or state==\"California\" or state==\"Hawaii\" or state==\"Oregon\" or state==\"Washington\":\n        pacific_states.append(state)\n\nYou can simplify it using a list comprehension:\n\npacific_states = [state for state in united_states if state==\"Alaska\" or state==\"California\" or state==\"Hawaii\" or state==\"Oregon\" or state==\"Washington\"]\nprint(pacific_states)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#accessing-dictionary-data",
    "href": "python_basics.html#accessing-dictionary-data",
    "title": "Mastering the Basics",
    "section": "Accessing dictionary data",
    "text": "Accessing dictionary data\n\nKeys\nUse .keys() to get a list of all keys.\n\nuniversity_facts.keys()\n\ndict_keys(['institution_name', 'year_founded', 'school_colors', 'mascot', 'students', 'endowment'])\n\n\n\n\nValues\nUse .values() to get a list of all values.\n\nuniversity_facts.values()\n\ndict_values(['The Ohio State University', 1870, 'scarlet & grey', 'Brutus Buckeye', 60046, 7.9])\n\n\n\n\nAccess a value\nUse the key inside square brackets\n\nfirst_president = {\"name\":\"Edward Francis Baxter Orton Sr\", \n              \"tenure\":\"1873-1881\"}\n\nprint(first_president[\"name\"])\nprint(first_president[\"tenure\"])\n\nEdward Francis Baxter Orton Sr\n1873-1881",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#modifying-and-adding-data",
    "href": "python_basics.html#modifying-and-adding-data",
    "title": "Mastering the Basics",
    "section": "Modifying and adding data",
    "text": "Modifying and adding data\n\nAdd a new key-value pair\n\n\nfirst_president[\"rank\"] = \"first president\"\nfirst_president\n\n{'name': 'Edward Francis Baxter Orton Sr',\n 'tenure': '1873-1881',\n 'location': 'Columbus, Ohio',\n 'rank': 'first president'}\n\n\n\nStart with an empty dictionary:\nYou can begin with an empty dictionary and add key-value pairs as needed.\n\nstate_facts={}\nstate_facts[\"state\"]=\"Ohio\"\nstate_facts[\"nicknames\"]=[\"The Buckeye State\",\"Birthplace of Aviation\",\"The Heart of It All\"]\nstate_facts[\"capital\"]=\"Columbus\"\nstate_facts[\"population\"]=11785935\n\nstate_facts\n\n{'state': 'Ohio',\n 'nicknames': ['The Buckeye State',\n  'Birthplace of Aviation',\n  'The Heart of It All'],\n 'capital': 'Columbus',\n 'population': 11785935}\n\n\n\n\nModify an existing value\nTo update a value in a dictionary, use the dictionary name followed by the key in square brackets, then assign a new value using the equals sign (=).\n\nstate_facts[\"state\"]=\"Michigan\"\nstate_facts[\"nicknames\"]=[\"The Great Lake State\",\"The Wolverine State\",\"Water (Winter) Wonderland\"]\nstate_facts[\"capital\"]=\"Lansing\"\nstate_facts[\"population\"]=10077331\n\nstate_facts\n\n{'state': 'Michigan',\n 'nicknames': ['The Great Lake State',\n  'The Wolverine State',\n  'Water (Winter) Wonderland'],\n 'capital': 'Lansing',\n 'population': 10077331}",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#nesting-dictionaries",
    "href": "python_basics.html#nesting-dictionaries",
    "title": "Mastering the Basics",
    "section": "Nesting dictionaries",
    "text": "Nesting dictionaries\nDictionaries can contain other dictionaries, allowing for more complex data structures.\n\nstates={\n    \"state1\":{'name': 'Ohio',\n              'nicknames': ['The Buckeye State','Birthplace of Aviation','The Heart of It All'],\n              'capital': 'Columbus',\n              'population': 11785935},\n    \"state2\":{\n        'name': 'Michigan',\n        'nicknames': ['The Great Lake State','The Wolverine State','Water (Winter) Wonderland'],\n        'capital': 'Lansing',\n        'population': 10077331}\n    }\n\nstates\n\n{'state1': {'name': 'Ohio',\n  'nicknames': ['The Buckeye State',\n   'Birthplace of Aviation',\n   'The Heart of It All'],\n  'capital': 'Columbus',\n  'population': 11785935},\n 'state2': {'name': 'Michigan',\n  'nicknames': ['The Great Lake State',\n   'The Wolverine State',\n   'Water (Winter) Wonderland'],\n  'capital': 'Lansing',\n  'population': 10077331}}\n\n\n\nAccess a nested dictionary\nTo access the first state in the dictionary states, use the key for ‚Äústate1.\n\nstates[\"state1\"]\n\n{'name': 'Ohio',\n 'nicknames': ['The Buckeye State',\n  'Birthplace of Aviation',\n  'The Heart of It All'],\n 'capital': 'Columbus',\n 'population': 11785935}\n\n\n\n\nAccess a nested value\nTo access the nicknames*for the first state in the dictionary states, use the keys for state1 and nicknames.\n\nstates[\"state1\"][\"nicknames\"]\n\n['The Buckeye State', 'Birthplace of Aviation', 'The Heart of It All']",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#looping-through-a-dictionary",
    "href": "python_basics.html#looping-through-a-dictionary",
    "title": "Mastering the Basics",
    "section": "Looping through a dictionary",
    "text": "Looping through a dictionary\nPython provides several ways to iterate through the contents of a dictionary, depending on what you need to access: ### 1. Loop through key-value pairs Use the .items() method to access both keys and their corresponding values:\n\nfor key, value in states.items():\n    print(f\"\\nKey: {key}\")\n    print(f\"\\nValue: {value}\")\n\n\nKey: state1\n\nValue: {'name': 'Ohio', 'nicknames': ['The Buckeye State', 'Birthplace of Aviation', 'The Heart of It All'], 'capital': 'Columbus', 'population': 11785935}\n\nKey: state2\n\nValue: {'name': 'Michigan', 'nicknames': ['The Great Lake State', 'The Wolverine State', 'Water (Winter) Wonderland'], 'capital': 'Lansing', 'population': 10077331}\n\n\n\n2. Loop through keys only\nUse the .keys() method to iterate over just the keys.\n\nfor key in states.keys():\n    print(f\"Key: {key}\")\n\nKey: state1\nKey: state2\n\n\n\n\n3. Loop through the values only\nUse the .values() method to iterate over just the values.\n\nfor value in states.values():\n    print(f\"Value: {value}\")\n\nValue: {'name': 'Ohio', 'nicknames': ['The Buckeye State', 'Birthplace of Aviation', 'The Heart of It All'], 'capital': 'Columbus', 'population': 11785935}\nValue: {'name': 'Michigan', 'nicknames': ['The Great Lake State', 'The Wolverine State', 'Water (Winter) Wonderland'], 'capital': 'Lansing', 'population': 10077331}\n\n\nEach method gives you flexibility depending on whether you need the keys, the values, or both.",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#looping-through-a-nested-dictionary",
    "href": "python_basics.html#looping-through-a-nested-dictionary",
    "title": "Mastering the Basics",
    "section": "Looping through a nested dictionary",
    "text": "Looping through a nested dictionary\nWhen working with nested dictionaries‚Äîdictionaries that contain other dictionaries as values‚Äîyou can use a nested for loop to access the inner keys and values.\n\nfor outer_key, inner_dict in states.items():\n    for inner_key, inner_value in value.items():\n        print(f\"\\nKey: {inner_key}\")\n        print(f\"\\nValue: {inner_value}\")\n\n\nKey: name\n\nValue: Ohio\n\nKey: nicknames\n\nValue: ['The Buckeye State', 'Birthplace of Aviation', 'The Heart of It All']\n\nKey: capital\n\nValue: Columbus\n\nKey: population\n\nValue: 11785935\n\nKey: name\n\nValue: Michigan\n\nKey: nicknames\n\nValue: ['The Great Lake State', 'The Wolverine State', 'Water (Winter) Wonderland']\n\nKey: capital\n\nValue: Lansing\n\nKey: population\n\nValue: 10077331\n\n\n\nExplanation:\n\nThe first loop (for outer_key, inner_dict in states.items()) iterates through the outer dictionary.\nThe second loop (for inner_key, inner_value in inner_dict.items()) goes through each inner dictionary.\n\nThis approach allows you to access and work with all levels of data in a nested structure.\n\n\nExamples:\n\nfor state in states.keys():\n    print(state)\n\nstate1\nstate2\n\n\n\nfor state in states.values():\n    for each_state in state.keys():\n        print(each_state)\n\nname\nnicknames\ncapital\npopulation\nname\nnicknames\ncapital\npopulation\n\n\n\nfor state in states.values():\n    for each_state in state.values():\n        print(each_state)\n\nOhio\n['The Buckeye State', 'Birthplace of Aviation', 'The Heart of It All']\nColumbus\n11785935\nMichigan\n['The Great Lake State', 'The Wolverine State', 'Water (Winter) Wonderland']\nLansing\n10077331\n\n\n\nfor each_dictionary in states:\n    print(each_dictionary)\n\nstate1\nstate2\n\n\n\nfor each_dictionary in states:\n    print(states[each_dictionary])\n\n{'name': 'Ohio', 'nicknames': ['The Buckeye State', 'Birthplace of Aviation', 'The Heart of It All'], 'capital': 'Columbus', 'population': 11785935}\n{'name': 'Michigan', 'nicknames': ['The Great Lake State', 'The Wolverine State', 'Water (Winter) Wonderland'], 'capital': 'Lansing', 'population': 10077331}\n\n\n\n\n\n\nExercise 9: Dictionaries\n\n\n\n\nFind the diets for each crab species in the dictionary crabs.\n#From  Alaska Fish and Game website\ncrabs={\"Blue King\":\n       {\"scientific name\":\"Paralithodes platypus\",\n        \"size\":\"Up to 18 pounds for a mature male\",\n        \"range\":\"major concentrations primarily in Bering Sea\",\n        \"diet\":[\"worms\",\"clams\",\"mussels\",\"snails\",\"brittle stars\",\"sea stars\",\"sea urchins\",\"sand dollars\",\"barnacles\",\"crabs\",\"other crustaceans\",\"fish parts\",\"sponges\",\"algae\"],\n        \"predators\":[\"marine fishes\",\"king crab\",\"octopus\"]},\n        \"Dungeness\":\n       {\"scientific name\":\"Metacarcinus magister\",\n        \"size\":\"A legal-sized Dungeness crab is 6 1/2 inches in carapace width (shoulder width) and weighs approximately 2 pounds.\",\n        \"range\":\"Aleutian Islands to Magdalena Bay, Mexico\",\n        \"diet\":[\"worms\",\"small clams\", \"shrimp\",\"fish\"],\n        \"predators\":[\"humans\",\"sea otter\",\"octopus\",\"Pacific halibut\"]},\n        \"Golden King\":\n       {\"scientific name\":\"Lithodes aequispinus\",\n        \"size\":\"5-8 pounds.\",\n        \"range\":\"Aleutian Islands, Pribilof and Shumagin Islands, Prince William Sound, lower Chatham Strait\",\n        \"diet\":[\"worms\",\"clams\",\"mussels\",\"snails\", \"brittle stars\",\"sea stars\",\"sea urchins\",\"sand dollars\",\"barnacles\",\"crabs\",\"other crustaceans\",\"fish parts\",\"sponges\",\"algae\"],\n        \"predators\":[\"Pacific cod\",\"sculpins\",\"octopus\",\"halibut\",\"yellowfin sole\",\"other king crabs\",\"sea otters\",\"nemertean worms\"]},\n        \"Red King\":\n       {\"scientific name\":\"Paralithodes camtschaticus\",\n        \"size\":\"Females up to 10.5 lbs; Males up to 24 lbs and leg span of five feet\",\n        \"range\":\"British Columbia to Japan north to the Bering Sea with Bristol  Bay and Kodiak Archipelago being the centers of its abundance in Alaska\",\n        \"diet\":[\"worms\",\"clams\",\"mussels\",\"algae\",\"fish\",\"sea stars\",\"sand dollars\",\"brittle stars\"],\n        \"predators\":[\"Pacific cod\",\"walleye pollock\",\"rock sole\",\"flathead sole\",\"rex sole\",\"Dover sole\",\"arrowtooth flounder\",\"Elasmobranchs\",\"halibut\",\"sculpin\",\"Greenland turbot\",\"Pacific salmon\",\"Pacific herring\",\"otters\",\"seals\"]},\n        \"Tanner\":\n       {\"scientific name\":\"Chionoecetes bairdi and C. opilio\",\n        \"size\":\"Mature males typically weigh 1-2 pounds for opilio and 2-4 pounds for bairdi\",\n        \"range\":\"North Pacific Ocean and Bering Sea\",\n        \"diet\":[\"fish\",\"shrimp\",\"crabs\",\"worms\",\"clams\",\"brittle stars\",\"snails\",\"algae\",\"sponges\"],\n        \"predators\":[\"seals\",\"sea otters\",\"octopi\",\"other crabs\",\"fish\"]}\n       }\n\n\n\n\n\n\nSolution:\n\n\n\n\n#From  Alaska Fish and Game website\n\ncrabs={\"Blue King\":\n       {\"scientific name\":\"Paralithodes platypus\",\n        \"size\":\"Up to 18 pounds for a mature male\",\n        \"range\":\"major concentrations primarily in Bering Sea\",\n        \"diet\":[\"worms\",\"clams\",\"mussels\",\"snails\",\"brittle stars\",\"sea stars\",\"sea urchins\",\"sand dollars\",\"barnacles\",\"crabs\",\"other crustaceans\",\"fish parts\",\"sponges\",\"algae\"],\n        \"predators\":[\"marine fishes\",\"king crab\",\"octopus\"]},\n       \"Dungeness\":\n       {\"scientific name\":\"Metacarcinus magister\",\n        \"size\":\"A legal-sized Dungeness crab is 6 1/2 inches in carapace width (shoulder width) and weighs approximately 2 pounds.\",\n        \"range\":\"Aleutian Islands to Magdalena Bay, Mexico\",\n        \"diet\":[\"worms\",\"small clams\", \"shrimp\",\"fish\"],\n        \"predators\":[\"humans\",\"sea otter\",\"octopus\",\"Pacific halibut\"]},\n      \"Golden King\":\n       {\"scientific name\":\"Lithodes aequispinus\",\n        \"size\":\"5-8 pounds.\",\n        \"range\":\"Aleutian Islands, Pribilof and Shumagin Islands, Prince William Sound, lower Chatham Strait\",\n        \"diet\":[\"worms\",\"clams\",\"mussels\",\"snails\", \"brittle stars\",\"sea stars\",\"sea urchins\",\"sand dollars\",\"barnacles\",\"crabs\",\"other crustaceans\",\"fish parts\",\"sponges\",\"algae\"],\n        \"predators\":[\"Pacific cod\",\"sculpins\",\"octopus\",\"halibut\",\"yellowfin sole\",\"other king crabs\",\"sea otters\",\"nemertean worms\"]},\n      \"Red King\":\n       {\"scientific name\":\"Paralithodes camtschaticus\",\n        \"size\":\"Females up to 10.5 lbs; Males up to 24 lbs and leg span of five feet\",\n        \"range\":\"British Columbia to Japan north to the Bering Sea with Bristol  Bay and Kodiak Archipelago being the centers of its abundance in Alaska\",\n        \"diet\":[\"worms\",\"clams\",\"mussels\",\"algae\",\"fish\",\"sea stars\",\"sand dollars\",\"brittle stars\"],\n        \"predators\":[\"Pacific cod\",\"walleye pollock\",\"rock sole\",\"flathead sole\",\"rex sole\",\"Dover sole\",\"arrowtooth flounder\",\"Elasmobranchs\",\"halibut\",\"sculpin\",\"Greenland turbot\",\"Pacific salmon\",\"Pacific herring\",\"otters\",\"seals\"]},\n       \"Tanner\":\n       {\"scientific name\":\"Chionoecetes bairdi and C. opilio\",\n        \"size\":\"Mature males typically weigh 1-2 pounds for opilio and 2-4 pounds for bairdi\",\n        \"range\":\"North Pacific Ocean and Bering Sea\",\n        \"diet\":[\"fish\",\"shrimp\",\"crabs\",\"worms\",\"clams\",\"brittle stars\",\"snails\",\"algae\",\"sponges\"],\n        \"predators\":[\"seals\",\"sea otters\",\"octopi\",\"other crabs\",\"fish\"]}\n       }\n\ncrab_diet=[]\ncrab_diet_nested={}\n\nfor key, value in crabs.items():\n    crab=key\n    diet=value['diet']\n    for food in diet:\n        if food == \"worms\":\n            eats_worms=\"yes\"\n        else:\n            eats_worms=\"no\"\n            \n    ##Create nested dictionary try 1\n    new_crabs_dictionary={crab:\n                          {\"eats worms\": eats_worms}\n                            }\n    ##Creates a list of dictionaries\n    crab_diet.append(new_crabs_dictionary)\n    \n    ##Create nested dictionary try 2\n    if crab not in crab_diet_nested.keys():\n         crab_diet_nested[crab]={\"eats worms\":eats_worms}\n\nprint(f\"crab diet list \")\nprint(crab_diet) \nprint(f\"crab diet nested = \")\nprint(crab_diet_nested)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#what-is-a-function",
    "href": "python_basics.html#what-is-a-function",
    "title": "Mastering the Basics",
    "section": "What is a function?",
    "text": "What is a function?\nA function is a reusable block of code designed to perform a particular task. Functions help make your code more organized, readable, and efficient.",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#defining-a-function",
    "href": "python_basics.html#defining-a-function",
    "title": "Mastering the Basics",
    "section": "Defining a function",
    "text": "Defining a function\nTo define a function, use the def keyword, followed by the function name, parentheses (), and a colon :. The code inside the function is indented.\n\ndef say_hello():\n    print(\"Hello\")",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#calling-a-function",
    "href": "python_basics.html#calling-a-function",
    "title": "Mastering the Basics",
    "section": "Calling a function",
    "text": "Calling a function\nTo call or execute a function, simply write its name followed by parentheses.\n\nsay_hello()\n\nHello",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#arguments-and-parameters",
    "href": "python_basics.html#arguments-and-parameters",
    "title": "Mastering the Basics",
    "section": "Arguments and parameters",
    "text": "Arguments and parameters\nWhen defining a function, you can include parameters inside the parentheses. These parameters act as placeholders for values that will be passed into the function when it‚Äôs called. This allows your function to work dynamically with different inputs.\nIn this example:\n\nname is a parameter‚Äîa variable defined in the function header.\nWhen the function is called, you provide an argument‚Äîa specific value that replaces the parameter.\n\n\ndef say_hello(name):\n    print(f\"Hello, {name}\")\n\n\nCalling the function with an argument\n\nsay_hello(\"Sarah\")\n\nHello, Sarah\n\n\nThis makes your function flexible and reusable with different inputs.\n\n\nParameters vs.¬†arguments\n\nA parameter is a variable listed inside the parentheses when defining a function. It acts as a placeholder for the value the function will receive.\n\n\ndef function_name(parameter):\n    # function body\n\n\nAn argument is the actual value you pass into the function when calling it.\n\n\n\nfunction_name(argument)\n\n\nIf a function is defined with a parameter, you must provide a corresponding argument when calling the function‚Äîotherwise, Python will raise an error.\n\n\n\nPositional vs.¬†keyword arguments\nWhen calling a function in Python, you can pass arguments in two main ways: positional and keyword.\n\n1. Positional arguments\nBy default, Python matches arguments to parameters based on their position in the function call. The first argument is assigned to the first parameter, the second to the second, and so on.\n\ndef describe_pet(animal_type, pet_name):\n    print(f\"\\nI have a {animal_type}\")\n    print(f\"\\nMy {animal_type}'s name is {pet_name.title()}.\")\n\n\n#FUNCTION CALLED WITH CORRECT POSITIONAL ARGUMENTS    \ndescribe_pet('dog','stanley') \n\n\nI have a dog\n\nMy dog's name is Stanley.\n\n\n\n#FUNCTION CALLED WITH INCORRECT POSITIONAL ARGUMENTS    \ndescribe_pet('stanley','dog') \n\n\nI have a stanley\n\nMy stanley's name is Dog.\n\n\n\n\n2. Keyword arguments\nWith keyword arguments, you explicitly specify which value goes to which parameter using the parameter=value syntax. This removes the dependency on the order of arguments.\n\ndef describe_pet(pet_name=\"name\",animal_type='species'):\n    print(f\"I have a {animal_type}\")\n    print(f\"My {animal_type}'s name is {pet_name.title()}.\")\n\npet_description=describe_pet(pet_name='Charlotte', animal_type='spider')\n\nI have a spider\nMy spider's name is Charlotte.\n\n\n\n\n\nReturn Values from Functions\nSo far, we‚Äôve used print() to display output from functions. However, functions can also return values using the return statement. This allows you to store the result and use it later.\n\ndef format_name(first_name, last_name):\n    full_name=f\"{first_name} {last_name}\"\n    return full_name.title()\n\nname=format_name('sarah','murphy')\nprint(name)\n\nSarah Murphy\n\n\nReturning values is useful when you want to process data and use the result elsewhere in your program.\n\n\n\n\nExercise 10: Functions\n\n\n\n\nMake a function that transforms a book title to lower case, then call your function.\n#define your function first. Give it a name and assign a parameter.\n    #your code block here\n    return #your code here\n\n\n\n\n\n\nSolution:\n\n\n\n\ndef lowercase(title):\n    lowercase_title=title.lower()\n    return lowercase_title\n\n\nbook=\"Loansome Dove\"\nlowercase(book)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#syntaxerror",
    "href": "python_basics.html#syntaxerror",
    "title": "Mastering the Basics",
    "section": "üî¥ SyntaxError",
    "text": "üî¥ SyntaxError\nExample messages: - SyntaxError: EOL while scanning string literal - SyntaxError: invalid syntax\nThis error usually means:\n\nYou forgot a colon : at the end of a control structure like a for loop or if statement.\nYou left a string unclosed (missing a quotation mark).\n\n\n#‚ùå Missing closing quote\nprint(\"It's a wonderful day!)\n\n\ntrees=[\"maple\",\"walnut\",\"oak\"]\n#‚ùå Missing colon in for loop\nfor tree in trees\n    print(tree)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#filenotfounderror",
    "href": "python_basics.html#filenotfounderror",
    "title": "Mastering the Basics",
    "section": "üìÅ FileNotFoundError",
    "text": "üìÅ FileNotFoundError\nExample message: - FileNotFoundError: [Errno 2] No such file or directory: 'sample_file.txt'\nThis means Python can‚Äôt find the file you‚Äôre trying to open. Double-check:\n\nThe file name and path\nUse / instead of ¬†in file paths on some systems\n\n\n#‚ùå File does not exist in the specified path\nopen('sample_file.txt').read()",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#typeerror",
    "href": "python_basics.html#typeerror",
    "title": "Mastering the Basics",
    "section": "üî¢ TypeError",
    "text": "üî¢ TypeError\nExample message: - TypeError: can only concatenate str (not \"int\") to str\nThis happens when you try to perform an operation on incompatible data types.\n\n#‚ùå Cannot add a string and an integer\nname=\"Sarah\"\nname+8\n\n#‚úÖ Check data type type(name)\n\n#‚úÖ Convert int to str before concatenation\nname+str(8)\n\n\n\nTip:\n\n\nWhen writing print functions, avoid a TypeError by using f-strings.\n\n\n\n#‚ùå Cannot add a string and an integer\ndog=\"Jack\"\nage=8\n\nprint(dog + \" is \" + age) \n\n#‚úÖ Use f-strings to avoid TypeErrors\nprint(f\"{dog} is {age}\")",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#nameerror",
    "href": "python_basics.html#nameerror",
    "title": "Mastering the Basics",
    "section": "‚ùì NameError",
    "text": "‚ùì NameError\nExample message: - NameError: name 'Name' is not defined\nThis means Python can‚Äôt find the variable or function you‚Äôre trying to use. Common causes:\n\nTypos\nUsing a variable before defining it\n\n\n#‚ùå Variable 'Name' is not defined\nName\n\n\n\nTip:\n\n\n‚úÖ Use lowercase consistently for variable names to avoid confusion.",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#attributeerror",
    "href": "python_basics.html#attributeerror",
    "title": "Mastering the Basics",
    "section": "‚öôÔ∏è AttributeError",
    "text": "‚öôÔ∏è AttributeError\nExample message: - AttributeError: 'list' object has no attribute 'enumerate'\nThis occurs when you try to use a method that doesn‚Äôt exist for a particular data type.\n\n##‚ùå Incorrect: enumerate used as a method\ntrees=[\"maple\",\"walnut\",\"oak\"]\nfor tree in trees.enumerate():\n    print(tree)\n\n##‚úÖ Correct: enumerate used as a function\ntrees=[\"maple\",\"walnut\",\"oak\"]\nfor tree in enumerate(trees):\n    print(tree)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#method-1-using-the-csv-module",
    "href": "python_basics.html#method-1-using-the-csv-module",
    "title": "Mastering the Basics",
    "section": "Method 1: Using the csv module",
    "text": "Method 1: Using the csv module\n\nimport csv\n\n#####     STEP 1 - CREATE EMPTY DATASET AND DEFINE CSV HEADINGS     ##### \ndataSet=[]\ncolumns=['institution_name','year_founded','school_colors','mascot','students','endowment'] # for CSV headings\n\n#####     STEP 2 - DEFINE FUNCTION TO WRITE RESULTS TO CSV FILE     #####\n\n\ndef writeto_csv(data,filename,columns):\n    with open(filename,'w+',newline='',encoding=\"UTF-8\") as file:\n        writer = csv.DictWriter(file,fieldnames=columns)\n        writer.writeheader()\n        writer = csv.writer(file)\n        for element in data:\n            writer.writerows([element])\n\n#####     STEP 3 - ADD A ROW OF DATA     #####\ninstitution_name= \"The Ohio State University\" \nyear_founded = 1870\nschool_colors = \"scarlet & grey\"\nmascot = \"Brutus Buckeye\"\nstudents = 60046\nendowment = 7.9\n\ndataSet.append([institution_name,year_founded,school_colors,mascot,students,endowment])\n\n#####     STEP 4 - EXPORT TO CSV     #####\nwriteto_csv(dataSet,'data/university_facts.csv',columns)",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "python_basics.html#method-2-using-pandas",
    "href": "python_basics.html#method-2-using-pandas",
    "title": "Mastering the Basics",
    "section": "Method 2: Using Pandas",
    "text": "Method 2: Using Pandas\n\n#####     STEP 1 - CREATE DATAFRAME TO STORE ROWS     #####\nresults=pd.DataFrame(columns=['institution_name','year_founded','school_colors','mascot','students','endowment'])\n\n#####     STEP 2 - DEFINE EACH ROW AS A DICTIONARY    #####\nuniversity_facts = {\"institution_name\": \"The Ohio State University\", \n                    \"year_founded\" : 1870, \n                    \"school_colors\" : \"scarlet & grey\", \n                    \"mascot\" : \"Brutus Buckeye\", \n                    \"students\" : 60046, \n                    \"endowment\": 7.9 }\n\n#####     STEP 3 - CONVERT EACH DICTIONARY ROW TO A DATAFRAME    #####\nadd_row_to_results=pd.DataFrame(university_facts, index=[0])\n\n#####     STEP 4 - CONCATENATE RESULTS EACH ROW TO RESULTS DATAFRAME    #####\nresults=pd.concat([add_row_to_results, results], axis=0, ignore_index=True)\n\n#####     STEP 5 - EXPORT THE RESULTS DATAFRAME TO A CSV FILE    #####\nresults.to_csv('data/university_facts.csv', encoding='utf-8')",
    "crumbs": [
      "Python Tutorials",
      "INTRODUCTION",
      "Mastering the Basics"
    ]
  },
  {
    "objectID": "crossref.html",
    "href": "crossref.html",
    "title": "Lesson 7. Crossref",
    "section": "",
    "text": "Crossref is a nonprofit organization that manages a registry of Digital Object Identifiers (DOIs). Publishers collaborate with Crossref to assign a unique DOI to each journal article, book, conference paper, or dataset they publish. This DOI acts like a permanent web address, enabling seamless linking between references, citations, research outputs, funding information, and more.\nThe Crossref REST API offers free access to the nonprofit‚Äôs metadata. This tutorial introduces two useful tools: JSON, a simple data format that resembles Python dictionaries and is easy to read and use, and Python‚Äôs built-in logging module.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 7. Crossref"
    ]
  },
  {
    "objectID": "crossref.html#data-skills-concepts",
    "href": "crossref.html#data-skills-concepts",
    "title": "Lesson 7. Crossref",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nAPIs\nlogging\nJSON data",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 7. Crossref"
    ]
  },
  {
    "objectID": "crossref.html#learning-objectives",
    "href": "crossref.html#learning-objectives",
    "title": "Lesson 7. Crossref",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nInterpret documentation and apply concepts to write functional code.\nExtract and work with JSON data using Python‚Äôs built-in tools.\nUse Python‚Äôs logging module to capture and report errors that interrupt code execution.\n\nThis tutorial is designed to support multi-session workshops hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts visit the Python - Mastering the Basics tutorial.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 7. Crossref"
    ]
  },
  {
    "objectID": "crossref.html#crossref",
    "href": "crossref.html#crossref",
    "title": "Lesson 7. Crossref",
    "section": "Crossref",
    "text": "Crossref\nCrossref provides detailed documentation and a wide range of robust learning resources to help users effectively work with its REST API.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 7. Crossref"
    ]
  },
  {
    "objectID": "crossref.html#json",
    "href": "crossref.html#json",
    "title": "Lesson 7. Crossref",
    "section": "JSON",
    "text": "JSON\nCrossref queries return data in JSON format, which is easy to read and looks similar to Python dictionaries. You can work with JSON data by looping through its key-value pairs to access the information you need.\n\n\n\n\nExercise 1: Crossref API\n\n\n\n\n\nRead through the Crossref REST API documentation. Then ‚Ä¶\n\n\n\nRead data/dois.csv into a Pandas DataFrame\n\n\nUse the Crossref works API to gather the following fields for each DOI:\n\n\npublisher\n\n\narticle_title\n\n\njournal_title\n\n\njournal_abbr\n\n\nyear\n\n\nreference count\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport requests\nimport pandas as pd\n\ndef lookup(target_doi):\n    base_url='https://api.crossref.org/works/'\n    url=base_url+target_doi\n    response=requests.get(url)\n    response.raise_for_status() #Raise an HTTP Error for bad responses\n    json_data = response.json() #Parse JSON response\n    return json_data\n\nfile=pd.read_csv('C:/Users/murphy.465/Documents/GitHub/data_visualization/data/dois.csv')\ndois=file.doi.tolist()\nresults=pd.DataFrame(columns=['doi','publisher','article_title','journal_title','year','reference_count'])\n\nfor doi in dois:\n    data={}\n    response=lookup(doi)\n    entry=response['message']\n    data['doi']=doi\n    data['publisher']=entry['publisher']\n    data['article_title']=entry['title'][0]\n    data['journal_title']=entry['container-title'][0]\n    data['year']=entry['published']['date-parts'][0][0]\n    data['reference_count']=entry['reference-count']\n    row=pd.DataFrame(data, index=[0])\n    results=pd.concat([row,results], axis=0, ignore_index=True)",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 7. Crossref"
    ]
  },
  {
    "objectID": "crossref.html#logging",
    "href": "crossref.html#logging",
    "title": "Lesson 7. Crossref",
    "section": "Logging",
    "text": "Logging\nAPIs sometimes return error codes which interrupt our program‚Äôs execution. Logging tells Python how to handle these errors. It can also help to identify issues with your code.\n\n\nTip - Copilot\n\n\n\nAsk Copilot how to handle exceptions in logging module. Copilot will return code you can modify for your project and provide additional tips.\n\n\n\n\n\n\n\n\nExercise 2: Handling exceptions\n\n\n\n\nModify code from Exercise 1 to add a function that logs and handles HTTP Errors for bad responses.\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport requests\nimport pandas as pd\nimport logging\nimport time\n\n#  Configure logging\nformatstring=\"%(asctime)s - %(levelname)s - %(message)s\"\ndatestring=\"%m/%d/%Y %I%M%S %p\"\nlogging.basicConfig(filename=\"cr_errors_find_dois.log\", level=logging.ERROR, format=formatstring, datefmt=datestring)\n\n# Define function to request url and log HTTP errors\ndef lookup(target_doi):\n    try:\n        base_url='https://api.crossref.org/works/'\n        url=base_url+target_doi\n        response=requests.get(url)\n        response.raise_for_status() #Raise an HTTP Error for bad responses\n        json_data = response.json() #Parse JSON response\n        return json_data\n    except requests.exceptions.HTTPError as http_err:\n        logging.error(f\"HTTP Error = {http_err}\") # Log the HTTP error\n        time.sleep(10)\n    except Exception as err:\n        logging.error(f\"Other error = {err}\") #Log any other errors\n        time.sleep(10)\n        \nfile=pd.read_csv('C:/Users/murphy.465/Documents/GitHub/data_visualization/data/dois.csv')\ndois=file.doi.tolist()\nresults=pd.DataFrame(columns=['doi','publisher','article_title','journal_title','year','reference_count'])\n\nfor doi in dois[0:2]:\n    data={}\n    response=lookup(doi)\n    entry=response['message']\n    data['doi']=doi\n    data['publisher']=entry['publisher']\n    data['article_title']=entry['title'][0]\n    data['journal_title']=entry['container-title'][0]\n    data['year']=entry['published']['date-parts'][0][0]\n    data['reference_count']=entry['reference-count']\n    row=pd.DataFrame(data, index=[0])\n    results=pd.concat([row,results], axis=0, ignore_index=True)",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 7. Crossref"
    ]
  },
  {
    "objectID": "r_language.html",
    "href": "r_language.html",
    "title": "R",
    "section": "",
    "text": "R is a free, open-source programming language designed for statistical computing and widely used across various academic disciplines. It features a rich ecosystem of packages for data cleaning and visualization, including popular tools like ggplot2 and Plotly, making it a powerful choice for data analysis and research.\n\nInstall R\n\nDownload and install the R programming language.\nDownload and install R Studio\nDownload and install Tidyverse packages\n\n\n\nWebsites\n\nR Studio Education\nGetting Help with R\n\n\n\nBooks\n\n\n\n\nR FOR DATA SCIENCE\n\n\n\n\nR for Data Science, 2nd edition\n\n\n\n\nby Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund\n\n\nSebastopol, CA : O‚ÄôReilly Media, 2023.\n\n\n\n\n\n\n\nO‚ÄôReilly Online Learning\nFor additional books and learning materials, the O‚ÄôReilly Online Learning: Academic/Public Library Edition collection provides extensive access to eBooks and videos in computer science, IT, business, and related subjects, featuring content from O‚ÄôReilly and other top publishers. This resource is provided by University Libraries and is available to all Ohio State faculty, students, and staff with a valid osu.edu email address.\n\n\nWorkshops and Events\nOSU Code Club supports learning the R language and welcomes anyone who studies or works at The Ohio State University to join.",
    "crumbs": [
      "Library Resources",
      "TOOLS",
      "... R"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html",
    "href": "tableau_research_read_interpret_evaluate.html",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "",
    "text": "Lesson 2 introduced basic charts for comparing categorical values, visualizing trends over time, and exploring relationships within data. It also explored the distinction between discrete and continuous dates in Tableau and introduced text tables and callout numbers to emphasize key insights. This lesson introduces additional basic charts and techniques.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html#data-skills-concepts",
    "href": "tableau_research_read_interpret_evaluate.html#data-skills-concepts",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nTableau\nWorking with data\nAnalyzing data",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html#learning-objectives",
    "href": "tableau_research_read_interpret_evaluate.html#learning-objectives",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nCreate groups to normalize data, correct errors, and simplify categories.\nApply hierarchies for better data organization.\nUse formatting strategically to enhance visual impact.\nAnalyze data with table calculations.\nVisualize part-to-whole relationships effectively.\n\nThis tutorial is designed to support a multi-session Tableau for Research workshop hosted by The Ohio State University Libraries Research Commons. It is intended to help the ABSOLUTE beginner, or anyone who is relatively new to Tableau to build the skills and confidence to apply Tableau to research projects.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html#inspiration-and-examples",
    "href": "tableau_research_read_interpret_evaluate.html#inspiration-and-examples",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "Inspiration and examples",
    "text": "Inspiration and examples\n\nTableau Public Viz of the Day\nFlowingData",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html#explore-data-journalism",
    "href": "tableau_research_read_interpret_evaluate.html#explore-data-journalism",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "Explore data journalism",
    "text": "Explore data journalism\nMajor news outlets often feature compelling data visualizations created by professional data journalists. Check out examples from: - CNN - The New York Times - The Washington Post",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html#why-group-data-in-tableau",
    "href": "tableau_research_read_interpret_evaluate.html#why-group-data-in-tableau",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "Why group data in Tableau?",
    "text": "Why group data in Tableau?\nGrouping data is a powerful way to clean and organize your dataset. It allows you to:\n\nNormalize values ‚Äì Combine similar entries to ensure consistency (e.g., merging ‚ÄúOH‚Äù and ‚ÄúOhio‚Äù).\nCorrect errors ‚Äì Fix inconsistencies or typos in category names.\nSimplify categories ‚Äì Consolidate detailed values into broader, more meaningful groups.\n\n\n\nMultiple ways to accomplish a task!!!\n\n\n\n\nGroups are another great example where there more than one way to accomplish the same task!!!\n\n\n\nThe simpliest way to create a group in Tableau is to:\n\nSelect one or more data points directly in the view.\nClick the paperclip icon üìé that appears in the tooltip.\n\n\n\n\n\nExercise 1. Group data\n\n\n\n\n\nCreate a text table showing the peak US chart positions for albums by five of your favorite artists.\n\n\n\nStart a new worksheet and rename it FavoriteArtistAlbum\n\n\nSelect the relevant fields\n\n\nDrag Artist to the Rows shelf.\n\n\nPlace Peak US on the Columns shelf.\n\n\n\n\nOpen Show Me and select Text Tables.\n\n\nSelect your top 5 favorite artists\n\n\nClick on your first favorite artist.\n\n\nHold Ctrl (or Cmd on Mac) + Click  to select the others.\n\n\n\n\nClick the paperclip icon üìé on the tooltip for the last artist selected\n\n\nEdit group alias\n\n\nRight-click the first artist.\n\n\nSelect Edit Alias.\n\n\nRename to Favorite artists.\n\n\n\n\nRight-click Favorite artists and choose ‚úî Keep only\n\n\nAdjust fields\n\n\nDrag Artist to the Rows shelf.\n\n\nDrag album_title to the Rows shelf.\n\n\nRemove Favorite artists from the Rows shelf\n\n\n\n\nRename group\n\n\nRight-click Artist (group) 1 in the Data Pane.\n\n\nSelect Rename and type Favorite artists.\n\n\n\n\n\n\n\n\n\nVideo showing steps 1-9 in exercise 1\n\n\n\nA second way to create a group in Tableau is to:\n\nRight-click a dimension or measure in the Data Pane\nSelect Create &gt; Group.\n\n\n\n\n\nExercise 2. Group data\n\n\n\n\n\nOn the FavoriteArtistAlbum sheet, use the Data Pane to group 10 of your Favorite albums into a single group.\n\n\n\nRight-click album_title and select Create &gt; Group\n\n\nRename the group Favorite albums\n\n\nFind and add albums to the group\n\n\nClick Find &gt;&gt; to search for the first album title.\n\n\nUnder Find members, type the name of your first favorite album title and click Find All.\n\n\nWhen the album appears in the list, select it and click Group\n\n\nAlias the group Favorite albums\n\n\nRepeat the process for each additional album\n\n\nType the album name under Find members\n\n\nClick Find All\n\n\nSelect the album\n\n\nUse the Add to: dropdown to add each additional album to Favorite albums.\n\n\n\n\n\n\n‚úî Include ‚ÄòOther‚Äô to consolidate non-favorite titles\n\n\n\n\n\n\n\nVideo showing steps 1-4 in exercise 2",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html#creating-custom-hierarchies",
    "href": "tableau_research_read_interpret_evaluate.html#creating-custom-hierarchies",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "Creating custom hierarchies",
    "text": "Creating custom hierarchies\nCustom hierarchies help save space on a view and add interactivity by allowing users to drill down into more detailed levels of data. In Lesson 2, you saw how Tableau automatically creates a date hierarchy‚ÄîYEAR, QUARTER, MONTH, and DAY‚Äîwhich can be expanded using the + symbol.\nYou can create your own hierarchies by stacking one dimension onto another in the Data Pane (or one measure onto another).\nLet‚Äôs walk through an example:\n\nDuplicate the existing sheet\n\nRight-click the FavoriteAristsAlbums sheet tab.\nSelect Duplicate.\nRename the new sheet Hierarchies.\n\nCreate a hierarchy\n\nIn the Data Pane drag album_title and drop it onto Artists.\n\nFilter to show only your favorite albums\n\nDrag Favorite albums group to the Filters shelf.\nSelect Favorite albums only.\n\nExpand or collapse the hierarchy\n\nClick the + or - next to Artist to expand or collapse the hierarchy.\n\n\n\nVideo showing steps 1-4 in example above",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html#applying-formatting-in-tableau",
    "href": "tableau_research_read_interpret_evaluate.html#applying-formatting-in-tableau",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "Applying formatting in Tableau",
    "text": "Applying formatting in Tableau\n\nFormat the entire workbook:\n\nGo to the Format menu and select Workbook. This allows you to apply consistent styles across all sheets.\n\n\n\nFormat an individual worksheet:\n\nUse the Format pane by selecting Font, Alignment, Shading, Borders, or Lines from the Format toolbar.\nAlternatively, right-click on a pill and choose Format.\n\nWhen formatting from a pill, the field name appears at the top of the Format pane. Changes made here apply only to the axis or headers for that specific field. If you close the Format pane and reopen it using the toolbar, the default view applies formatting to the entire sheet. In this case, no field name appears at the top, and you can manually select a dimension or measure from the Fields menu.\n\n\n\n\nExercise 3: Format BarChart1\n\n\n\n\n\nPrepare BarChart1 as a clear, engaging figure for an academic paper, follow these steps:\n\n\n\nEnsure full visibility of Artist names on the X-axis\n\n\nHover over the X-axis line until the until the ‚Üï (up-down arrow) appears.\n\n\nClick and drag the line upward to increase the height of the axis area.\n\n\nHover over the right edge of the Artist name Bad Company until the ‚ÜîÔ∏é (left-right arrow) appears.\n\n\nClick and drag to the right to expand the column header, ensuring the full name is visible.\n\n\n\n\nAdjust bar width\n\n\nOn the Marks Card click Size.\n\n\nDrag the slider to the left to decrease the width of the horizontal bars.\n\n\n\n\nRemove redundant field label\n\n\nSince the artist names are already shown in the column headers, the field label is unnecessary.\n\n\nRight-click Artist in view and select Hide Field Labels for Columns.\n\n\n\n\nRemove unnecessary lines\n\n\nFrom the Format toolbar, select Lines.\n\n\nIn the Formatting Pane, go to the Rows tab and set Grid Lines to None.\n\n\nSwitch to the Sheet tab and set Axis Rulers and Zero Lines to None.\n\n\n\n\nCustomize Y-Axis Label and Tick Marks\n\n\nRight-click on Y-Axis and select Edit Axis.\n\n\nRename the axis title to Peak US Chart Position (Average).\n\n\nClick the Tick Marks tab, set Major Tick Marks to Fixed and define the interval as 15.\n\n\nClose the dialog.\n\n\n\n\nExperiment with removing Y-axis\n\n\nCopy AVG(Peak US) to Label on Marks Card.\n\n\nClick Format on the AVG(Peak US) pill to open the Formatting Pane.\n\n\nUnder the Pane tab, set Numbers to Number (Custom) and reduce decimal places to 0.\n\n\nRight-click the Y-axis and uncheck Show Header to hide it.\n\n\nTo restore: click the ‚ñº caret on the AVG(Peak US) pill and select Show Header.\n\n\n\n\nAdjust font for clarity\n\n\nIn the Formatting Pane, select Font.\n\n\nFrom the Fields ‚ñº dropdown, choose AVG(Peak US) and ‚Ä¶\n\n\nIncrease the font size from 9pt to 12pt.\n\n\nMatch font color to the bar color and apply Bold.\n\n\nThen select Artist from the same dropdown and set the default font color to match the bar color.\n\n\n\n\nAdd a descriptive title\n\n\nDouble click the worksheet title Barchart1.\n\n\nOn the first line enter: Average Peak US Chart Position\n\n\nOn the second line enter:  025 Rock n Roll Hall of Fame Inductees\n\n\nAdjust font sizes and styling as needed. \n\n\n\n\nAdd a Zero Line\n\n\nFrom the Format toolbar, select Lines.\n\n\nOn the Rows tab, set a solid, thick, dark gray zero line.\n\n\n\n\nExport the visualization for publication\n\n\nGo to the Worksheet toolbar and select Export &gt; Image.\n\n\nUncheck all options except Title and View.\n\n\nSave the image as Scalable Vector Graphics (*.svg) for high-quality print use.\n\n\n\n\n\n\n\n\n\n\nSolution:",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "tableau_research_read_interpret_evaluate.html#part-to-whole-relationships",
    "href": "tableau_research_read_interpret_evaluate.html#part-to-whole-relationships",
    "title": "Lesson 3. Read, Interpret, and Evaluate Data",
    "section": "Part-to-Whole Relationships",
    "text": "Part-to-Whole Relationships\n\nPie chart\nPie charts are commonly used to illustrate how individual parts contribute to a whole. However, data visualization experts often discourage their use because humans struggle to accurately compare angles and areas, making interpretation less precise than other chart types.\nThat said, there are situations where a pie chart may be appropriate‚Äîsuch as when the audience expects it or when it provides a clear and immediate visual impact.\nLet‚Äôs return to our rock_n_roll_performers dataset to explore the pie chart.\n\nStart a new worksheet and rename it PieChart\nSet the active data source\n\nOn the Data Pane, select rock_n_roll_performers.\nThe fields for the rock_n_roll_performers and rock_n_roll_studio_albums should appear.\n\nFilter by 2025 inductees\n\nChange Year to Date & Time data type (if not already).\nFilter for the Discrete Year: 2005.\n\nCreate the pie chart\n\nOn the Marks Card change the marks type to Pie.\n\nColor the pie with a dimension\n\nOn the Marks Card, drag Artist to Color.\nIn the Toolbar, change the view from Standard to Entire View.\n\nSize the slices with a measure\n\nOn the Marks Card, drag rock_n_roll_studio_albums.csv (Count) to Size\n\nAdd a descriptive title\n\nDouble click the worksheet title PieChart.\nEnter:\n\nNumber of Albums Released by 2025 Inductees\n\n\nEnhance clarity\n\nCopy Artist and rock_n_roll_studio_albums.csv (Count) to Label on the Marks Card.\nClick Label on the Marks Card and select the ‚Ä¶ tile next to Text.\nEnter:\n\nalbums next to &lt;CNT(rock_n_roll_studio_albums.csv)&gt;\n\n\n\n\n\n\nDonut chart\nThe donut chart is a variation of the pie chart, distinguished by its blank center. This design not only offers a cleaner visual but also makes it slightly easier for viewers to compare the relative sizes of each category.\n\n\n\n\nExercise 4. Donut Chart\n\n\n\n\n\nCreate a Donut Chart showing the number of albums released by 2025 Rock N Roll Hall of Fame inductees.\n\n\n\nDuplicate the existing sheet\n\n\nRight-click on PieChart and select Duplicate.\n\n\nRename the new sheet DonutChart.\n\n\n\n\nCreate a calculated field\n\n\nDouble click on the Rows shelf and type avg(0).\n\n\nIt doesn‚Äôt really matter whether you use average, minimum, or maximum. Tableau requires aggregated measures, and the average of zero is always zero.\n\n\n\n\nDuplicate this calculation\n\n\nHold Ctrl (or Option on Mac) and drag AGG(avg(0)) to the right to duplicate the pill on the Rows shelf.\n\n\nNotice there are now 2 pie charts and a separate marks card for each chart.\n\n\n\n\nFormat Marks Cards\n\n\nOn the Marks Card labeled AGG(avg(0)) (2):\n\n\nClick the dropdown next to Automatic and select Circle.\n\n\nRemove Artist, CNT(rock_n_roll_studio_albums.csv) from color, size, and label.\n\n\nClick on Color:\n\n\nChange color to White.\n\n\nAdd a dark gray border.\n\n\n\n\n\n\n\n\nCreate a dual axis\n\n\nOn the second AGG(avg(0)) pill, click the ‚ñº caret and select Dual Axis.\n\n\n\n\nAdjust size\n\n\nOn the Marks Card labeled AGG(avg(0)):\n\n\nClick Size.\n\n\nDrag the slider to the right to increase the size of the colored pie chart.\n\n\n\nOn the Marks Card labeled AGG(avg(0)) (2):\n\n\nDrag the slider to the right to increase the size of the white circle.\n\n\n\n\n\n\nAdjust formatting\n\n\nRemove unnecessary lines:\n\n\nFrom the Format toolbar, select Lines.\n\n\nOn the Rows tab set:\n\n\nSet Zero Lines to None.\n\n\n\n\n\n\n\n\nRemove borders:\n\n\nFrom the Formatting Pane select Borders.\n\n\nOn the Sheet tab, set:\n\n\nRow Divider to None on Pane:.\n\n\nColumn Divider to None on Pane:.\n\n\n\n\n\n\nRemove headers:\n\n\nRight-click the left avg(0) axis and uncheck Show Header.\n\n\n\n\n\n\n\n\n\nVideo showing steps 1-7 above\n\n\n\n\nTree Map\nTree maps are a compelling alternative to pie charts, using nested rectangles to represent the proportional size of categories relative to one another.\n\n\n\n\nExercise 5. Tree Map\n\n\n\n\n\nQuickly convert the Donut Chart created in Exercise 4.\n\n\n\nDuplicate the existing sheet\n\n\nRight-click on DonutChart and select Duplicate.\n\n\nRename the new sheet TreeMap.\n\n\n\n\nOpen Show Me and select Treemaps\n\n\nOn the Marks Card\n\n\nCopy Artist to Color.\n\n\nCopy CNT(rock_n_roll_studio_albums.csv) to Label\n\n\n\n\nAdd a Percent of Total Table Calculation\n\n\nClick the ‚ñº caret on the CNT(rock_n_roll_studio_albums.csv) pill assigned to Label\n\n\nHover over Quick Table Calculation ‚ñ∫ and select Percent of Total.\n\n\n\n\nUpdate title\n\n\nDouble click the worksheet title.\n\n\nEnter:\n\n\nPercentage of total albums released by 2025 inductees\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n\n\n\n\nSupplemental readings\n\n\n\n\nBETTER DATA VISUALIZATIONS\n\n\n\n\nBetter Data Visualizations: A Guide for Scholars, Researchers, and Wonks\n\n\n\n\nby Jonathan Schwabish\n\n\nNew York : Columbia University Press, 2021.\n\n\n\n\n\n\n\nEFFECTIVE DATA VISUALIZATION\n\n\n\n\nEffective Data Visualization: The Right Chart for the Right Data\n\n\n\n\nby Stephanie Evergreen\n\n\nThousand Oaks, California: SAGE Publications, 2020",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 3. Read, Interpret, and Evaluate Data"
    ]
  },
  {
    "objectID": "wikipedia.html",
    "href": "wikipedia.html",
    "title": "Lesson 3. Wikipedia",
    "section": "",
    "text": "This lesson introduces pandas.read_html, a useful tool for extracting tables from HTML, and continues to explore BeautifulSoup, a Python library designed for parsing XML and HTML documents. We will start by gathering artists found on the List of Rock and Roll Hall of Fame inductees webpage in Wikipedia. We will then assemble discographies for 2-3 of our favorite artists.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 3. Wikipedia"
    ]
  },
  {
    "objectID": "wikipedia.html#data-skills-concepts",
    "href": "wikipedia.html#data-skills-concepts",
    "title": "Lesson 3. Wikipedia",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nSearch parameters\nHTML\nWeb scraping\nPandas",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 3. Wikipedia"
    ]
  },
  {
    "objectID": "wikipedia.html#learning-objectives",
    "href": "wikipedia.html#learning-objectives",
    "title": "Lesson 3. Wikipedia",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nExtract and store tables and other HTML elements in a structured format\nApply best practices for managing data\n\nThis tutorial is designed to support multi-session workshops offered by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts visit the Python - Mastering the Basics tutorial.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 3. Wikipedia"
    ]
  },
  {
    "objectID": "wikipedia.html#read_html",
    "href": "wikipedia.html#read_html",
    "title": "Lesson 3. Wikipedia",
    "section": ".read_html()",
    "text": ".read_html()\nRead HTML tables directly into DataFrames with .read_html() . This extremely useful tool extracts all tables present in specified URL or file, allowing each table to be accessed using standard list indexing and slicing syntax.\nThe following code instructs Python to go to the Wikipedia List of states and territories of the United States and retrieve the second table.\n\nimport requests\nimport pandas as pd\nfrom io import StringIO\n\nurl='https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States'\nheaders = {'User-Agent': 'Mozilla/5.0'}\nresponse=requests.get(url, headers=headers).text\ntables=pd.read_html(StringIO(response))\ntables[1]\n\n\n\n\n\nExercise 6: .read_html()\n\n\n\n\nVisit the Wikipedia List of Rock and Roll Hall of Fame inductees and extract the Performers table.\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport requests\nimport pandas as pd\nfrom io import StringIO\n\nurl='https://en.wikipedia.org/wiki/List_of_Rock_and_Roll_Hall_of_Fame_inductees'\nheaders = {'User-Agent': 'Mozilla/5.0'}\nresponse=requests.get(url, headers=headers).text\ntables=pd.read_html(StringIO(response))\nperformers=tables[0]\nperformers",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 3. Wikipedia"
    ]
  },
  {
    "objectID": "wikipedia.html#find_previous-and-.find_all_previous",
    "href": "wikipedia.html#find_previous-and-.find_all_previous",
    "title": "Lesson 3. Wikipedia",
    "section": ".find_previous( ) and .find_all_previous( )",
    "text": ".find_previous( ) and .find_all_previous( )\nSimilar to .find_next( ) and .find_all_next( ), .find_previous( ) and .find_all_previous( ) gathers the previous instance of a named tag.\n\n\n\n\nExercise 7: Gather discographies\n\n\n\n\n\nGather the discography tables from the discography Wikipedia pages for the 2-3 artists you identified in Exercise 3.\n\n\n\nIdentify base url\n\n\nStore the secondary_url parts for each artist in a list, such as ‚Ä¶\n\nartists=['Cyndi_Lauper_discography','Joe_Cocker_discography','The_White_Stripes_discography']\n\nCreate a for loop to iterate through each artist in your list.\n\n\nUse .read_html to gather the discography tables each artist on your list.\n\n\n\nTip\n\n\nUse index slicing to focus on one artist page first as you work through steps 4 and 5.\n\n\n\nCreate an empty list for headers to gather table headers for each artist.\n\n\nUse requests with BeautifulSoup and .find_previous() to gather the headers for each table and append each header to your headers list. Check to see if your table headers match the html tables you gathered with .read_html\n\n\n\nTip\n\n\nHeaders do not always exist for each table. You may need to use a try-except block and/or if statements to accurately construct your header list.\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nartists=['Cyndi_Lauper_discography','Joe_Cocker_discography','The_White_Stripes_discography']\nbase_url='https://en.wikipedia.org/wiki/'\n\nfor artist in artists[0:1]:\n\n    url=base_url+artist\n    headers = {'User-Agent': 'Mozilla/5.0'}\n    response=requests.get(url, headers=headers)\n    response.encoding = 'utf-8' #requests.get() does not accept an encoding parameter, but you can set encoding on the response object after the request.\n    text=response.text\n    soup=BeautifulSoup(text, 'html.parser')\n    \n    #FIND TABLES\n    html_tables=pd.read_html(text)\n\n    #FIND TABLE_HEADERS\n    table_headers=[]\n    table_number=0\n    tables=soup.find_all('table')\n    for table in tables:\n        if table.find_previous('div',{'class':'mw-heading2'}) is not None:\n            h2=table.find_previous('div',{'class':'mw-heading2'}).text.split('[')[0]\n            table_headers.append(h2.lower().replace(' ','_'))\n            # print(f\"table_number_{table_number}: {h2}\")\n        else:\n            h2='no_header'\n            table_headers.append('h2')\n        if table.find_previous('div',{'class':'mw-heading3'}) is not None:\n            h3=table.find_previous('div',{'class':'mw-heading3'}).text.split('[')[0]\n            table_headers.append(h3.lower().replace(' ','_'))\n            print(f\"table_number_{table_number}: {h3}\")\n        else:\n            h3='no_header'\n            table_headers.append(h3)\n        print(f\"table_number_{table_number}: {h2}, {h3}\")\n        table_number += 1",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 3. Wikipedia"
    ]
  },
  {
    "objectID": "wikipedia.html#os-module",
    "href": "wikipedia.html#os-module",
    "title": "Lesson 3. Wikipedia",
    "section": "os module",
    "text": "os module\nThe os module tells Python where to find and save files.\n\nos.mkdir(‚Äòpath‚Äô)\nCreates a new directory in your project folder or another specified location. Makes a directory in your project folder or another folder you specify. If a directory by the same name already exists in the path specified, os.mkdir will raise an OSError. Use a try-except block to handle the error.\n\nimport os\nartist = \"Cyndi_Lauper\"\n\ntry:\n    os.mkdir(artist)\nexcept FileExistsError:\n    print(f\"Directory '{artist}' already exists.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n\n\n\n\nExercise 8\n\n\n\n\n\n\nMake a directory for each artist in your project folder.\n\n\nUse pd.read_csv to create a file for each table. Incorporate the table number and header in the filename.\n\n\n\nTip\n\n\nIncrement counter variables for table numbers.\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nartists=['Cyndi_Lauper_discography','Joe_Cocker_discography','The_White_Stripes_discography']\nbase_url='https://en.wikipedia.org/wiki/'\n\nfor artist in artists[0:1]:\n\n    url=base_url+artist\n    headers = {'User-Agent': 'Mozilla/5.0'}\n    response=requests.get(url, headers=headers)\n    response.encoding = 'utf-8' #requests.get() does not accept an encoding parameter, but you can set encoding on the response object after the request.\n    text=response.text\n    soup=BeautifulSoup(text, 'html.parser')\n    \n    #FIND TABLES\n    html_tables=pd.read_html(text)\n\n    #FIND TABLE_HEADERS\n    table_headers=[]\n    table_number=0\n    tables=soup.find_all('table')\n    for table in tables:\n        if table.find_previous('div',{'class':'mw-heading3'}) is not None:\n            h3=table.find_previous('div',{'class':'mw-heading3'}).text.split('[')[0]\n            table_headers.append(h3.lower().replace(' ','_'))\n            print(f\"table_number_{table_number}: {h3}\")\n        elif table.find_previous('div',{'class':'mw-heading2'}) is not None:\n            h2=table.find_previous('div',{'class':'mw-heading2'}).text.split('[')[0]\n            table_headers.append(h2.lower().replace(' ','_'))\n            print(f\"table_number_{table_number}: {h2}\")\n        else:\n            h='no_header'\n            table_headers.append(h)\n            print(f\"table_number_{table_number}: {h}\")\n        table_number += 1\n\n    #CREATE A DIRECTORY FOR EACH ARTIST AND OUTPUT TABLES TO THE DIRECTORY\n    position=0\n    for each_header in table_headers:\n        if each_header != 'no header':\n            table_name=each_header\n            number=position\n            artist_directory=artist.replace('_discography','').lower()\n            try:\n                os.mkdir(artist_directory)\n            except FileExistsError:\n                print(f\"Directory '{artist_directory}' already exists.\")\n            except Exception as e:\n                print(f\"An error occurred: {e}\")\n            filename=artist_directory+'/table_number_'+str(number)+'_'+table_name+'.csv'\n            print(filename)\n            html_table=html_tables[position]\n            html_table.to_csv(filename)\n        position += 1",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 3. Wikipedia"
    ]
  },
  {
    "objectID": "animals.html",
    "href": "animals.html",
    "title": "Lesson 2. Meet the Animals",
    "section": "",
    "text": "This lesson continues to explore the diverse features of BeautifulSoup, a Python library designed for parsing XML and HTML documents. We will utilize BeautifulSoup to extract information about a select group of animals showcased on the Meet the Animals webpage of Smithsonian‚Äôs National Zoo and Conservation Biology Institute. Additionally, we will explore Pandas, a powerful Python library used for structuring, analyzing, and manipulating data.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#data-skills-concepts",
    "href": "animals.html#data-skills-concepts",
    "title": "Lesson 2. Meet the Animals",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nSearch parameters\nHTML\nWeb scraping\nPandas data structures",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#learning-objectives",
    "href": "animals.html#learning-objectives",
    "title": "Lesson 2. Meet the Animals",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nIdentify search parameters and understand how they are inserted into a url.\nNavigate document, element, attribute, and text nodes in a Document Object Model (DOM).\nExtract and store HTML elements\nExport data to .csv\n\nThis tutorial is designed to support multi-session workshops hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts visit the Python - Mastering the Basics tutorial.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#review-and-understand-the-terms-of-use.",
    "href": "animals.html#review-and-understand-the-terms-of-use.",
    "title": "Lesson 2. Meet the Animals",
    "section": "Review and understand the terms of use.",
    "text": "Review and understand the terms of use.\n\nDo the terms of service include any restrictions or guidelines?\nAre permissions/licenses needed to scrape data? If yes, have you obtained these permissions/licenses?\nIs the information publicly available?\nIf a database, is the database protected by copyright? Or in the public domain?",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#fair-use",
    "href": "animals.html#fair-use",
    "title": "Lesson 2. Meet the Animals",
    "section": "Fair Use",
    "text": "Fair Use\nLimited use of copyrighted materials is allowed under certain conditions for journalism, scholarship, and teaching. Use the Resources for determining fair use to verify your project is within the scope of fair use. Contact University Libraries Copyright Services if you have any questions.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#check-for-robots.txt-directives",
    "href": "animals.html#check-for-robots.txt-directives",
    "title": "Lesson 2. Meet the Animals",
    "section": "Check for robots.txt directives",
    "text": "Check for robots.txt directives\nrobots.txt directives limit web-scraping or web-crawling. Look for this file in the root directory of the website by adding /robots.txt to the end of the url. Respect these directives.\n\n\n\n\nExercise 1: Examine Copyright | Terms of Use\n\n\n\n\n\nLocate and read the terms of use for the Smithsonian‚Äôs National Zoo & Conservation Biology Institute\n\n\n\nWhat are the copyright restrictions for this resource?\n\n\nWhat are the terms of use?\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe copyright restrictions for the Smithsonian‚Äôs National Zoo & Conservation Biology Institute are listed with the Terms of Use and can be found on the center of the bottom footer of the webpage.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#example",
    "href": "animals.html#example",
    "title": "Lesson 2. Meet the Animals",
    "section": "Example:",
    "text": "Example:\nFind the common name for meerkat.\n\nOpen the meerkat Meet the Animals webpage in Chrome.\nRight-click on the element you want to inspect (e.g., the common name).\nSelect Inspect.\n\n\n\n\nmeerkat_inspect.png\n\n\nThis opens the Developer Tools panel, typically on the right of the screen.\n\nThe default Elements tab shows the HTML structure (DOM).\nScroll through the rendered HTML to explore more content.\n Click the inspect icon in the top-left corner of the in the Developer Tools panel.\nHover over elements on the webpage to highlight them in the HTML.\n\nAs you hover, Chrome will:\n\nHighlight the corresponding element on the page\nShow a tooltip with tag details (e.g., class, ID)\nReveal the element‚Äôs location in the HTML tree\n\n\n\n\nmeerkat_inspect_element\n\n\nThis process helps you identify the exact tags and attributes you‚Äôll need to target when scraping data from the page.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#viewing-an-elements-html-structure",
    "href": "animals.html#viewing-an-elements-html-structure",
    "title": "Lesson 2. Meet the Animals",
    "section": "Viewing an Element‚Äôs HTML Structure",
    "text": "Viewing an Element‚Äôs HTML Structure\nTo examine an element‚Äôs exact location within the DOM:\n\nIn Chrome Developer Tools, right-click on the highlighted element.\nSelect Copy &gt; Copy element.\nPaste the copied HTML into Notepad or any text editor to view its full structure and attributes.\n\nThis is especially helpful for identifying tags, classes, and nesting when preparing to extract data through web scraping.\n\n\n\nmeerkat_copy_element.png\n\n\n\n\n\nmeerkat_notepad.png\n\n\n\n\n\n\nExercise 3: Inspect the Elements\n\n\n\n\n\nGo to Meet the Animals and choose an animal to examine from the list. Inspect the following elements, select Copy &gt; Copy element, and then past the text to Notepad or a similar text editor.\n\n\n\nCommon name\n\n\nScientific name\n\n\nTaxonomic information\n\n\n\nClass\n\n\nOrder\n\n\nFamily\n\n\nGenus and species\n\n\n\nPhysical description\n\n\nSize\n\n\nNative habitat\n\n\nConservation status\n\n\nFun facts",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#requests",
    "href": "animals.html#requests",
    "title": "Lesson 2. Meet the Animals",
    "section": "requests",
    "text": "requests\nThe requests library retrieves HTML or XML documents from a server and processes the response.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#beautifulsoup",
    "href": "animals.html#beautifulsoup",
    "title": "Lesson 2. Meet the Animals",
    "section": "BeautifulSoup",
    "text": "BeautifulSoup\nBeautifulSoup parses HTML and XML documents, helping you search for and extract elements from the DOM.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#pandas",
    "href": "animals.html#pandas",
    "title": "Lesson 2. Meet the Animals",
    "section": "pandas",
    "text": "pandas\nPandas is a large Python library used for manipulating and analyzing tabular data. Helpful Pandas methods include:\n\npd.DataFrame\nA Pandas DataFrame is one of the most powerful and commonly used data structures in Python for working with tabular data‚Äîdata that is organized in rows and columns, similar to a spreadsheet or SQL table.\nA DataFrame is a 2-dimensional labeled data structure with:\n\nRows (each representing an observation or record)\nColumns (each representing a variable or feature)\n\nThink of it like an Excel sheet or a table in a database.\n\nimport pandas as pd\n\ndf=pd.DataFrame([data, index, columns, dtype, copy])\n\nüîó See __Pandas DataFrame documentation.\n\n\npd.read_csv( )\nThe pd.read_csv() function is used to read data from a CSV (Comma-Separated Values) file and load it into a DataFrame.\n\npd.read_csv('INSERT FILEPATH HERE')\n\nExample:\n\nimport pandas as pd\ndf=pd.read_csv('data/meet_the_animals.csv')  #df is a common abbreviation for DataFrame\ndf\n\n\n\n\n\n\n\n\nanimal\n\n\n\n\n0\nblack-throated-blue-warbler\n\n\n1\nelds-deer\n\n\n2\nfalse-water-cobra\n\n\n3\nhooded-merganswer\n\n\n4\npatagonian-mara\n\n\n\n\n\n\n\nüîó See Pandas .read_csv( ) documentation.\n\n\n.tolist( )\nThe method .tolist() is used in to convert a Series (a single column of data) into a Python list.\n\ndf.Series.tolist()\n\nExample:\n\nimport pandas as pd\ndf=pd.read_csv('data/meet_the_animals.csv')\nanimals=df.animal.tolist()\nanimals\n\n['black-throated-blue-warbler',\n 'elds-deer',\n 'false-water-cobra',\n 'hooded-merganswer',\n 'patagonian-mara']\n\n\nüîó See .tolist( ) documentation.\n\n\n.dropna( )\nThe dropna() method is used to remove missing values (NaN) from a DataFrame or Series. It‚Äôs a fast and effective way to clean your data‚Äîbut it should be used with care.\n\nDataFrame.dropna(*, axis=0, how=&lt;no_default&gt;, thresh=&lt;no_default&gt;, subset=None, inplace=False, ignore_index=False)\n\n\n\n.fillna( )\nThe .fillna() method is used to replace NaN (missing) values with a value you specify.\n\ndf.Series.fillna(value=None, *, method=None, axis=None, inplace=False, limit=None, downcast=&lt;no_default&gt;)\n\nThis is especially useful when you want to:\n\nFill in missing data with a default value\nUse statistical values like the mean or median\nForward-fill or backward-fill based on surrounding data\n\nüîó See .fillna( ) documentation.\n\n\n.iterrows( )\nThe .iterrows() method allows you to iterate over each row in a DataFrame as a pair:\n\nThe index of the row\nThe row data as a pandas Series\n\n\ndf=DataFrame.iterrows()\n\nExample:\n\nimport pandas as pd\ndf=pd.read_csv('data/meet_the_animals.csv')\nfor idx, row in df.iterrows():\n    print(row.animal)\n\nblack-throated-blue-warbler\nelds-deer\nfalse-water-cobra\nhooded-merganswer\npatagonian-mara\n\n\nThis is useful when you need to process rows one at a time, especially for tasks like conditional logic or row-wise operations.\n\n\nCaution!\n\n\n\n.iterrows() is not the most efficient method for large datasets. For better performance, consider using vectorized operations or .itertuples().\n\n\n\nüîó See .iterrows( ) documentation.\n\n\n.iloc\nThe .iloc property is used to select rows (and columns) by their integer position (i.e., by index number, not label).\n\nDataFrame.iloc[start:end]\n\nExample:\n\nimport pandas as pd\ndf=pd.read_csv('data/meet_the_animals.csv')\nfor idx, row in df.iloc[0:1].iterrows():\n    print(row.animal)\n\nblack-throated-blue-warbler\n\n\n\n.iloc[row_index] accesses a specific row\n.iloc[row_index, column_index] accesses a specific cell\nYou can also use slicing to select multiple rows or columns\n\nUse .iloc when:\n\nYou want to access data by position, not by label\nYou‚Äôre working with numeric row/column indices\nYou‚Äôre iterating or slicing through rows or columns\n\nüîó See .iloc documentation.\n\n\n.concat( )\nThe pandas.concat function is used to join two or more DataFrames along a specific axis:\n\naxis=0 ‚Üí stacks DataFrames vertically (adds rows)\naxis=1 ‚Üí stacks DataFrames horizontally (adds columns)\n\n\npandas.concat(objs, *, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=False, copy=None)\n\nExample:\n\nimport pandas as pd\n\nresults=pd.DataFrame(columns=['common_name','size'])\ndf=pd.read_csv('data/meet_the_animals.csv')\nfor idx, row in df.iterrows():\n    common_name=row.animal\n    size=10\n    data_row={\n        'common_name':common_name,\n        'size':size     \n    }\n    data=pd.DataFrame(data_row, index=[0])\n    results=pd.concat([data, results], axis=0, ignore_index=True)\n\nresults\n\n\n\n\n\n\n\n\ncommon_name\nsize\n\n\n\n\n0\npatagonian-mara\n10\n\n\n1\nhooded-merganswer\n10\n\n\n2\nfalse-water-cobra\n10\n\n\n3\nelds-deer\n10\n\n\n4\nblack-throated-blue-warbler\n10\n\n\n\n\n\n\n\nüîó See .concat documentation.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "animals.html#how-it-works",
    "href": "animals.html#how-it-works",
    "title": "Lesson 2. Meet the Animals",
    "section": "üß™ How It Works",
    "text": "üß™ How It Works\n\nThe code inside the try block is executed first.\nIf an error occurs, Python jumps to the except block.\nYour program continues running without stopping unexpectedly.\n\nExample:\n\nimport pandas as pd\n\nresults=pd.DataFrame(columns=['common_name','size'])\nfor idx, row in df.iterrows():\n    try:\n        common_name=row.animal\n        size=10\n        data_row={\n            'common_name':common_name,\n            'size':size     \n        }\n        data=pd.DataFrame(data_row, index=[0])\n        results=pd.concat([data, results], axis=0, ignore_index=True)\n    except:\n        common_name='no name found'\n        size=0\n        data_row={\n                    'common_name':common_name,\n                    'size':size     \n                }\n        data=pd.DataFrame(data_row, index=[0])\n        results=pd.concat([data, results], axis=0, ignore_index=True)\n\nresults\n\n\n\n\n\n\n\n\ncommon_name\nsize\n\n\n\n\n0\npatagonian-mara\n10\n\n\n1\nhooded-merganswer\n10\n\n\n2\nfalse-water-cobra\n10\n\n\n3\nelds-deer\n10\n\n\n4\nblack-throated-blue-warbler\n10\n\n\n\n\n\n\n\n\n\nTip:\n\n\n\nFor a more detailed explanation with examples, ask Copilot to explain try except python.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 2. Meet the Animals"
    ]
  },
  {
    "objectID": "scopus.html",
    "href": "scopus.html",
    "title": "Lesson 8. Scopus",
    "section": "",
    "text": "Elsevier provides API access to its Scopus database to academic researchers. This allows researchers to programmatically retrieve metadata about publications, authors, institutions, and more.\nThis tutorial introduces pybliometrics an API wrapper designed to simplify retrieving data from Scopus‚Äôs multiple API access points. An API wrapper is a python library or module that handles requests, authentication, parsing, and more.\nPybliometrics will help you to ‚Ä¶",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 8. Scopus"
    ]
  },
  {
    "objectID": "scopus.html#data-skills-concepts",
    "href": "scopus.html#data-skills-concepts",
    "title": "Lesson 8. Scopus",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nAPI keys\nAPI wrappers",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 8. Scopus"
    ]
  },
  {
    "objectID": "scopus.html#learning-objectives",
    "href": "scopus.html#learning-objectives",
    "title": "Lesson 8. Scopus",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nInstall and use an API wrapper to authenticate, request, parse, and store data.\nInterpret documentation and apply concepts to write functional code.\n\nThis tutorial is designed to support multi-session workshops hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts visit the Python - Mastering the Basics tutorial.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 8. Scopus"
    ]
  },
  {
    "objectID": "scopus.html#getting-started",
    "href": "scopus.html#getting-started",
    "title": "Lesson 8. Scopus",
    "section": "Getting started",
    "text": "Getting started\nTo use the Scopus APIs, researchers must first request an API key via the Elsevier‚Äôs Developers Portal and agree to comply with Elsevier‚Äôs API usage policies.\nWhen requesting an API key, be ready to provide a few details:\n\nYour use case - What you‚Äôre planning to do with the data?\nThe type of Scopus metadata you want to access - like publications, author or institutional profiles, or citations.\nHow much data you expect to retrieve - for example, ‚Äúaround 3,500 records.‚Äù\nWhat your final product will be ‚Äì such as a research paper, website, or something else.\n\nIt‚Äôs a good idea to read through the Getting Started guide for Scopus APIs before submitting your request. It will help you understand how the API works and what to expect. Also, be aware if you usually work off campus, you may need to request an institutional token and answer a few additional questions. Otherwise, you will need to use your API key while connected to the university‚Äôs network.\nOnce you have your API key and institutional token, if needed, install the stable version of pybliometrics from PyPI:\n\npip install pybliometrics\n\nThe first time you use pybliometrics, you will be prompted to input your API key and institutional token. These will be saved in ~/.config/pybliometrics.cfg.\n\nimport pybliometrics",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 8. Scopus"
    ]
  },
  {
    "objectID": "scopus.html#scopussearch",
    "href": "scopus.html#scopussearch",
    "title": "Lesson 8. Scopus",
    "section": "ScopusSearch",
    "text": "ScopusSearch\nScopusSearch is one of the 11 API interfaces available to interact with Elsevier‚Äôs Scopus database through the pybliometrics library. The ScopusSearch class in pybliometrics allows you to ‚Ä¶\n\nQuery Scopus using all fields available in Scopus advanced search except ‚ÄúINDEXTERMS()‚Äù and ‚ÄúLIMIT-TO()‚Äù.\nFilter results\nRetrieve metadata\n\nThe search returns a list of named tuples that can be converted into a DataFrame with pandas for futher analysis or export to CSV.\n\nStep 1. Construct query\nTo get started with the the ScopusSearch class in pybliometrics, we will begin by searching for publications that were: - Funded by the National Science Foundation (NSF) - Authored by researchers affiliated with The Ohio State University - Published between 2000 and 2001.\n\n#identify libraries needed for project\nfrom pybliometrics.scopus import ScopusSearch\nimport pandas as pd\nimport time\n\n#initializes the class\npybliometrics.scopus.init() \n\n#query\nq='(FUND-SPONSOR ( \"National Science Foundation\") AND AFFIL (\"Ohio State University\")) AND PUBYEAR &gt; 2020 AND PUBYEAR &lt; 2022' \n\n#search (creates an object)\ns=ScopusSearch(q, verbose=True) #setting verbose to True turns on a progress bar for search\n\n\n\nStep 2. Retrieve and store results\ns.results retrieves the list of named tuples. Each item in s.results is an object with attributes.\narticle_title=s.results[0].title looks at the first tuple in the list, finds the attribute title and assigns the attribute to the variable article_title.\njournal_title=s.results[5].publicationName looks at the sixth tuple in the list, finds the attribute publicationName and assigns the attribute to the variable journal_title.\nYou can loop through the list of tuples or use list indexing to pull specific attributes out of s.results or you can immediately create a DataFrame to filter, analyze, and store your search results.\n\nresults=pd.DataFrame(s.results)\n\n#examine DataFrame shape\nprint(results.shape)\n\n#examine column names\nprint(results.columns)\n\n#export results to csv file\nresults.to_csv('results.csv', encoding='utf-8')",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 8. Scopus"
    ]
  },
  {
    "objectID": "scopus.html#authorsearch",
    "href": "scopus.html#authorsearch",
    "title": "Lesson 8. Scopus",
    "section": "AuthorSearch",
    "text": "AuthorSearch\nLearning to read and interpret documentation is an essential skill for anyone working with data. Good documentation can:\n\nUncover powerful or lesser-known features that can enhance your project.\nIntroduce optional parameters that help you fine-tune your queries‚Äîfor example, setting enncoding=utf8, specifying column headers, or filtering results.\nDefine error messages and guide you through troubleshooting when things don‚Äôt work as expected.\nSave your time by offering accurate, up-to-date information‚Äîoften more reliable that what you‚Äôll find in scattered or outdated online forums.\n\n\n\n\n\nExercise: AuthorSearch\n\n\n\n\n\nThe AuthorSearch class in pybliometrics connects to the Scopus AuthorSearch API and allows you to retrieve detailed information about authors indexed in Scopus. To practice reading, interpreting, and applying concepts from the pybliometrics AuthorSearch documentation:\n\n\n\nCreate a list of unique author_ids from the publication results you obtained in Step 2.\n\n\nUse the pybliometrics.scopus.AuthorSearch documentation to write Python code that retrieves the following data for the first 10 unique author_id:\n\nauthor_id\n\nauthor_surname\n\nauthor_givenname\n\nauthor_initials\n\nauthor_affiliation\n\nauthor_city\n\nauthor_country\n\n\n\n\nExport results to .csv file.\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nfrom pybliometrics.scopus import AuthorSearch\nunique_author_ids=[]\nauthor_ids=results.author_ids.tolist()\n\nfor each_list in author_ids:\n    individual_ids=each_list.split(';')\n    for each_id in individual_ids:\n        if each_id not in unique_author_ids:\n            search_string='AU-ID('+str(each_id)+')' + ' OR '\n            unique_author_ids.append(search_string)\n\n\n#query first 10 unique author ids\nunique_author_ids=unique_author_ids[0:10]\n\n#construct query\nquery=''.join(unique_author_ids).rstrip(' OR').strip()\n\n#search\ns_author=AuthorSearch(query, verbose=True)\n\n#insert results into DataFrame\nresults_authors=pd.DataFrame(s_author.authors)\n\n#select columns\nresults_authors=results_authors[['surname','initials','givenname','affiliation','city','country']]\n\n#export results to csv file\nresults_authors.to_csv('results_authors.csv', encoding='utf-8')",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 8. Scopus"
    ]
  },
  {
    "objectID": "lantern.html",
    "href": "lantern.html",
    "title": "Lesson 1. The Lantern",
    "section": "",
    "text": "Web scraping and APIs are popular methods for collecting data from websites. Webscraping involves directly parsing a website‚Äôs HTML, allowing extraction of a wide range of data available on a page. Webscraping, however, introduces complexity to a project, especially if the website‚Äôs data is not consistently structured. APIs provide structured data and detailed documentation for querying the website and filtering results. They are generally easier to use but may come with restrictions, such as limits on the number of requests per day or the number of records you can retrieve.\nIn publication since 1881, The Lantern is The Ohio State University‚Äôs award-winning student newspaper. The Lantern Digital Archives includes all articles, illustrations, and advertisments published in The Lantern between 1881 and 2018.\nThis lesson introduces BeautifulSoup, a Python library used to parse XML and HTML documents. We will use BeautifulSoup to extract elements from The Lantern‚Äôs XML.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#data-skills-concepts",
    "href": "lantern.html#data-skills-concepts",
    "title": "Lesson 1. The Lantern",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nSearch parameters\nXML\nWeb scraping",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#learning-objectives",
    "href": "lantern.html#learning-objectives",
    "title": "Lesson 1. The Lantern",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nIdentify search parameters and understand how they are inserted into a url.\nNavigate document, element, attribute, and text nodes in a Document Object Model (DOM).\nExtract and store XML elements.\n\nThis tutorial is designed to support multi-session workshops hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts see Python - Mastering the Basics tutorial.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#review-and-understand-the-terms-of-use.",
    "href": "lantern.html#review-and-understand-the-terms-of-use.",
    "title": "Lesson 1. The Lantern",
    "section": "Review and understand the terms of use.",
    "text": "Review and understand the terms of use.\n\nDo the terms of service include any restrictions or guidelines?\nAre permissions/licenses needed to scrape data? If yes, have you obtained these permissions/licenses?\nIs the information publicly available?\nIf a database, is the database protected by copyright? Or in the public domain",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#fair-use",
    "href": "lantern.html#fair-use",
    "title": "Lesson 1. The Lantern",
    "section": "Fair Use",
    "text": "Fair Use\nLimited use of copyrighted materials is allowed under certain conditions for journalism, scholarship, and teaching. Use the Resources for determining fair use to verify your project is within the scope of fair use. Contact University Libraries Copyright Services if you have any questions.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#check-for-robots.txt-directives",
    "href": "lantern.html#check-for-robots.txt-directives",
    "title": "Lesson 1. The Lantern",
    "section": "Check for robots.txt directives",
    "text": "Check for robots.txt directives\nrobots.txt directives limit web-scraping or web-crawling. Look for this file in the root directory of the website by adding /robots.txt to the end of the url. Respect these directives.\n\n\n\n\nExercise 1: Examine Copyright | Terms of Use\n\n\n\n\n\n\nGo to the OSU Publication Archives website\n\n\n. Where is the Copyright Notice for this resource?\n\n\nWhat are the terms of use?\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe Copyright Notice for The Ohio State University‚Äôs online archive of The Lantern student newspaper can be found on the bottom right of the OSU Publication Archives website. The Terms of use are listed separately at the bottom left.\n\n\n\n\n\n\n\n\nNote:\n\n\nThe Ohio State University provides the online archives of Ohio State‚Äôs student newspaper The Lantern, the student yearbook The Makio, and alumni magazines for research and educational purposes only. The Terms of Use specify that unauthorized mass downloading or scraping into any format is prohibited. For this lesson, please limit your search results to scrape no more than 100 records.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#requests",
    "href": "lantern.html#requests",
    "title": "Lesson 1. The Lantern",
    "section": "requests",
    "text": "requests\nThe requests library retrieves HTML or XML documents from a server and processes the response.\n\nimport requests\nurl=\"https://library.osu.edu\" #INSERT URL HERE\nresponse=requests.get(url)\ntext=response.text # This returns the response content as text\nbytes=response.content  # This returns the response content as bytes.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#beautifulsoup",
    "href": "lantern.html#beautifulsoup",
    "title": "Lesson 1. The Lantern",
    "section": "BeautifulSoup",
    "text": "BeautifulSoup\nBeautifulSoup parses HTML and XML documents, helping you search for and extract elements from the DOM. The first argument is the content to be parsed, and the second specifies the parsing library to use.\n\nhtml.parser The default HTML parser\nlxml a faster parser with more features*\nxml parses XML\nhtml5lib for HTML5 parsing*\n\nüîó Additional keyword arguments (**kwargs) are available. See the BeautifulSoup documentation.\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl=\"https://library.osu.edu\" #INSERT URL HERE\nresponse=requests.get(url).content\nsoup=BeautifulSoup(response, 'xml')\n\nOther Python libraries for parsing include:\n\nlxml.html\npyQuery\nSelenium\n\nEach library has its strengths and weaknesses. To learn more about different parsing tools read Anish Chapagain‚Äôs Hands-On Web Scraping with Python, 2nd edition\n*Verify that lxml or html5lib is installed in your Anaconda environment before using.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#csv",
    "href": "lantern.html#csv",
    "title": "Lesson 1. The Lantern",
    "section": "csv",
    "text": "csv\nThe csv module both writes and reads .csv data.\nSample workflow\n\nimport csv\nCreate an empty list named dataset\nAssign .csv headers to a list named columns\nDefine the writeto_csv function to write results to a .csv file\nGather variables\nFor each row of data, append a list of variables following the order of the .csv headers to the dataset list.\nUse the writeto_csv function to write results to a .csv file\n\n\nimport csv\n\n#####     STEP 1 - CREATE EMPTY DATASET AND DEFINE CSV HEADINGS     ##### \ndataSet=[]\ncolumns=['name','pet','age','profession'] # for CSV headings\n\n#####     STEP 2 - DEFINE FUNCTION TO WRITE RESULTS TO CSV FILE     #####\n\ndef writeto_csv(data,filename,columns):\n    with open(filename,'w+',newline='',encoding=\"UTF-8\") as file:\n        writer = csv.DictWriter(file,fieldnames=columns)\n        writer.writeheader()\n        writer = csv.writer(file)\n        for element in data:\n            writer.writerows([element])\n\n\nname=\"Stanley\"\npet=\"dog\"\nage=8\nprofession=\"chipmunk control\"\n\ndataSet.append([name, pet, age, profession])\n\nwriteto_csv(dataSet,'data/pets.csv',columns)",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#time.sleep",
    "href": "lantern.html#time.sleep",
    "title": "Lesson 1. The Lantern",
    "section": "time.sleep()",
    "text": "time.sleep()\nMost APIs limit the number of records you can request per second. time.sleep( ) suspends your program for a specified number of seconds.\n\nimport time\ntime.sleep(5)",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#datetime",
    "href": "lantern.html#datetime",
    "title": "Lesson 1. The Lantern",
    "section": "datetime",
    "text": "datetime\nIt is good practice to include a last_updated column in any dataset you‚Äôve created after gathering HTML or XML data. The datetime module can be used to identify the date you last ran your Python program.\n\nfrom datetime import date\ntoday = date.today()\n\nlast_updated=today",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "lantern.html#using-beautifulsoup",
    "href": "lantern.html#using-beautifulsoup",
    "title": "Lesson 1. The Lantern",
    "section": "Using BeautifulSoup",
    "text": "Using BeautifulSoup\nVarious methods and approaches may be used to gather and extract XML elements using BeautifulSoup. First we ask BeautifulSoup to search the tree to find the element nodes we identified in Step 4. Helpful methods include:\n\n.find_all( )\nGathers all instances of a tag (i.e.¬†element) and returns a response object. Each instance of the tag is then examined using a for loop.\n\nfind_all(name, attrs, recursive, string, limit, **kwargs)\n\nExample:\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl=\"https://osupublicationarchives.osu.edu/?a=q&r=1&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\nresponse=requests.get(url).content\nsoup=BeautifulSoup(response, 'xml')\nlogical_sections = soup.find_all(\"LogicalSection\")\nfor each_section in logical_sections:\n    result_number = each_section.SearchResultNumber.string\n\nNote that each_section.SearchResultNumber in the example above is equivalent to each_section.find_all(\"SearchResultNumber\"). Appending .string to each_section.SearchResultNumber extracts the text between the tags for SearchResultNumber.\n\n\nTip:\n\n\n\n\nLearning to navigate tags with BeautifulSoup can be difficult at first. Try using Copilot in tandem with BeautifulSoup‚Äôs documentation to help you both identify and understand how to apply useful methods for your project.\n\n\nExample: Ask Copilot what is the difference between tag.string and tag.text in BeautifulSoup.\n\n\n\n\n\n.find( )\nGathers the first instance of a tag.\n\nfind(name, attrs, recursive, string, **kwargs)\n\n\n\n.find_next( ) and .find_all_next( )\nGathers the following instance of a named tag.\n\nfind_next(name, attrs, string, **kwargs)\nfind_all_next(name, attrs, string, limit, **kwargs)\n\n\n\nattributes\n\nname[attr]\n\nTags can have any number of attributes.\nExample:\nhref is an attribute of an &lt;a href=\"url\"&gt; tag. To find an `href attribute value:\n\na['href']\n\n\n\n\n\nExercise 3: Gather and extract elements\n\n\n\n\n\n\n\nExamine the XML structure and identify the nodes for the following elements:\n\n\nunique_id\n\n\narticle_title\n\n\narticle_type\n\n\ndocument_date\n\n\n\n\nWrite a Python script that uses csv and BeautifulSoup to gather and store these elements. Focus on the first 20 search results.\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n# 1. Import libraries\nimport requests\nimport csv\nfrom bs4 import BeautifulSoup\n\n# 2. Create a function to write search results to csv\ndef writeto_csv(data, filename, columns):\n    with open(filename, 'w+', newline='', encoding=\"UTF-8\") as file:\n        writer = csv.DictWriter(file, fieldnames=columns)\n        writer.writeheader()\n        writer = csv.writer(file)\n        for element in data:\n            writer.writerows([element])\n\n\n\n# 3. Define headers for final dataset\ncolumns=['result_number','unique_id','artile_title','article_type','document_date']\n\n#4. Create empty dataset to store each row of data for csv file\ndataset = []\n\n#5. Retrieve the XML\nurl=\"https://osupublicationarchives.osu.edu/?a=q&r=1&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\nresponse=requests.get(url).content\n\n#6. Parse the content with Beautiful Soup \nsoup = BeautifulSoup(response, 'xml')    \nlogical_sections = soup.find_all(\"LogicalSection\")\nfor each_section in logical_sections:\n    result_number = each_section.SearchResultNumber.string\n    unique_id = each_section.find(\"LogicalSectionID\").string\n    article_title = each_section.find(\"LogicalSectionTitle\").string\n    article_type = each_section.find(\"LogicalSectionType\").string\n    document_date = each_section.find(\"DocumentDate\").string\n    dataset.append([result_number,unique_id,article_title,article_type,document_date]) #adds a list of variables for each section to the dataset\n\n#7. Use the writeto_csv function defined in #2 to create the final .csv file with each row  \nwriteto_csv(dataset,'data/lantern_results.csv',columns)     #uses the function defined in #2 to create the final .csv file with each row  \n\n\n\n\n\n\n\n\nExercise 4: Modify code\n\n\n\n\nModify your code to gather search results 1-51.\n\n\n\n\n\n\nSolution:\n\n\n\n\n# 1. Import libraries\nimport requests\nimport csv\nfrom bs4 import BeautifulSoup\n\n# 2. Create a function to write search results to csv\ndef writeto_csv(data, filename, columns):\n    with open(filename, 'w+', newline='', encoding=\"UTF-8\") as file:\n        writer = csv.DictWriter(file, fieldnames=columns)\n        writer.writeheader()\n        writer = csv.writer(file)\n        for element in data:\n            writer.writerows([element])\n\n# 3. Define headers for final dataset\ncolumns=['result_number','unique_id','article_title','article_type','document_date']\n\n#4. Create empty dataset to store each row of data for csv file\ndataset = []\n\n#5. Retrieve the XML\n## modify the url to increment r= by 20. Move response variable to the for loop below.\nurl1=\"https://osupublicationarchives.osu.edu/?a=q&r=\"\nurl2=\"&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\n\n#6. Create a for loop to search each page of results. Use range(start, stop, step) to ask\n# Python to increment i by 20 for each loop. The first SearchResultNumber (i.e. start) is 1. The last\n# SearchResultNumber (i.e. stop) is 51.  Construct a new url for the response variable.\n# Parse the response content with Beautiful Soup.\nfor i in range(1,51,20):\n    response=requests.get(url1+str(i)+url2).content \n    soup = BeautifulSoup(response, 'xml')    \n    logical_section = soup.find_all(\"LogicalSection\")\n    for each_section in logical_section:\n        result_number = each_section.SearchResultNumber.string\n        unique_id = each_section.find(\"LogicalSectionID\").string\n        article_title = each_section.find(\"LogicalSectionTitle\").string\n        article_type = each_section.find(\"LogicalSectionType\").string\n        document_date = each_section.find(\"DocumentDate\").string\n        dataset.append([result_number,unique_id,article_title,article_type,document_date]) #adds a list of variables for each section to the dataset\n\n#7. Use the writeto_csv function defined in #2 to create the final .csv file with each row  \nwriteto_csv(dataset,'data/lantern_results.csv',columns)     #uses the function defined in #2 to create the final .csv file with each row  \n\n\n\n\n\n\n\n\nExercise 5: Gather article text\n\n\n\n\n\nThe default search output provide structured bibliographic metadata for each homecoming parade article. How might you modify your code to gather the publication_text for each article?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n# 1. Import libraries\nimport requests\nimport csv\nfrom bs4 import BeautifulSoup\n\n# 2. Create a function to write search results to csv\ndef writeto_csv(data, filename, columns):\n    with open(filename, 'w+', newline='', encoding=\"UTF-8\") as file:\n        writer = csv.DictWriter(file, fieldnames=columns)\n        writer.writeheader()\n        writer = csv.writer(file)\n        for element in data:\n            writer.writerows([element])\n\n# 3. Define headers for final dataset\ncolumns=['result_number','unique_id','artile_title','article_type','document_date']\n\n#4. Create empty dataset to store each row of data for csv file\ndataset = []\n\n#5. Retrieve the XML\n## modify the url to increment r= by 20. Move response variable to the for loop below.\nurl1=\"https://osupublicationarchives.osu.edu/?a=q&r=\"\nurl2=\"&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\n\n#6. Create a for loop to search each page of results. Use range(start, stop, step) to ask\n# Python to increment i by 20 for each loop. The first SearchResultNumber (i.e. start) is 1. The last\n# SearchResultNumber (i.e. stop) is 51.  Construct a new url for the response variable.\n# Parse the response content with Beautiful Soup.\nfor i in range(1,51,20):\n    response=requests.get(url1+str(i)+url2).content \n    soup = BeautifulSoup(response, 'xml')    \n    logical_section = soup.find_all(\"LogicalSection\")\n    for each_section in logical_section:\n        result_number = each_section.SearchResultNumber.string\n        unique_id = each_section.find(\"LogicalSectionID\").string\n        article_title = each_section.find(\"LogicalSectionTitle\").string\n        article_type = each_section.find(\"LogicalSectionType\").string\n        document_date = each_section.find(\"DocumentDate\").string\n        dataset.append([result_number,unique_id,article_title,article_type,document_date]) #adds a list of variables for each section to the dataset\n\n#7. Use the writeto_csv function defined in #2 to create the final .csv file with each row  \nwriteto_csv(dataset,'data/lantern_results.csv',columns)     #uses the function defined in #2 to create the final .csv file with each row  \n\n#8. Go back to your original search HTML by removing &f=XML from your search url and click on article #1: \"Whose homecoming?\" \n# Note the unique_id LTN19781023-01.2.24 for the article and the position of this unique_id in the search url. Try using the url without \n# the parameters listed after the unique_id. 'https://osupublicationarchives.osu.edu/?a=d&d=LTN19781023-01.2.24' Append &f=XML to the\n# end of this url to retrieve XML for this publication from the server. Note that publication_text is present in the node\n# LogicalSectionTextHTML.\n\n# Now use a for loop to iterate through the urls for each unique_id listed in your dataset variable to gather the publication_text. But\n# first, create a new list of column headers and a new dataset variable to pass into your writeto_csv function. This will allow you to output\n# your results to a new .csv file.\n\ncolumns2=['unique_id','article_title','article_type','publication_date','publication_text']\ndataset2=[]\n\n#sample_url='https://osupublicationarchives.osu.edu/?a=d&d=LTN19781023-01.2.24&f=XML'\nbase_url='https://osupublicationarchives.osu.edu/?a=d&d='\n\nfor each_list in dataset:\n    unique_id=each_list[1]\n    article_title=each_list[2]\n    article_type=each_list[3]\n    publication_date=each_list[4]\n    url=base_url+unique_id+'&f=XML'\n    xml=requests.get(url).content\n    soup=BeautifulSoup(xml,'xml')\n    publication_text=soup.find(\"LogicalSectionTextHTML\").text\n\n    \n    dataset2.append([unique_id,article_title,article_type,publication_date,publication_text])\n\nwriteto_csv(dataset2,'data/lantern_text.csv',columns2)",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 1. The Lantern"
    ]
  },
  {
    "objectID": "ohiolink.html",
    "href": "ohiolink.html",
    "title": "Lesson 6. OhioLINK ETD",
    "section": "",
    "text": "The OhioLINK Electronic Theses and Dissertations (ETD) Center provides access to abstracts and full-text PDFs of theses and dissertations submitted by participating Ohio colleges and universities. Users can perform basic searches by title, author, or keyword, or use advanced search to filter by subject, year, language, institution, ORCID iD, committee members, topic keywords, and full-text content.\nFor prospective PhD students, the ETD Center is a valuable resource to: - Explore research aligned with their interests - Identify academic programs that fit their goals - Discover potential advisors or committee members by reviewing recent dissertations",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 6. OhioLINK ETD"
    ]
  },
  {
    "objectID": "ohiolink.html#data-skills-concepts",
    "href": "ohiolink.html#data-skills-concepts",
    "title": "Lesson 6. OhioLINK ETD",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nWeb scraping\nDynamic vs.¬†static HTML",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 6. OhioLINK ETD"
    ]
  },
  {
    "objectID": "ohiolink.html#learning-objectives",
    "href": "ohiolink.html#learning-objectives",
    "title": "Lesson 6. OhioLINK ETD",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nUnderstand the difference between dynamic and static HTML\nDevelop strategies and approaches to gather dynamic HTML content.\n\nThis tutorial is designed to support multi-session workshops hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts visit the Python - Mastering the Basics tutorial.\n\n\n\nImportant!\n\n\nRemember to examine copyright and terms of use before starting any web scraping project.\n\n\nOhioLINK ETD Acceptable Use Policy",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 6. OhioLINK ETD"
    ]
  },
  {
    "objectID": "ohiolink.html#handling-dynamic-content",
    "href": "ohiolink.html#handling-dynamic-content",
    "title": "Lesson 6. OhioLINK ETD",
    "section": "Handling dynamic content",
    "text": "Handling dynamic content\nThe Electronic Theses and Dissertations Center is a dynamic website. While the page layout, such as headers, footers, and overall structure, is built with static HTML, the search results are loaded dynamically via JavaScript after the initial page load. If you inspect the page using browser Developer Tools, you‚Äôll find that search results are inserted into the element &lt;t-SearchResults-content&gt;:\nExample:\n\n&lt;div class=\"t-SearchResults-content\"&gt;\n            &lt;h3 class=\"t-SearchResults-title\"&gt;\n                1. &lt;span class=\"t-SearchResults-author\"&gt;Mackey-Alfonso, Sabrina&lt;/span&gt;\n                &lt;a href=\"/acprod/odb_etd/r/etd/search/10?p10_accession_num=osu1744894008278402&amp;clear=10&amp;session=10473290292427\"&gt;Short-term High Fat Diet Accelerates Synaptic and Memory Deficits via Neuroinflammatory Mechanisms in an Alzheimer's Disease Mouse Model&lt;/a&gt;\n            &lt;/h3&gt;\n            &lt;div class=\"t-SearchResults-info\"&gt;\n              &lt;p class=\"t-SearchResults-degree\"&gt;\n                  Doctor of Philosophy, The Ohio State University, 2025, Neuroscience Graduate Studies Program\n              &lt;/p&gt;\n              &lt;p class=\"t-SearchResults-desc hide-class\"&gt;&lt;div style=\"overflow: hidden; height: 40px;\"&gt;Alzheimer's Disease (AD) is a neurodegenerative disease characterized by profound memory impairments, synaptic loss, neuroinflammation, and hallmark pathological markers. High-fat diet (HFD) consumption increases the risk of developing AD even after controlling for metabolic syndrome, pointing to a role of the diet itself in increasing risk. In AD, the complement system, an arm of the immune system which normally tags redundant or damaged synapses for pruning, becomes pathologically overactivated leading to tagging of healthy synapses. While the unhealthy diet to AD link is strong, the underlying mechanisms are not well understood in part due to confounding variables associated with long-term HFD which can independently influence the brain. Therefore, we experimented with a short-term diet regimen to isolate the diet's impact on brain function without causing changes in metabolic markers.\nThis project investigated potential mechanisms underlying cognitive impairments evoked by short-term diet consumption using the 3xTg-AD model. In chapter 1 we discuss the link between HFD and AD and outline the current findings and hypothesis regarding of relevant mechanisms. In chapter 2 we characterize the effect of short-term HFD on 1) memory, 2) neuroinflammation including complement, 3) AD pathology markers, 4) synaptic markers, and 5) in vitro microglial synaptic phagocytosis in the 3xTg-AD mouse model. In chapter 3 we analyze two potential mechanisms underlying HFD-mediated AD vulnerability: toll-like receptor 4 (TLR4)-evoked neuroinflammation and complement system activation. Finally, in chapter 4 we validate the absence of glucose modifications as an effect of the diet and drug treatment, explore potential mitochondrial mechanisms in the hippocampus, and evaluate the diet's impact on the pre-frontal cortex (PFC).\nFollowing the consumption of either standard chow or HFD, 3xTg-AD mice exhibited impaired long-term memory performance which was associated with increased level (open full item for complete abstract)&lt;/div&gt; &lt;a href=\"#\" data-ctrl=\"\" class=\"\"&gt;... &lt;em&gt;More&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\n              &lt;span class=\"t-SearchResults-misc\"&gt;&lt;b&gt;Committee:&lt;/b&gt; Ruth Barrientos (Advisor); Benedetta Leuner (Committee Member); Nikki Kokiko-Cochran (Committee Member); Harry Fu (Committee Member)&lt;/span&gt;\n              &lt;span class=\"t-SearchResults-misc\"&gt;&lt;b&gt;Subjects:&lt;/b&gt; Neurosciences&lt;/span&gt;\n              &lt;!-- span class=\"t-SearchResults-misc\"&gt;Score: 100&lt;/span --&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n\nThis setup allows the site to update content without reloading the entire page.\n‚ö†Ô∏è Why requests and BeautifulSoup Alone Won‚Äôt Work\nIf you try to scrape the page using requests and BeautifulSoup, you‚Äôll notice that the response only contains the static HTML shell‚Äînone of the dynamically loaded search results are included. That‚Äôs because the content is rendered by JavaScript, which requests cannot execute.\nüõ†Ô∏è Workaround for Small-Scale Projects\nIf you‚Äôre working on a small project and just need to extract a limited number of results, you can manually save the page and parse it locally:\n‚úÖ Steps:\n\nSet Results Per Page to 100 to minimize the number of pages you need to save.\nRight-click on the results per page and choose Save As.\nSelect **Webpage, Single File (*.mhtml)** as the format.\nOpen the saved .mhtml file using Notepad or any other plain text editor.\nDelete everything above the &lt;!DOCTYPE html&gt; line.\nSave the file again, changing the extension from .mhtml to .html.\n\nOnce saved as a .html file, you can read and parse it using BeautifulSoup.\n\n\n\n\nExercise 2: Save HTML\n\n\n\n\nSave your results from Exercise 1 by following the steps listed above.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3: Parse HTML\n\n\n\n\n\nUse BeautifulSoup to extract the following elements from your saved HTML file:\n\n\ntitle\n\n\nauthor\n\n\ndegree\n\n\ndegre_year\n\n\nadvisor\n\n\ncommittee_members\n\n\nsubjects\n\n\n\n\nRemember to read the HTML` file into Python first. Export the results to a CSV file.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nresults=pd.DataFrame()\n\ncontents = open('data/results.html').read()\nsoup = BeautifulSoup(contents, 'html.parser')\netds = soup.find_all(attrs={\"class\":'3D\"t-SearchResults-content\"'})\n\nfor each_etd in etds:\n\n    row={}\n    title=each_etd.find(\"h3\").find(\"a\").text.replace('=\\n','').replace('=',' ')\n    row['title']=title\n    author=each_etd.find('span', {\"class\":'3D\"t-SearchResults-author\"'}).text.replace('=\\n','').split('\\n')[0]\n    row['author']=author\n    degree=each_etd.find('p', {\"class\":'3D\"t-SearchResults-degree\"'}).text.split(',')[0].strip()\n    row['degree']=degree\n    degree_year=each_etd.find('p', {\"class\":'3D\"t-SearchResults-degree\"'}).text.split(',')[2].strip()\n    row['degree_year']=degree_year\n    misc_results=author=each_etd.find_all('span', {\"class\":'3D\"t-SearchResults-misc\"'})\n\n    advisors=[]\n    committee=[]\n    subjects=[]\n    for misc in misc_results:\n        if \"Committee:\" in misc.text:\n            members=misc.text.replace('=\\n','').replace(\"Committee: \",\"\").split(';')\n            for member in members:\n                if \"advisor\" in member.lower():\n                    advisors.append(member.split('(')[0].replace('=\\n','').strip())\n                    committee.append(member.split('(')[0].replace('=\\n','').strip())\n                else:\n                    committee.append(member.split('(')[0].strip())\n\n        elif \"Subjects:\" in misc.text:\n            subjs=misc.text.replace('=\\n','').replace(\"Subjects: \",\"\").split(';')\n            for subject in subjs:\n                subjects.append(subject.strip())\n\n                \n    advisors=(';').join(advisors).rstrip(';')\n    row['advisors']=advisors\n    committee=(';').join(committee).rstrip(';')\n    row['committee']=committee\n    subjects=(';').join(subjects).rstrip(';')\n    row['subjects']=subjects\n    \n    df_row=pd.DataFrame(row, index=[0])\n    results=pd.concat([df_row, results], axis=0, ignore_index=True)\n    \nresults.to_csv('data/npsg.csv')\n\n\n\n\nFor larger projects, consider using tools such as Requests-HTML or Selenium, which are capable of rendering JavaScript‚Äîmaking them ideal for scraping dynamic web content that standard libraries like requests and BeautifulSoup can‚Äôt access.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 6. OhioLINK ETD"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Stephen Few in Now You See It: Simple Visualization Techniques for Quantitative Analysis defines data visualization as ‚Äúall types of visual representations that support the exploration, examination, and communication of data‚Äù. In essence, it‚Äôs about turning raw data into visuals that help us understand and communicate insights more effectively.\nVisualizing data allows us to quickly summarize complex information, making it easier to digest and interpret. It helps reveal patterns and trends that might otherwise go unnoticed when examining raw numbers alone‚Äîsuch as those illustrated by Anscombe‚Äôs Quartet. By transforming data into visual formats we can make information more accessible, meaningful, and easier to communicate to a wider audience."
  },
  {
    "objectID": "index.html#know-your-audience",
    "href": "index.html#know-your-audience",
    "title": "Data Visualization",
    "section": "üë• Know your audience",
    "text": "üë• Know your audience\nBefore creating a chart, think about your goal:\n\nAre you explaining a concept or exploring a pattern?\nDoes your audience need a simple overview or a detailed analysis?\n\nTailoring your visuals to your audience ensures your message is clear and impactful."
  },
  {
    "objectID": "index.html#choose-the-right-chart-for-your-data",
    "href": "index.html#choose-the-right-chart-for-your-data",
    "title": "Data Visualization",
    "section": "üìä Choose the right chart for your data",
    "text": "üìä Choose the right chart for your data\nPicking the right chart type is crucial‚Äîand sometimes tricky. Luckily, there are great tools to help:\nThe Financial Times Visual Vocabulary offers a visual taxonomy‚Äîor categorized collection of chart types‚Äîbased kind of relationship or message you want to show (e.g., change over time, distribution, correlation. It helps answer the question: ‚ÄúWhat‚Äôs the best way to visualize this data?‚Äù\n\n\n\nFinancial Times Visual Vocabulary\n\n\nAbela‚Äôs Chart Chooser helps you select a chart based on how many variables (measures and dimensions) you are working with.\nStephanie Evergreen‚Äôs Quantitative Chart Chooser, featured on the inside front cover of her book Effective Data Visualization: the Right Chart for the Right Data), helps guide your chart selection based on your communication goal.‚Äîwhether you‚Äôre emphasizing a single number, showing change over time, or comparing data to a benchmark. On the inside back cover, her Qualitative Chart Chooser, along with dedicated chapter in the book, offers practical guidance on how to effectively visualize qualitative data."
  },
  {
    "objectID": "index.html#use-colors-and-fonts-wisely",
    "href": "index.html#use-colors-and-fonts-wisely",
    "title": "Data Visualization",
    "section": "üé® Use colors and fonts wisely",
    "text": "üé® Use colors and fonts wisely\n\nLimit your color palette to maintain clarity and accessbility.\nUse color to emphasize key point.\nStick to clean, readable fonts and avoid unnecessary styling."
  },
  {
    "objectID": "index.html#less-is-better.",
    "href": "index.html#less-is-better.",
    "title": "Data Visualization",
    "section": "‚úÇÔ∏è Less is better.",
    "text": "‚úÇÔ∏è Less is better.\n\nSimplify your chart.\nRemove tick marks and grid lines that do not add value.\nLabel data point directly when possible to reduce cognitive load.\nAim for clarity over complexity ‚Äî less is often more!"
  },
  {
    "objectID": "index.html#add-interactivity-when-appropriate",
    "href": "index.html#add-interactivity-when-appropriate",
    "title": "Data Visualization",
    "section": "üñ±Ô∏è Add interactivity (when appropriate)",
    "text": "üñ±Ô∏è Add interactivity (when appropriate)\nInteractive tools like Tableau, RShiny, or Power BI allow users to explore data on their own. - Use filters to let users drill down from high-level overviews to detailed views. - Structure interactivity to support meaningful exploration."
  },
  {
    "objectID": "index.html#provide-context-and-clear-instructions",
    "href": "index.html#provide-context-and-clear-instructions",
    "title": "Data Visualization",
    "section": "üß≠ Provide context and clear instructions",
    "text": "üß≠ Provide context and clear instructions\n\nDon‚Äôt assume your audience knows how to interpret your chart.\nInclude a clear title that communicates the key takeaway.\nAdd brief instructions or a help link if your visualization is interactive.\nProvide context for your data source(s).\nAlways cite your data source(s)."
  },
  {
    "objectID": "index.html#test-your-visualization",
    "href": "index.html#test-your-visualization",
    "title": "Data Visualization",
    "section": "üß™ Test your visualization",
    "text": "üß™ Test your visualization\nBefore sharing, ask a colleague or friend to critique your chart: - Do they understand the message? - Can they use the filters (if interactive)? - Does the visual guide them to the insight you intended?"
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python",
    "section": "",
    "text": "Python is a free, general purpose programming language widely used in both academia and industry. It provides a rich ecosystem of libraries, such as Matplotlib, Plotly, Pandas, and BeautifulSoup, that support tasks like data collection, cleaning, and visualization.\n\nFree Resources\nExplore a collection of open-access, discipline-specific textbooks that are completely free to use. Each resource includes highly valuable Jupyter notbooks, which you can download or clone to reinforce your learning. Practice the concepts using the included demo datasets or apply them to your own data for a more personalized experience.\n\n\n\n\nCULTURAL ANALYTICS & PYTHON\n\n\n\n\nIntroduction to Cultural Analytics & Python\n\n\n\n\n\n\n\nby Melanie Walsh, 2024.\n\n\nü•áWinner of ‚ÄúBest DH Training Material‚Äù 2021.\n\n\n\n\n\n\n\nPYTHON FOR DIGIAL HUMANISTS\n\n\n\n\nIntroduction to Python for Digital Humanists\n\n\n\n\n by William J.B. Mattingly\n\n\nBoca Raton : CRC Press, 2023.\n\n\n\n\n\n\n\nCODING FOR ECONOMISTS\n\n\n\n\nCoding for Economists\n\n\n\n\n by Arthur Turrell, 2023.\n\n\n\n\n\n\n\nBooks\n\n\n\n\nPYTHON CRASH COURSE\n\n\n\n\nPython crash course : a hands-on, project-based introduction to programming, 3rd edition\n\n\n\n\nby Eric MatthesSan Francisco: No Starch Press, 2023.\n\n\nThis beginner-friendly book uses clear, accessible language and practical coding examples to introduce core Python concepts‚Äîlike variables, data types, methods, and functions‚Äîwhile encouraging you to actively apply what you learn through hands-on practice.\n\n\n\n\n\n\n\nAUTOMATE THE BORING STUFF\n\n\n\n\nAutomate the boring stuff with Python : practical programming for total beginners, 3rd edition\n\n\n\n\nby Al SweigartSan Francisco: No Starch Press, 2025.\n\n\nThis book starts with the basics, then dives into real-world tasks like web scraping, pattern matching with regular expressions, reading documents, handling .csv and .json files, scheduling tasks, manipulating images, and much more. Each chapter includes hands-on practice questions to help reinforce your skills and boost your confidence as a programmer.\n\n\n\n\n\n\n\nPYTHON TOOLS FOR SCIENTISTS\n\n\n\n\nPython tools for scientists : an introduction to using Anaconda, Jupyterlab, and Python‚Äôs scientific libraries\n\n\nby Lee VaughanSan Francisco: No Starch Press, 2023.\n\n\n\n\nby Lee VaughanSan Francisco: No Starch Press, 2023.\n\n\n\n\n\n\n\nO‚ÄôReilly Online Learning\nFor additional books and learning materials, the O‚ÄôReilly Online Learning: Academic/Public Library Edition collection provides extensive access to eBooks and videos in computer science, IT, business, and related subjects, featuring content from O‚ÄôReilly and other top publishers. This resource is provided by University Libraries and is available to all Ohio State faculty, students, and staff with a valid osu.edu email address.\n\n\nWorkshops and Events\nUniversity Libraries offers a variety of data skills workshops and events for Ohio State faculty, students and staff throughout the academic year.",
    "crumbs": [
      "Library Resources",
      "TOOLS",
      "... Python"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "What tool is right for my project?",
    "section": "",
    "text": "How do I decide what data visualization tool is right for my project? The answer really depends on your project, timeframe, and skillset. If you have fewer than 1,000 rows of data, Excel may be completely appropriate. For larger projects, you may need to use an analytics or visualization program like Tableau, or a coding language like python or R.",
    "crumbs": [
      "Library Resources",
      "TOOLS",
      "What tool is right for my project?"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html",
    "href": "pubmed_author_affiliations.html",
    "title": "Lesson 5. Author Affiliations",
    "section": "",
    "text": "The Entrez E-utilities offer a suite of tools that enable researchers to automate the search and retrieval of scientific information from PubMed and other databases maintained by the National Center for Biotechnology Information (NCBI). In Lesson 4 we identified an active NIH funded research project at The Ohio State University and generated a list of PMIDs (PubMed Identifiers) associated with each project. In Lesson 5, we will use this list of PMIDs with the Entrez E-utilities to gather the affiliations of each author listed on the corresponding articles. We will also begin to explore regular expressions, a tool used across programming languages for matching and manipulating string data.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html#data-skills-concepts",
    "href": "pubmed_author_affiliations.html#data-skills-concepts",
    "title": "Lesson 5. Author Affiliations",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nWorking with APIs\nManipulating text\nRegular expressions",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html#learning-objectives",
    "href": "pubmed_author_affiliations.html#learning-objectives",
    "title": "Lesson 5. Author Affiliations",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nLocate API documentation and identify key components required to formulate an API request\nParse an API response and store extracted data.\nUtilize regular expressions to search, match, and manipulate text.\n\nThis tutorial is designed to support multi-session workshops hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts visit the Python - Mastering the Basics tutorial.\n\n\nTip\n\n\nThe Enztrez E-utilities manual can be overwhelming to read and comprehend at first. The example code is written in Perl script, not Python and the documentation assumes you are familiar working with APIs and programming tools. Before starting this tutorial, ask Copilot to explain entrez e-utilities. Copilot returns a useful summary of the key components and functionalities of these tools, explains how they work, provides an example workflow, and identifies potential use cases. As you become more comfortable working with APIs, you can revisit the Enztrez E-utilities manual to learn how to do more complex tasks.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html#what-is-efetch",
    "href": "pubmed_author_affiliations.html#what-is-efetch",
    "title": "Lesson 5. Author Affiliations",
    "section": "What is EFetch?",
    "text": "What is EFetch?\nEFetch is a utility provided by NCBI‚Äôs Entrez system that retrieves detailed records for a list of unique identifiers (like PMIDs) from databases such as PubMed.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html#where-are-author-affiliations",
    "href": "pubmed_author_affiliations.html#where-are-author-affiliations",
    "title": "Lesson 5. Author Affiliations",
    "section": "Where are author affiliations?",
    "text": "Where are author affiliations?\nIn PubMed records, author affiliations are embedded in the XML under:\n\n&lt;Author&gt;\n¬† &lt;AffiliationInfo&gt;\n¬†¬†¬† &lt;Affiliation&gt;...&lt;/Affiliation&gt;\n¬† &lt;/AffiliationInfo&gt;\n&lt;/Author&gt;\n\n\n\n\n\nExercise 1: Inspect a PubMed record\n\n\n\n\n\nUse the first PMID from your list from Lesson 4.\n\n\n\nSearch for it on PubMed.\n\n\nObserve:\n\n\nThe structure of the URL\n\n\nThe location of author affiliations in the record\n\n\nInspect the author elements.\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nURL = https://pubmed.ncbi.nlm.nih.gov/39773557/\n\n\nNote: The PMID is at the end of the URL",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html#step-1.-construct-an-efetch-request",
    "href": "pubmed_author_affiliations.html#step-1.-construct-an-efetch-request",
    "title": "Lesson 5. Author Affiliations",
    "section": "Step 1. Construct an EFetch request",
    "text": "Step 1. Construct an EFetch request\nTo retrieve XML data for a PubMed article, use the following components: - Base URL: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi - Parameters: - Database name: ?db=pubmed - Unique identifier: &id=39773557 - API key: &api_key=INSERT YOUR API KEY HERE\nRequired parameters for an EFetch request depend on the specific Entrez database you are querying. For PubMed, the default EFetch response format is XML.\nTo manage request volume, the NCBI enforces rate limits: - Without an API key: 3 requests per second - With an API key: up to 10 requests per second\nWhile you can view a single XML record without an API key, completing the exercises in this tutorial requires one. You can obtain an API key by visiting the Settings page of your NCBI account.\n\n\n\n\nExercise 2: Construct a request\n\n\n\n\nUse the EFetch utility to retrieve the XML record for the first PMID from the list you generated in Lesson 4. This XML will contain detailed metadata about the article, including author affiliations (if available).\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nExample:\n\n\nhttps://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=39773557",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html#step-2.-identify-python-libraries-for-project",
    "href": "pubmed_author_affiliations.html#step-2.-identify-python-libraries-for-project",
    "title": "Lesson 5. Author Affiliations",
    "section": "Step 2. Identify Python libraries for project",
    "text": "Step 2. Identify Python libraries for project\nThe following Python libraries are needed for this project: - requests ‚Äì to make HTTP requests - pandas ‚Äì to manage and store data - BeautifulSoup‚Äì to parse XML and extract affiliations\n\n\n\n\nExercise 3: Write and test code\n\n\n\n\n\nUsing the list of PMIDs you generated in Lesson 4, write and test a Python script that:\n\n\nIterates through your list of PMIDs\n\n\nSends an EFetch request for each\n\n\nParses the XML to extract author affiliations\n\n\nHandles missing or incomplete data gracefully\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup as bs\nimport pandas as pd\nimport time\nfrom datetime import date\n\n#1. Create a last_updated variable with today's date.\ntoday = date.today()\nlast_updated=today\n\n#2. Create list of PMIDs\npmids=['39773557', '39656677', '37398045', '39229161', '39713331', '39315813', '38338688', '36721057', '37322069']\n\n#3.Create a dataframe to store the search results. \nauthor_affiliations=pd.DataFrame(columns=[\"pmid\",\"name\",\"affiliation\",\"last_updated\"])\n\n#4. Use requests, BeautifulSoup, and the EFetch utility to retrieve author affiliations.\n# Store results in a DataFrame.\ncount=0\nfor each_record in pmids:\n    # try:\n    count += 1\n    print('starting record '+str(count)+': '+str(each_record))\n    search_url=\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=\"+str(each_record)+\"&api_key=INSERT YOUR API KEY HERE\"\n    xml_data=requests.get(search_url).text\n    soup = bs(xml_data, 'xml')\n    records=soup.find('PubmedArticle')\n    pmid=records.PMID.text\n    authors=records.find_all(\"Author\")\n    for each_author in authors:\n        if each_author.LastName != None:\n            lastname=each_author.LastName.text\n        else:\n            lastname=''\n        if each_author.ForeName != None:\n            forename=each_author.ForeName.text\n        else:\n            forename=''\n        if lastname != '' and forename != '':\n            name=lastname+', '+forename\n        else:\n            name=''\n        \n\n        if each_author.Affiliation != None:\n            affiliation=each_author.Affiliation.text\n        else:\n            affiliation=''\n        print(f\"{name}, {affiliation}\")\n                \n        row={\n            \"pmid\": pmid,\n            \"name\": name,\n            \"affiliation\": affiliation,\n            \"last_updated\": last_updated\n\n            }\n        author_info=pd.DataFrame(row, index=[0])\n        author_affiliations = pd.concat([author_info,author_affiliations], axis=0)\n        time.sleep(0.15)\n        \n#5. Export results to csv        \nauthor_affiliations.to_csv('data/pubmed_author_affiliations.csv')",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html#regular-expressions-regex",
    "href": "pubmed_author_affiliations.html#regular-expressions-regex",
    "title": "Lesson 5. Author Affiliations",
    "section": "Regular Expressions (regex)",
    "text": "Regular Expressions (regex)\nAnalyzing author and affiliation data can be messy due to: - Inconsistent naming conventions - Variations in institutional affiliation formats - Ambiguities in author identify.\n\n\nTip: ORCID\n\n\n\n\nCreate your ORCID iDto help researchers distinguish your work from others and better track the impact of your work.\n\n\nPubMed now embeds the ORCID in Author tags. Several journals and funding agencies also now require ORCID iDs for submissions. See Tracking and Enhacing the Impact of your Research for more information.\n\n\n\nRegular expressions (regex) match patterns in text. Often described as wildcards on steroids, regular expressions help: - Validate patterns (e.g., ZIP codes: (^$)) - Extract variations (e.g., ‚Äúha?ematology‚Äù matches both ‚Äúhematology‚Äù and ‚Äúhaematology‚Äù) - Replace text (e.g., re.sub(r‚Äô, ‚ÄòOhio‚Äô, text))\nRegular expressions are included in several programming langauges and software programs including Python, JavaScript, and Tableau.\n\nCommon Regex Patterns\n\n\n\nPattern\nMatches\n\n\n\n\n[A-Z]\nAny uppercase letter\n\n\n[a-z]\nAny lowercase letter\n\n\n[0-9]{5}\nExactly 5 digits\n\n\n^Ohio\nStarts with ‚ÄúOhio‚Äù\n\n\nState$\nEnds with ‚ÄúState‚Äù\n\n\nha?ematology\n‚Äúhematology‚Äù or ‚Äúhaematology‚Äù\n\n\nOhio State\\|OSU\n‚ÄúOhio State‚Äù or ‚ÄúOSU‚Äù\n\n\n\nMetacharacters are special symbols in regular expressions that represent patterns rather than literal characters. To match them as literal characters, you must escape them with a backslash ( ¬†).\n\n\nCommon metacharacters and their functions\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nExample\n\n\n\n\n[ ]\nA set or range of characters\n[a-f] matches any lowercase letter from a to f\n\n\n\\\nStarts a special sequence\n\\w matches any word character (letter, digit, or underscore)\n\n\n.\nAny character except newline\nd.g matches ‚Äúdog‚Äù, ‚Äúdig‚Äù, ‚Äúdug‚Äù, etc.\n\n\n^\nStart of a string\n^Ohio matches any string that starts with ‚ÄúOhio‚Äù\n\n\n$\nEnd of a string\nState$ matches any string that ends with ‚ÄúState‚Äù\n\n\n.*\nZero or more of the preceding character\nh*matology matches ‚Äúhematology‚Äù, ‚Äúhaematology‚Äù, etc.\n\n\n+\nOne or more of the preceding character\nspe+d matches ‚Äúsped‚Äù, ‚Äúspeed‚Äù, etc.\n\n\n?\nZero or one of the preceding character\ntravel?ling matches ‚Äútraveling‚Äù, ‚Äútravelling‚Äù. etc.\n\n\n{ }\nExactly a specified number of repetitions\n[0-9]{5} matches any 5-digit number\n\n\n( )\nGrouping or capturing\nThe (Ohio) State University extracts ‚ÄúOhio‚Äù\n\n\n\\|\nLogical OR\nOhio State\\|OSU matches ‚ÄúOhio State‚Äù or ‚ÄúOSU‚Äù\n\n\n\n\n\nLEARNING RESOURCES\n\n\n\n\nREGEX 101\n\n\n\n\nregular expressions 101: build, test, and debug regex\n\n\n\nregular expressions 101: build, test, and debug regex is an interactive tool that helps you build, test, and debug regular expressions across multiple programming languages. It lets you test your regex against sample text, provides real-time explanations as you type, and includes a searchable reference for regex syntax.\n\n\n\n\n\n\nEFFECTIVE DATA VISUALIZATION\n\n\n\n\nLearning Regular Expressions\n\n\n\nLearning Regular Expressions by Ben Forta** is available through the Libraries‚Äô O‚ÄôReilly Online Learning collection of technical books and videos. Each chapter is structured as a lesson, guiding you through how to match individual characters or sets of characters, use metacharacters, and more‚Äîmaking it a practical resource for mastering regex step by step.",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "pubmed_author_affiliations.html#re-module",
    "href": "pubmed_author_affiliations.html#re-module",
    "title": "Lesson 5. Author Affiliations",
    "section": "re module",
    "text": "re module\nTo work with regular expressions in Python, start by importing the built-in re module:\n\nimport re\n\nüîó See re module documentation.\n\nCommonly used re methods\n\n\nre.match( )\n\nre.match(pattern, string, flags=0)\n\n\nChecks for a match only at the beginning of the string.\nReturns a match object if found, otherwise None.\n\nExample:\n\nimport pandas as pd\nimport re\n\naddresses=pd.read_csv('data/pubmed_author_affiliations.csv')\naddresses=addresses.dropna(subset='affiliation') #drops rows with null affiliation values\n\nfor idx, row in addresses.iloc[0:10].iterrows():\n    affiliation=str(row.affiliation)\n    print(affiliation)\n    osu_match=re.match(r\"Ohio State University\",affiliation) \n    if osu_match:\n        print(f\" MATCH {osu_match.group()}: {affiliation}\")\n\nThe Ohio State Biochemistry Program, The Ohio State University, Columbus, OH, 43210, USA. fu.978@osu.edu.\nThe Ohio State Biochemistry Program, The Ohio State University, Columbus, OH, 43210, USA.\nCenter for Cancer Metabolism, The Ohio State University Comprehensive Cancer Center, Columbus, OH, 43210, USA.\nDepartment of Biological Chemistry and Pharmacology, The Ohio State University, Columbus, OH, 43210, USA.\nDepartment of Biological Chemistry and Pharmacology, The Ohio State University, Columbus, OH, 43210, USA.\nThe Ohio State Biochemistry Program, The Ohio State University, Columbus, OH, 43210, USA.\nDepartment of Biological Chemistry and Pharmacology, The Ohio State University, Columbus, OH, USA. fu.978@osu.edu.\nDepartment of Biological Chemistry and Pharmacology, The Ohio State University, Columbus, OH, USA.\nDepartment of Physics, Northeastern University, Boston, MA 02115, USA.\nDepartment of Chemistry and Biochemistry, Center for RNA Biology, Ohio State University, Columbus, OH 43210, USA.\n\n\nüîó See re.match() documentation.\n\n\nre.search( )\n\nre.search(pattern, string, flags=0)\n\n\nScans through the string and returns the first match of the pattern.\nReturns a match object or None.\n\nExample:\n\nimport pandas as pd\nimport re\n\naddresses=pd.read_csv('data/pubmed_author_affiliations.csv')\naddresses=addresses.dropna(subset='affiliation')\n\nfor idx, row in addresses.iloc[0:10].iterrows():\n    affiliation=str(row.affiliation)\n    # print(affiliation)\n    osu_search=re.search(r\"The Ohio State University\",affiliation) \n    if osu_search:\n        print(osu_search.group())\n    else:\n        print(f\"No match: affiliation = {affiliation}\")\n\nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\nNo match: affiliation = Department of Physics, Northeastern University, Boston, MA 02115, USA.\nNo match: affiliation = Department of Chemistry and Biochemistry, Center for RNA Biology, Ohio State University, Columbus, OH 43210, USA.\n\n\nüîó See re.search() documentation.\n\n\nre.findall( )\n\nre.findall(pattern, string, flags=0)\n\n\nReturns all non-overlapping matches of the pattern in the string as a list.\n\nExample:\n\n# HOW MANY TORTOISES AND TURTLES ARE IN THIS LIST OF ANIMALS?\n\nimport pandas as pd\nimport re\n\ndf=pd.read_csv('data/animals_tortoises.csv')\nanimals=df.common_name.tolist()\nanimals=','.join(animals)\nprint('LIST OF ANIMALS')\nprint(animals)\n\ntortoises_turtles=re.findall(r\"tortoise|turtle\", animals)\nprint('ANSWER')\nprint(f\"There are {len(tortoises_turtles)} tortoises and turtles in this list of animals.\")\n\nLIST OF ANIMALS\nabyssinian-ground-hornbill,addax,african-clawed-frog,african-pancake-tortoise,african-plated-lizard,aldabr-tortoise,allens-swamp-monkey,alligator-lizard,alligator-snapping-turtle,alpaca\nANSWER\nThere are 3 tortoises and turtles in this list of animals.\n\n\nüîó See re.findall() documentation.\n\n\nre.sub( )\n\nre.sub(pattern, repl, string, count=0, flags=0)\n\n\nReplaces all occurrences of the pattern in the string with the replacement text (repl).\nYou can limit the number of replacements using the count parameter.\n\nExamples:\n\n# FIND TORTOISES AT THE NATIONAL ZOO AND REPLACE THE COMMON_NAME WITH \"SLOW TORTOISE\"\nimport pandas as pd\nimport re\n\ndf=pd.read_csv('data/animals_tortoises.csv')\nanimals=df.common_name.tolist()\nanimals=','.join(animals)\npattern=\"tortoise|turtle\"\ntortoises_slow=re.sub(pattern,\"SLOW TORTOISE,\",animals)\ntortoises_slow\n\n'abyssinian-ground-hornbill,addax,african-clawed-frog,african-pancake-SLOW TORTOISE,,african-plated-lizard,aldabr-SLOW TORTOISE,,allens-swamp-monkey,alligator-lizard,alligator-snapping-SLOW TORTOISE,,alpaca'\n\n\nüîó See re.sub() documentation.\n\n\n\n\nExercise 4: Use re to normalize affiliations\n\n\n\n\nUse regular expressions to create a list of institution names from the affiliations list you generated in Exercise 3.\n\n\n\n\n\n\nSolution:\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup as bs\nimport pandas as pd\nimport time\nfrom datetime import date\nimport re\n\n#1. Create a last_updated variable with today's date.\ntoday = date.today()\nlast_updated=today\n\n#2. Create list of PMIDs\npmids=['39773557', '39656677', '37398045', '39229161', '39713331', '39315813', '38338688', '36721057', '37322069']\n\n#3.Create a dataframe to store the search results. \nauthor_affiliations=pd.DataFrame(columns=[\"pmid\",\"name\",\"affiliation\",\"institution\",\"last_updated\"])\n\n#4. Use requests, BeautifulSoup, and the EFetch utility to retrieve author affiliations.\n# Store results in a DataFrame.\ncount=0\nfor each_record in pmids:\n    # try:\n    count += 1\n    print('starting record '+str(count)+': '+str(each_record))\n    search_url=\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=\"+str(each_record)+\"&api_key=INSERT API KEY HERE\"\n    xml_data=requests.get(search_url).text\n    soup = bs(xml_data, 'xml')\n    records=soup.find('PubmedArticle')\n    pmid=records.PMID.text\n    authors=records.find_all(\"Author\")\n    for each_author in authors:\n        if each_author.LastName != None:\n            lastname=each_author.LastName.text\n        else:\n            lastname=''\n        if each_author.ForeName != None:\n            forename=each_author.ForeName.text\n        else:\n            forename=''\n        if lastname != '' and forename != '':\n            name=lastname+', '+forename\n        else:\n            name=''\n        \n\n        if each_author.Affiliation != None:\n            affiliation=each_author.Affiliation.text\n\n            ohio_state=re.search(r\"Ohio State\", affiliation)\n            harvard_medical_school=re.search(r\"Harvard Medical School\", affiliation)\n            institut_genetique=re.search(r\"Institut de G√©n√©tique et de Biologie Mol√©culaire et Cellulaire\", affiliation)\n            johns_hopkins=re.search(r\"Johns Hopkins University\", affiliation)\n            mcgill=re.search(r\"McGill University\", affiliation)\n            nci=re.search(r\"National Cancer Institute\", affiliation)\n            nidcd=re.search(r\"National Institute on Deafness and Other Communication Disorders\", affiliation)\n            northeastern=re.search(r\"Northeastern University\", affiliation)\n            u_bristol=re.search(r\"University of Bristol\", affiliation)\n            u_maryland=re.search(r\"University of Maryland\", affiliation)\n            u_virginia=re.search(r\"University of Virginia\", affiliation)\n            vicosa=re.search(r\"Universidade Federal de Vi√ßosa\", affiliation)\n            if ohio_state:\n                institution=\"The Ohio State University\"\n            elif harvard_medical_school:\n                institution=harvard_medical_school.group()\n            elif institut_genetique:\n                institution=institut_genetique.group()\n            elif johns_hopkins:\n                institution=johns_hopkins.group()\n            elif mcgill:\n                institution=mcgill.group()\n            elif nci:\n                institution=nci.group()\n            elif nidcd:\n                institution=nidcd.group()\n            elif northeastern:\n                institution=northeastern.group()\n            elif u_bristol:\n                institution=u_bristol.group()\n            elif u_maryland:\n                institution=u_maryland.group()\n            elif u_virginia:\n                institution=u_virginia.group()\n            elif vicosa:\n                institution=vicosa.group()\n\n        else:\n            affiliation=''\n        print(f\"{name}, {affiliation}\")\n\n        row={\n            \"pmid\": pmid,\n            \"name\": name,\n            \"affiliation\": affiliation,\n            \"institution\": institution,\n            \"last_updated\": last_updated\n\n            }\n        author_info=pd.DataFrame(row, index=[0])\n        author_affiliations = pd.concat([author_info,author_affiliations], axis=0)\n        time.sleep(0.15)\n        \n#5. Export results to csv        \nauthor_affiliations.to_csv('pubmed_author_affiliations.csv')",
    "crumbs": [
      "Python Tutorials",
      "WEBSITES AND APIS",
      "Lesson 5. Author Affiliations"
    ]
  },
  {
    "objectID": "tableau_continue_learning.html",
    "href": "tableau_continue_learning.html",
    "title": "üöÄ Continue Learning Tableau",
    "section": "",
    "text": "Congratulations on completing the Tableau for Research workshop! Here are some recommendations to help you continue your Tableau learning journey:\n\nJoin a community\nDid you know The Ohio State University has an active Tableau User Group? Join a vibrant bunch of campus Tableau enthusiasts who meet regularly to:\n\nShare tips and tricks\nExplore new ways to visualize data\nPrepare for Tableau certification exams\n\nThe group‚Äôs Microsoft Teams channel has over 200 members and is a great place to ask questions, get feedback, and stay inspired.\nüëâ Join directly with the code: e880cmx\n\n\nTake another training class\nYou already know that completing a training class can help you pause your daily responsibilities to learn a new skill. There are several online, in-person, and hybrid training opportunities available to you. Keep your eye on the Research Commons workshop offerings ‚Äì or try a LinkedIn Learning tutorial.\n\n\nRead! Read! Read!\nSearch University Libraries catalog for data visualization books, videos, and more. Follow respected blogs by Tableau teachers and innovators. Bookmark effective or innovative data visualizations you find on websites that resonate with you.\n\n\nPractice, practice, practice\nA master musician, artist, or chefs practices his or her craft. Set aside time each week to practice Tableau.\n\nTry weekly challenges like MakeoverMonday\nJoin a Tableau Community Project.\nRecreate visualizations you admire to learn new techniques\n\n\n\nFollow Tableau Public\nGet inspired by how others are using Tableau in creative and impactful ways. Explore the Viz of the Day on Tableau Public to see what‚Äôs possible‚Äîand spark your own ideas.\nrun file again",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "üöÄ Continue Learning Tableau"
    ]
  },
  {
    "objectID": "tableau.html",
    "href": "tableau.html",
    "title": "Tableau",
    "section": "",
    "text": "Tableau is a powerful data visualization tool with a user-friendly drag-and-drop interface and no-code features that make it easy to explore different visual perspectives and find the best way to present your data. Capable of handling datasets with over 25 million rows, Tableau supports flexible, visual thinking and encourages experimentation as you analyze and communicate insights.\n\nFree Resources\n\nTableau‚Äôs Academic Programs provides free licenses and learning resources for students and faculty.\nUniversity Libraries offers free Tableau training throughout the year, tailored for faculty, graduate students and other university researchers.\nA wide range of training videos and books is also available through University Libraries.\n\n\n\nBooks\n\n\n\n\nPRACTICAL TABLEAU\n\n\n\n\nPractical Tableau : 100 tips, tutorials, and strategies from a Tableau Zen Master\n\n\n\n\nby Ryan SleeperSebastopol, CA : O‚ÄôReilly Media, 2018.\n\n\nWith screenshots, step-by-step instructions and succinct explanations of key Tableau concepts this book will quickly orient you to Tableau. Sleeper covers basic and advanced chart types as well as parameters, filters, calculated fields and more, allowing you to create interactive visualizations for presentations, data exploration, or analysis.\n\n\n\n\n\n\n\nINNOVATIVE TABLEAU\n\n\n\n\nInnovative Tableau : 100 more tips, tutorials, and strategies\n\n\n\n\nby Ryan SleeperSebastopol, CA : O‚ÄôReilly Media, 2020.\n\n\nWith an additional 100 chapters, this book continues where Practical Tableau : 100 tips, tutorials, and strategies from a Tableau Zen Master left off, offering instructions for conditionally formatting text, adding images to tooltips, and showing you how to use parameter and set actions. Sleeper introduces additional chart types, including diverging bar charts, waterfall charts, gauges, and more.\n\n\n\n\n\n\n\nO‚ÄôReilly Online Learning\nFor additional books and learning materials, the O‚ÄôReilly Online Learning: Academic/Public Library Edition collection provides extensive access to eBooks and videos in computer science, IT, business, and related subjects, featuring content from O‚ÄôReilly and other top publishers. This resource is provided by University Libraries and is available to all Ohio State faculty, students, and staff with a valid osu.edu email address.\n\n\nWorkshops and Events\nUniversity Libraries offers a variety of data skills workshops and events for Ohio State faculty, students and staff throughout the academic year.",
    "crumbs": [
      "Library Resources",
      "TOOLS",
      "... Tableau"
    ]
  },
  {
    "objectID": "tableau_research_getting_started.html",
    "href": "tableau_research_getting_started.html",
    "title": "Lesson 1. Getting Started",
    "section": "",
    "text": "This lesson provides an introduction to the Tableau workspace and explores effective strategies for finding and critically assessing publicly available data. Readings also offer guidance on understanding audience needs prior to data visualization and outline key principles for creating more impactful visual representations.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 1. Getting Started"
    ]
  },
  {
    "objectID": "tableau_research_getting_started.html#data-skills-concepts",
    "href": "tableau_research_getting_started.html#data-skills-concepts",
    "title": "Lesson 1. Getting Started",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nTableau\nFinding data\nReading data",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 1. Getting Started"
    ]
  },
  {
    "objectID": "tableau_research_getting_started.html#learning-objectives",
    "href": "tableau_research_getting_started.html#learning-objectives",
    "title": "Lesson 1. Getting Started",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nApply the DRAMA Framework to critically evaluate a dataset.\nNavigate the Tableau start page, data source page, and workspace.\n\nThis tutorial is designed to support a multi-session Tableau for Research workshop hosted by The Ohio State University Libraries Research Commons. It is intended to help the ABSOLUTE beginner, or anyone who is relatively new to Tableau to build the skills and confidence to apply Tableau to research projects.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 1. Getting Started"
    ]
  },
  {
    "objectID": "tableau_research_getting_started.html#the-drama-framework",
    "href": "tableau_research_getting_started.html#the-drama-framework",
    "title": "Lesson 1. Getting Started",
    "section": "The DRAMA framework",
    "text": "The DRAMA framework\nThe DRAMA Framework is a helpful tool for critically evaluating data sources. (Primeau, n.d.)\n\n\n\n\nDRAMA Framework\n\n\n \n\n\nDate\n\n\nWhen was the data last updated? Is it current? Does it reflect current trends?  \n\n\nRelevance\n\n\nWhat procedures were used to collect the data? Is the data relevant to my research project? Did sampling procedures target the right audience or population? What was the context for collecting the data? Is there a description of the data set and what data it does and does not contain?  \n\n\nAccuracy\n\n\nIs the data reliable? Valid? Were procedures for gathering the data followed consistently?  \n\n\nMotivation\n\n\nWhy was the data collected? Are there any potential biases in the data? Was any relevant data not included in the dataset? If yes, was this disclosed?  \n\n\nAuthority\n\n\nWho collected the data? an individual? a government agency? a business? or a political action committee? Are they credible?",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 1. Getting Started"
    ]
  },
  {
    "objectID": "tableau_research_getting_started.html#the-start-page",
    "href": "tableau_research_getting_started.html#the-start-page",
    "title": "Lesson 1. Getting Started",
    "section": "The Start page",
    "text": "The Start page\nWhen you open Tableau Desktop, you‚Äôll land on the blue Start Page. Here‚Äôs what you‚Äôll find:\n\nConnect (Left Pane): Connect to your data sources. Connections to flat files, such as .xlsx, .csv, and .json documents are listed on the top. Direct connections to tables hosted on servers are listed below.\nCenter Pane: Open recently used workbooks.\nDiscover (Right Pane): Learn more about Tableau.\n\n\nWith each new release, Tableau introduces new features and improvements. The Discover section is a valuable resource for staying informed and seeing how these features may support your ability to work with, analyze, and argue with data.\n\nMake Learning Meaningful!!!üåü\nLet‚Äôs take a moment here to emphasize something important:\nWhen learning data analysis and visualization, it‚Äôs incredibly helpful to work with data that matters to you.\n\nüéµ Love music? Try using a Wikipedia table listing albums or songs by your favorite artist.1\nüçΩÔ∏è Foodie at heart? Explore recipes using the TheMealDB API.\nüèÄ Into sports? Check out the curated Sports Data Sets from The Ohio State University Sports and Society Initiative.\n\nWorking with familiar or interesting data makes the learning process more engaging‚Äîand more fun!\n\n\n\nImportant!\n\n\nAlways review the copyright and terms of use before sourcing data from any website.\n\n\nLimited use of copyrighted materials is allowed under certain conditions for journalism, scholarship, and teaching. Use the Resources for determining fair use to verify your project is within the scope of fair use. Contact University Libraries Copyright Services if you have any questions.",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 1. Getting Started"
    ]
  },
  {
    "objectID": "tableau_research_getting_started.html#the-data-source-page",
    "href": "tableau_research_getting_started.html#the-data-source-page",
    "title": "Lesson 1. Getting Started",
    "section": "The Data Source page",
    "text": "The Data Source page\nTo get started with the Tableau Data Source page, we‚Äôll use the Performers table from the Wikipedia page on Rock and Roll Hall of Fame inductees. The Performers category honors recording artists and bands who have had a significant and lasting impact on the development and legacy of rock and roll.\nThat said, you‚Äôre encouraged to use a dataset that‚Äôs personally meaningful to you! Feel free to substitute the example with your own data as you work through the practice activities in this tutorial.\nTo connect to rock_n_roll_performers.csv in your workshop materials.\nOn Tableau‚Äôs Start Page\n\nGo to the Connect pane on the left.\nSelect Text file.\nNavigate to and open the CSV file.\n\nThis opens the Data Source Page.\n\n\nOn the Data Source page you‚Äôll find:\nThe Connections** pane on the left. Tableau Desktop allows multiple connections, which can be joined or related using common fields. For this tutorial, we will keep things simple and use only the rock_n_roll_performers.csv file.\nThe Canvas in the top center. The canvas displays the rock_n_roll_performers.csv file in a rectangle. Right-click on this rectangle or use the ‚ñº caret to:\n\nRename tables\nJoin and relate tables\nApply pre-filters to your data\n\nThe Metadata Grid in the bottom center. Here the data headers are displayed as rows. This feature is particularly helpful when you connect to a dataset with multiple tables and fields. The metadata grid allows you to:\n\nUnderstand your data structure\nChange data types\nHide unnecessary fields not required for your analysis\n\nThe Data Grid on the bottom right. The data grid shows the first 1,000 rows of data in your data source. In the data grid you can:\n\nPivot data\nCreate groups and bins\nBuild calculated fields\n\n\nDimensions vs.¬†measures - Part 1\n\nWhat is a measure?\nThink of a measure as a variable used for math. Measures represent our quantitative data or units of measure.\n\n\nWhat is a dimension?\nDimensions represent the qualitative data used to segment the measures.¬†\n\n\nDimension or measure?\nDimensions sometimes function as measures. Age, for example, can be used to categorize data, or as a measure in a calculation. Measures sometimes function as dimensions. An identification number, for example, can represent a person, yet consist of all numbers. If the identification number is added sequentially to a database, it may also be used in a calculation.\n\n\n\n\n\n\nFigure¬†1: Dimensions and Measures\n\n\n\n\n\n\nChange data type - Option 1\nThe metadata grid for the rock_n_roll_performers.csv dataset primarily shows dimensions. The Index field represents an identification number and can serve as a dimension or measure. The Year field is currently recognized as a whole number. To change the data type of the Year field:\n\nLocate the YEAR field in the metadata grid.\nRight-click on the number icon before the field name.\nSelect Date & Time from the menu.\n\n\n\n\n\n\n\n\n\n\n\n\nType\nField Name\nPhysical Table\nRemote Field Name\n\n\n\n\n\nNumber (whole)\nIndex\nrock_n_roll_performers.csv\nindex\n\n\n\nDate & Time\nYear\nrock_n_roll_performers.csv\nyear\n\n\n\nString\nImage\nrock_n_roll_performers.csv\nimage\n\n\n\nString\nName\nrock_n_roll_performers.csv\nname\n\n\n\nString\nInducted Members\nrock_n_roll_performers.csv\ninducted_members\n\n\n\nString\nPrior Nominations\nrock_n_roll_performers.csv\nprior_nominations\n\n\n\nString\nInduction Presenter\nrock_n_roll_performers.csv\ninduction_presenter\n\n\n\nString\nArtist\nrock_n_roll_performers.csv\nartist\n\n\n\nString\nImage Url\nrock_n_roll_performers.csv\nimage_url\n\n\n\nString\nArtist Url\nrock_n_roll_performers.csv\nartist_url\n\n\n\n\n\nWide vs.¬†tall data\nSometimes to efficiently and effectively analyze and/or visualize data we must restructure our data from wide to tall format. In Figure¬†1 above, each individual is represented by a single row, with separate columns for name, age, and number of visits. To transform this data into tall format, each individual would have a row for each measure. This structure includes:\n\nA column for the measure name (e.g., ‚ÄúAge‚Äù, ‚ÄúVisits‚Äù)\nA column for the measure values (e.g., 34, 5)\n\n\n\n\n\n\n\nFigure¬†2: Tall data\n\n\n\nFor many Tableau projects, converting data to a tall format can enhance analysis and visualization.\n\n\n\n\nExercise 1. Transform data to a tall format\n\n\n\n\n\nTry transforming the rock_n_roll_performers.csv dataset from wide to tall format using Tableau‚Äôs pivot feature.\n\n\n\nIn the data grid, click to highlight the Year column.\n\n\nHold the Shift key, scroll to the right, and click to highlight the Artist Url column.\n\n\nClick the ‚ñº caret on right side of the Artist Url column and select Pivot.\n\n\nTableau will transform the selected columns from a wide format (many columns) to tall format (fewer columns, more rows). ‚úÖ This is useful for reshaping data to make it easier to analyze or visualize in Tableau.\n\n\nTo undo the pivot, press Ctrl+Z (Windows) or Command+Z (Mac).\n\n\n\n\n\n\n\n\nSolution:",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 1. Getting Started"
    ]
  },
  {
    "objectID": "tableau_research_getting_started.html#the-tableau-workspace",
    "href": "tableau_research_getting_started.html#the-tableau-workspace",
    "title": "Lesson 1. Getting Started",
    "section": "The Tableau Workspace",
    "text": "The Tableau Workspace\nTo access the Tableau workspace, select the Sheet1 tab at the bottom of workbook.\n\nVideo showing the location of elements listed below.\n\n\n\n\nThe Data Pane and the Analytics Pane are located in the Side Bar on the left.\nThe Marks Card is to the right of the Side Bar.\nThe Filters Shelf is above the Marks Card.\nDimensions and measures are placed on the columns and rows Shelves.\nThe data visualization is designed in the View.\n\n\n\nMultiple ways to accomplish a task!!!\n\n\n\n\nThe more you use Tableau, the more you‚Äôll notice there‚Äôs often more than one way to accomplish the same task!!!\n\n\n\n\nDimensions vs.¬†measures - Part 2\nWhen you connect to a data source, Tableau automatically categorizes each field as either a dimension or a measure. In the Data Pane, dimensions appear above the gray line, while measures are listed below it. If Tableau classifies a dimension as a measure, you can easily correct it by dragging the field above the gray line into the dimension area.\n\n\n\n\nExercise 2. Change measure to dimension\n\n\n\n\nDrag the Index field above the gray line to categorize Index as a dimension.\n\n\n\n\n\n\nChange data type - Option 2\nTo change the data type of a field in the Tableau Workspace:\n\nLocate the field in the data pane.\nRight-click on the data type icon before the field name.\nSelect the desired data type from the menu.\n\n\n\nToggle between workspace, data source and start page\n\n\n\n\n\n\n\n\n\n\n\nlocation\nToggles between\n\n\n\n\n\nbottom left\nTableau Workspace and Data Source\n\n\n\ntop left\nData Source and Start Page\n\n\n\ntop left\nStart Page and Data Source\n\n\nSheet 1\nbottom left\nData Source and Tableau Workspace",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 1. Getting Started"
    ]
  },
  {
    "objectID": "tableau_research_getting_started.html#footnotes",
    "href": "tableau_research_getting_started.html#footnotes",
    "title": "Lesson 1. Getting Started",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVisit the Websites and APIs. Lesson 3. Wikipedia tutorial to learn how to extract tables from HTML using pandas.read_html.See the Websites and APIs. Lesson 4. iCite tutorial and Websites and APIs. Lesson 7. Crossref tutorial to learn how to use APIs to gather data.‚Ü©Ô∏é",
    "crumbs": [
      "Tableau Tutorials",
      "TABLEAU FOR RESEARCH",
      "Lesson 1. Getting Started"
    ]
  },
  {
    "objectID": "python_data_visualization.html",
    "href": "python_data_visualization.html",
    "title": "Visualizing Data",
    "section": "",
    "text": "This lesson introduces three of the most popular Python libraries for data visualization: Pandas, Plotly, and Seaborn Each library offers unique capabilities for analyzing and presenting data. You will gain hands-on experience comparing these tools while developing skills to create insightful visualizations like bar charts, line charts, and scatterplots.",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#data-skills-concepts",
    "href": "python_data_visualization.html#data-skills-concepts",
    "title": "Visualizing Data",
    "section": "Data skills | concepts",
    "text": "Data skills | concepts\n\nPandas\nPlotly\nSeaborn",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#learning-objectives",
    "href": "python_data_visualization.html#learning-objectives",
    "title": "Visualizing Data",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nCompare and contrast Pandas, Plotly, and Seaborn for visualizing data in Python.\nFormulate a data-driven question and outline the steps needed to filter, aggregate, and visualize data effectively.\nCreate and customize bar charts to compare categorical data.\nIllustrate trends and patterns over time using line charts.\nExplore relationships between two variables through scatterplots.\n\nThis tutorial is designed to support workshops hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts visit the Python - Mastering the Basics tutorial.",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#bar-chart",
    "href": "python_data_visualization.html#bar-chart",
    "title": "Visualizing Data",
    "section": "üìä Bar Chart",
    "text": "üìä Bar Chart\nLet‚Äôs build a bar chart that highlights the average U.S. peak chart positions for albums by 2025 Rock & Roll Hall of Fame inductees to explore visualizing data with Pandas.1\nThe syntax for building a Basic Pandas Chart is:\nDataFrame.plot(*args, **kwargs)\n\nStep 1. Import libraries\nPandas works alongside matplotlib libraries to visualize data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\n\n\nStep 2. Read in files\nWe‚Äôll use the rock_n_roll_performers.csv table from the Wikipedia page on Rock and Roll Hall of Fame inductees to explore plotting with Pandas. The Performers category honors recording artists and bands who have had a significant and lasting impact on the development and legacy of rock and roll. We‚Äôll also enhance our analysis by linking this dataset with rock_n_roll_studio_albums.csv which contains studio album information of many of the inductees.\n\n.read_csv()\n\nperformers=pd.read_csv('data/rock_n_roll_performers.csv', encoding=\"utf-8\")\n\n# A UnicodeDecodeError occurs after asking Pandas to read in rock_n_roll_studio_albums. Co-pilot suggests trying a difference encoding, like latin1\nstudio_albums=pd.read_csv('data/rock_n_roll_studio_albums.csv', encoding='latin1')\n\n\n\n\nStep 2. Merge datasets\nAfter loading the performers and studio_albums tables using pd.read_csv, we can inspect the column headers using .columns.\n\nperformers.columns\n\nIndex(['index', 'year', 'image', 'name', 'inducted_members',\n       'prior_nominations', 'induction_presenter', 'artist', 'image_url',\n       'artist_url'],\n      dtype='object')\n\n\n\nstudio_albums.columns\n\nIndex(['index', 'album_title', 'artist', 'certification_aria',\n       'certification_aria_status', 'certification_aria_x',\n       'certification_bmvi', 'certification_bmvi_status',\n       'certification_bmvi_x', 'certification_bpi', 'certification_bpi_status',\n       'certification_bpi_x', 'certification_mc', 'certification_mc_status',\n       'certification_mc_x', 'certification_riaa', 'certification_riaa_status',\n       'certification_riaa_x', 'certification_snep',\n       'certification_snep_status', 'certification_snep_x', 'day',\n       'format_4_track', 'format_8_track', 'format_blueray', 'format_box_set',\n       'format_cassette', 'format_cd', 'format_digital_compact_cassette',\n       'format_digital_download', 'format_dvd', 'format_lp',\n       'format_mini_disc', 'format_picture_disc', 'format_reel',\n       'format_streaming', 'format_vhs', 'month', 'peakAUS', 'peakAUT',\n       'peakCAN', 'peakFRA', 'peakGER', 'peakIRE', 'peakITA', 'peakJPN',\n       'peakNLD', 'peakNOR', 'peakNZ', 'peakSPA', 'peakSWE', 'peakSWI',\n       'peakUK', 'peakUS', 'peakUS Country', 'peakUS R&B', 'Record label',\n       'Release date', 'year'],\n      dtype='object')\n\n\nBoth datasets share the columns artist and year, which could be used for merging. However, to avoid confusion after joining, we‚Äôll first rename the header year in the performers dataset to year_inducted.\n\n.rename()\n\nperformers=performers.rename(columns={'year':'year_inducted'})  \nperformers.columns\n\nIndex(['index', 'year_inducted', 'image', 'name', 'inducted_members',\n       'prior_nominations', 'induction_presenter', 'artist', 'image_url',\n       'artist_url'],\n      dtype='object')\n\n\nThen, we‚Äôll merge the two datasets using the shared artist column.\n\nperformers_albums=pd.merge(performers, studio_albums, on='artist')\nperformers_albums.columns\n\nIndex(['index_x', 'year_inducted', 'image', 'name', 'inducted_members',\n       'prior_nominations', 'induction_presenter', 'artist', 'image_url',\n       'artist_url', 'index_y', 'album_title', 'certification_aria',\n       'certification_aria_status', 'certification_aria_x',\n       'certification_bmvi', 'certification_bmvi_status',\n       'certification_bmvi_x', 'certification_bpi', 'certification_bpi_status',\n       'certification_bpi_x', 'certification_mc', 'certification_mc_status',\n       'certification_mc_x', 'certification_riaa', 'certification_riaa_status',\n       'certification_riaa_x', 'certification_snep',\n       'certification_snep_status', 'certification_snep_x', 'day',\n       'format_4_track', 'format_8_track', 'format_blueray', 'format_box_set',\n       'format_cassette', 'format_cd', 'format_digital_compact_cassette',\n       'format_digital_download', 'format_dvd', 'format_lp',\n       'format_mini_disc', 'format_picture_disc', 'format_reel',\n       'format_streaming', 'format_vhs', 'month', 'peakAUS', 'peakAUT',\n       'peakCAN', 'peakFRA', 'peakGER', 'peakIRE', 'peakITA', 'peakJPN',\n       'peakNLD', 'peakNOR', 'peakNZ', 'peakSPA', 'peakSWE', 'peakSWI',\n       'peakUK', 'peakUS', 'peakUS Country', 'peakUS R&B', 'Record label',\n       'Release date', 'year'],\n      dtype='object')\n\n\n\n\n\nStep 3. Create and apply filters\nNow that we‚Äôve merged the inductee and album datasets, we can begin filtering the data to focus on specific trends or groups.\nBefore applying any filters, it‚Äôs important to confirm the data type of the year_inducted column in the performers_albums DataFrame. This ensures we can perform numerical comparisons or sorting without errors.\n\n.dtypes\n\nperformers_albums['year_inducted'].dtypes\n\ndtype('int64')\n\n\nWe can create and apply filter to isolate the 2025 inductees using the year_inducted field.\n\n\nfilter_variable=df[‚Äòcolumn‚Äô]==value\n\n#First create the filter\n_2025_inductees= performers_albums['year_inducted']==2025 \n\n\n\nfiltered_df=df[filter_variable]\n\n#Then apply the filter\nperformers_albums_filtered=performers_albums[_2025_inductees]\nperformers_albums_filtered\n\n\n\n\n\n\n\n\nindex_x\nyear_inducted\nimage\nname\ninducted_members\nprior_nominations\ninduction_presenter\nartist\nimage_url\nartist_url\n...\npeakSWI\npeakUK\npeakUS\npeakUS Country\npeakUS R&B\nRecord label\nRelease date\nyear\nFormatted Release date\nRelease year\n\n\n\n\n4770\n264\n2025\nNaN\nBad Company[193]\nBoz Burrell, Simon Kirke, Mick Ralphs, and Pau...\nFirst nomination\nNaN\nBad Company\n/wiki/File:Bad_Company_-_1976.jpg\n/wiki/Bad_Company\n...\nNaN\n3.0\n1.0\nNaN\nNaN\nIsland, Swan Song\n1974-05-24\n1974\n1974\n1974\n\n\n4771\n264\n2025\nNaN\nBad Company[193]\nBoz Burrell, Simon Kirke, Mick Ralphs, and Pau...\nFirst nomination\nNaN\nBad Company\n/wiki/File:Bad_Company_-_1976.jpg\n/wiki/Bad_Company\n...\nNaN\n3.0\n3.0\nNaN\nNaN\nIsland, Swan Song\n1975-03-28\n1975\n1975\n1975\n\n\n4772\n264\n2025\nNaN\nBad Company[193]\nBoz Burrell, Simon Kirke, Mick Ralphs, and Pau...\nFirst nomination\nNaN\nBad Company\n/wiki/File:Bad_Company_-_1976.jpg\n/wiki/Bad_Company\n...\nNaN\n4.0\n5.0\nNaN\nNaN\nIsland, Swan Song\n1976-01-30\n1976\n1976\n1976\n\n\n4773\n264\n2025\nNaN\nBad Company[193]\nBoz Burrell, Simon Kirke, Mick Ralphs, and Pau...\nFirst nomination\nNaN\nBad Company\n/wiki/File:Bad_Company_-_1976.jpg\n/wiki/Bad_Company\n...\nNaN\n17.0\n15.0\nNaN\nNaN\nIsland, Swan Song\n1977-03-03\n1977\n1977\n1977\n\n\n4774\n264\n2025\nNaN\nBad Company[193]\nBoz Burrell, Simon Kirke, Mick Ralphs, and Pau...\nFirst nomination\nNaN\nBad Company\n/wiki/File:Bad_Company_-_1976.jpg\n/wiki/Bad_Company\n...\nNaN\n10.0\n3.0\nNaN\nNaN\nSwan Song\n1979-03-07\n1979\n1979\n1979\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4846\n270\n2025\nNaN\nThe White Stripes[193]\nJack White and Meg White.[197][198]\n1 (2023)\nNaN\nThe White Stripes\n/wiki/File:Jack_%26_Meg,_The_White_Stripes.jpg\n/wiki/The_White_Stripes\n...\nNaN\n137.0\nNaN\nNaN\nNaN\nSympathy for the Record Industry\n2000-06-20\n2000\n2000\n2000\n\n\n4847\n270\n2025\nNaN\nThe White Stripes[193]\nJack White and Meg White.[197][198]\n1 (2023)\nNaN\nThe White Stripes\n/wiki/File:Jack_%26_Meg,_The_White_Stripes.jpg\n/wiki/The_White_Stripes\n...\nNaN\n55.0\n61.0\nNaN\nNaN\nSympathy for the Record Industry, Third Man Re...\n2001-07-03\n2001\n2001\n2001\n\n\n4848\n270\n2025\nNaN\nThe White Stripes[193]\nJack White and Meg White.[197][198]\n1 (2023)\nNaN\nThe White Stripes\n/wiki/File:Jack_%26_Meg,_The_White_Stripes.jpg\n/wiki/The_White_Stripes\n...\nNaN\n1.0\n6.0\nNaN\nNaN\nV2\n2003-04-01\n2003\n2003\n2003\n\n\n4849\n270\n2025\nNaN\nThe White Stripes[193]\nJack White and Meg White.[197][198]\n1 (2023)\nNaN\nThe White Stripes\n/wiki/File:Jack_%26_Meg,_The_White_Stripes.jpg\n/wiki/The_White_Stripes\n...\nNaN\n3.0\n3.0\nNaN\nNaN\nV2\n2005-06-07\n2005\n2005\n2005\n\n\n4850\n270\n2025\nNaN\nThe White Stripes[193]\nJack White and Meg White.[197][198]\n1 (2023)\nNaN\nThe White Stripes\n/wiki/File:Jack_%26_Meg,_The_White_Stripes.jpg\n/wiki/The_White_Stripes\n...\nNaN\n1.0\n2.0\nNaN\nNaN\nThird Man, Warner Bros.\n2007-06-15\n2007\n2007\n2007\n\n\n\n\n81 rows √ó 70 columns\n\n\n\n\n\n\nStep 4. Aggregate data\nPandas supports a variety of basic summary statistics through build-in methods:\n\n\n\nMethod\nDescription\n\n\n\n\n.count()\nnumber of observations\n\n\n.sum()\nhistogram\n\n\n.mean()\nboxplot\n\n\n.medium()\ndensity plots\n\n\n.min()\narea plots\n\n\n.max()\nscatterplots\n\n\nmode()\nhexagonal bin plots\n\n\nstd()\npie charts\n\n\n\n\n.groupby()\nTo calculate statistics grouped by category‚Äîsuch as average chart positions by artist or year‚Äîwe use the .groupby() method. This allows us to aggregate data based on one or more columns before applying summary functions.\n\nperformers_albums_filtered.groupby('artist')['peakUS'].mean()\n\nartist\nBad Company          41.000000\nChubby Checker       57.333333\nCyndi Lauper         59.100000\nJoe Cocker           62.571429\nOutkast               4.833333\nSoundgarden          31.000000\nThe White Stripes    18.000000\nName: peakUS, dtype: float64\n\n\n\n\n\nStep 5. Plot\nThe last step to build our bar chart is to add the .plot(*args, **kwargs) method with relevant arguments and keyword arguments.\nFirst use the .sort_values(ascending=False) method first to sort the bars in descending order. Then .plot with the keyword arguments:\n\nkind = ‚Äòbar‚Äô\nxlabel = ‚Äô‚Äô (removes the redundant label on the x-axis)\ntitle = ‚ÄòPeak US Chart Position: Rock N Roll Hall of Fame Inductees‚Äô (= newline)\n\n\nperformers_albums_filtered.groupby('artist')['peakUS'].mean().sort_values(ascending=False).plot(kind='bar', xlabel='', title='Average Peak US Chart Position: \\n 2025 Rock N Roll Hall of Fame Inductees')",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#line-chart",
    "href": "python_data_visualization.html#line-chart",
    "title": "Visualizing Data",
    "section": "üìà Line chart",
    "text": "üìà Line chart\nLine charts reveal trends over time and at minimum require a date field and a measure. To create a line chart with Pandas, set the kind = parameter to line.",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#scatterplot",
    "href": "python_data_visualization.html#scatterplot",
    "title": "Visualizing Data",
    "section": "‚ñë Scatterplot",
    "text": "‚ñë Scatterplot\nScatterplots are useful for exploring relationships between two or more numerical variables. In Pandas, you can create a scatterplot using the .plot() method by specifying the x and y keyword arguments.",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#bar-chart-1",
    "href": "python_data_visualization.html#bar-chart-1",
    "title": "Visualizing Data",
    "section": "üìä Bar Chart",
    "text": "üìä Bar Chart\nBuild a Bar Chart showing the maximum U.S. peak chart position for any album released by 2025 Rock & Roll Hall of Fame inductees.\n\nimport plotly.io as pio\npio.renderers.default=\"plotly_mimetype+notebook_connected\" # This statement is needed to display Plotly in html\n\n#We already have a filtered DataFrame for the 2025 inductees. Since the highest chart position is 1, we need to tell Pandas to find the minimum peakUS chart position for each artist.\nagg_performers_albums_filtered=performers_albums_filtered.groupby('artist', as_index=False)['peakUS'].min()\n\n#Now we build our bar chart\nfig=pio.bar(agg_performers_albums_filtered, x='artist', y='peakUS', title=\"Peak US Chart Position: 2025 Rock n Roll Hall of Fame Inductees\", color='artist')\nfig.show()",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#line-chart-1",
    "href": "python_data_visualization.html#line-chart-1",
    "title": "Visualizing Data",
    "section": "üìà Line chart",
    "text": "üìà Line chart\nCreate a line chart that shows the total number of albums released each year by all artists in the performers_albums dataset.\n\n#We already converted 'Release date' to year in the code above. Now we tell Pandas to count the number of occurrences of each 'Year' using the .size() method\n\nagg_for_line_performers_albums=performers_albums.groupby(['Release year'], as_index=False).size()\n\n#Build the chart\nfig2=pio.line(agg_for_line_performers_albums, x='Release year', y='size', title=\"Line Chart\", markers=False)\nfig2.show()",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#scatterplot-1",
    "href": "python_data_visualization.html#scatterplot-1",
    "title": "Visualizing Data",
    "section": "‚ñí Scatterplot",
    "text": "‚ñí Scatterplot\nCreate a scatterplot that visualizes the relationship between the Peak US and Peak UK chart positions for albums released by your favorite artist inducted into the Rock and Roll Hall of Fame.\n\n#We already filtered the DataFrame for our favorite artist. Use this DataFrame to build the chart.\nfig3=pio.scatter(kate_bush, x='peakUS', y='peakUK', color='album_title', title=\"Kate Bush Albums: Peak US vs. UK Chart Positions\")\n\nfig3.show()",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#bar-chart-2",
    "href": "python_data_visualization.html#bar-chart-2",
    "title": "Visualizing Data",
    "section": "üìä Bar Chart",
    "text": "üìä Bar Chart\nBuild a Bar Chart showing the maximum U.S. peak chart position for any album released by 2025 Rock & Roll Hall of Fame inductees.\n\n# INSERT CODE HERE\nimport seaborn as sns\n\n#Create bar chart\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10,6))\nsns.barplot(x='artist', y='peakUS', data=agg_performers_albums_filtered)\n\n# Add title and labels\nplt.title(\"Peak US Chart Position \\n 2025 Rock n Roll Hall of Fame Inductees\")\nplt.xlabel(\"\")\nplt.ylabel(\"Peak US chart position\")\n\nText(0, 0.5, 'Peak US chart position')",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#line-chart-2",
    "href": "python_data_visualization.html#line-chart-2",
    "title": "Visualizing Data",
    "section": "üìà Line chart",
    "text": "üìà Line chart\nCreate a line chart that shows the total number of albums released each year by all artists in the performers_albums dataset.\n\n\n\n\nExercise 3: Seaborn Line Chart\n\n\n\n\n\nUsing the Seaborn library‚Äôs Emphasizing continuity with line plots tutorial with the seaborn.lineplot API documentation, create a line chart that shows the total number of albums released each year by all artists in the performers_albums dataset.\n\n\nFormat the chart to:\n\n\n\nRemove gridlines and borders\n\n\nAdjust the x and y axis labels\n\n\nName the chart\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n#Line chart\n\n#remove gridlines\nsns.set(style=\"white\")\n\n#despine (i.e. remove borders)\nsns.despine(top=True, right=True, left=True, bottom=True)\n\n#Group and count number of release years using Pandas .size() method. Reset the index and name the reset index 'count'\nreleased_albums_by_year=performers_albums.groupby(['Release year']).size().reset_index(name='count')\n\n#Build chart\nsns.lineplot(data=released_albums_by_year, x='Release year', y='count')\n\nplt.title(\"Line Chart\")\nplt.xlabel(\"\")\nplt.ylabel(\"# albums released\")",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#scatterplot-2",
    "href": "python_data_visualization.html#scatterplot-2",
    "title": "Visualizing Data",
    "section": "‚ñí Scatterplot",
    "text": "‚ñí Scatterplot\n\n\n\n\nExercise 4: Seaborn Scatterplot\n\n\n\n\n\nUsing the Seaborn library‚Äôs Visualizing statistical relationships tutorial with the seaborn.scatterplot API documentation, create a scatterplot that visualizes the relationship between the Peak US and Peak UK chart positions for albums released by your favorite artist inducted into the Rock and Roll Hall of Fame.\n\n\nBONUS: reverse the x and y axis.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n#We already filtered the DataFrame for our favorite artist. Use this DataFrame to build the chart.\n\nsns.relplot(data=kate_bush, x='peakUS', y='peakUK', hue='album_title')\n\n\n\n\nCheck out the Controlling figure aesthetics and Choosing color palettes tutorials to learn how to customize theme and fine-tune the appearance of your Seaborn visualizations.",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "python_data_visualization.html#footnotes",
    "href": "python_data_visualization.html#footnotes",
    "title": "Visualizing Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVisit the Websites and APIs. Lesson 3. Wikipedia tutorial to learn how to extract tables from HTML using pandas.read_html.See the Websites and APIs. Lesson 4. iCite tutorial and Websites and APIs. Lesson 7. Crossref tutorial to learn how to use APIs to gather data.‚Ü©Ô∏é",
    "crumbs": [
      "Python Tutorials",
      "DATA VISUALIZATION",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "How do I start visualizing data?",
    "section": "",
    "text": "Not sure where to start? The resources below provide an overview of key data visualization concepts, showcase inspiring examples to spark creativity, explore the theory behind effective visual communication, and offer practical guidance for developing your skills through structured practice.",
    "crumbs": [
      "Library Resources",
      "OVERVIEW",
      "How do I start visualizing data?"
    ]
  },
  {
    "objectID": "overview.html#overview",
    "href": "overview.html#overview",
    "title": "How do I start visualizing data?",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nBETTER DATA VISUALIZATIONS\n\n\n\n\n\n\n\nBetter Data Visualizations: A Guide for Scholars, Researchers, and Wonks\n\n\n\nby Jonathan Schwabish\n\n\nNew York : Columbia University Press, 2021.\n\n\n\n\n\n\n\nEFFECTIVE DATA VISUALIZATION\n\n\n\n\n\n\n\nEffective Data Visualization: The Right Chart for the Right Data\n\n\n\nby Stephanie Evergreen\n\n\nThousand Oaks, California: SAGE Publications, 2020\n\n\n\n\n\n\n\nSTORYTELLING WITH DATA\n\n\n\n\n\n\n\nStorytelling with Data: A Data Visualization Guide for Business Professionals\n\n\n\nby Cole Nussbaumer Knaflic\n\n\nHoboken, New Jersey: Wiley, 2015.\n\n\n\n\n\n\n\nDATA STORY\n\n\n\n\n\n\n\nData Story: Explain Data and Inspire Action Through Story\n\n\n\nby Nancy Duarte\n\n\nOakton, Virginia: Ideapress Publishing, 2019.",
    "crumbs": [
      "Library Resources",
      "OVERVIEW",
      "How do I start visualizing data?"
    ]
  },
  {
    "objectID": "overview.html#inspiration",
    "href": "overview.html#inspiration",
    "title": "How do I start visualizing data?",
    "section": "Inspiration",
    "text": "Inspiration\n\n\n\n\nBOOK OF CIRCLES\n\n\n\n\n\n\n\nThe Book of Circles: Visualizing Spheres of Knowledge\n\n\n\nby Manuel Lima\n\n\nNew York, New York: Princeton Architectural Press, 2017.\n\n\n\n\n\n\n\nBOOK OF TREES\n\n\n\n\n\n\n\nThe Book of Trees: Visualizing Branches of Knowledge\n\n\n\nby Manuel Lima\n\n\nNew York, New York: Princeton Architectural Press, 2014.\n\n\n\n\n\n\n\nBIG BOOK DASHBOARDS\n\n\n\n\n\n\n\nThe Big Book of Dashboards: Visualizing Your Data Using Real-World Business Scenarios\n\n\n\nby Steve Wexler, Jeffrey Shaffer, Andy Cotgreave\n\n\nHoboken, New Jersey : Wiley, 2017.",
    "crumbs": [
      "Library Resources",
      "OVERVIEW",
      "How do I start visualizing data?"
    ]
  },
  {
    "objectID": "overview.html#theory",
    "href": "overview.html#theory",
    "title": "How do I start visualizing data?",
    "section": "Theory",
    "text": "Theory\n\n\n\n\nVISUAL DISPLAY QUANTITATIVE INFORMATION\n\n\n\n\n\n\n\nThe Visual Display of Quantitative Information\n\n\n\nby Edward R. Tufte\n\n\nCheshire, Conn. : Graphic Press, 1983.\n\n\n\n\n\n\n\nBEAUTIFUL EVIDENCE\n\n\n\n\n\n\n\nBeautiful Evidence\n\n\n\nby Edward R. Tufte\n\n\nCheshire, Conn. : Graphic Press, 2006.",
    "crumbs": [
      "Library Resources",
      "OVERVIEW",
      "How do I start visualizing data?"
    ]
  },
  {
    "objectID": "overview.html#practice",
    "href": "overview.html#practice",
    "title": "How do I start visualizing data?",
    "section": "Practice",
    "text": "Practice\n\n\n\n\nSTORYTELLING WITH DATA: LET‚ÄôS PRACTICE!\n\n\n\n\n\n\n\nStorytelling With Data: Let‚Äôs Practice!\n\n\n\nby Cole Nussbaumer Knaflic\n\n\nHoboken, New Jersey : Wiley, 2020.\n\n\n\n\n\n\n\nDATA VISUALIZATION SKETCHBOOK\n\n\n\n\n\n\n\nThe Data Visualization Sketch Book\n\n\n\nby Stephanie D. H. Evergreen\n\n\nThousand Oaks : SAGE Publications, 2019.",
    "crumbs": [
      "Library Resources",
      "OVERVIEW",
      "How do I start visualizing data?"
    ]
  }
]