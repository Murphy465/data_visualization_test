{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc65d9c-d72e-4994-b249-acd1b80c1023",
   "metadata": {},
   "source": [
    "---\n",
    "title: Lesson 1. The Lantern\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-expand: 2\n",
    "    toc-title: CONTENTS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f2a3b-5052-4cbe-bc33-495349e106cd",
   "metadata": {},
   "source": [
    "Web scraping and APIs are popular methods for collecting data from websites. **Webscraping** involves directly parsing a website's HTML, allowing extraction of a wide range of data available on a page. Webscraping, however, introduces complexity to a project, especially if the website's data is not consistently structured. **APIs** provide structured data and detailed documentation for querying the website and filtering results. They are generally easier to use but may come with restrictions, such as limits on the number of requests per day or the number of records you can retrieve. \n",
    "\n",
    "In publication since 1881, [***The Lantern***](https://www.thelantern.com/) is The Ohio State University's award-winning student newspaper. [***The Lantern Digital Archives***](https://osupublicationarchives.osu.edu/) includes all articles, illustrations, and advertisments published in *The Lantern* between 1881 and 2018. \n",
    "\n",
    "<div class=\"alert alert-dismissible alert-primary\">\n",
    "  <button type=\"button\" class=\"btn-close\" data-bs-dismiss=\"alert\" aria-label=\"Close\"></button>\n",
    "  <h4 class=\"alert-heading\"><img src=\"images/star_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Important!</h4><p>The online archive of Ohio State University's student newspaper <strong><em>The Lantern</em></strong> is accessible for research and educational purposes only. <strong>Copyright</strong> and other restrictions may apply. Users must obtain the necessary permissions to reprint, reproduce, or otherwise use this material.</p>\n",
    "</div>\n",
    "\n",
    "This lesson introduces BeautifulSoup, a Python library used to parse XML and HTML documents. We will use BeautifulSoup to extract elements from ***The Lantern's*** XML.\n",
    "\n",
    "## Data skills | concepts\n",
    "\n",
    "- Search parameters\n",
    "- XML\n",
    "- Web scraping\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "1. Identify search parameters and understand how they are inserted into a url.\n",
    "2. Navigate document, element, attribute, and text nodes in a Document Object Model (DOM).\n",
    "3. Extract and store XML elements.\n",
    "\n",
    "This tutorial is designed to support multi-session __[workshops](https://library.osu.edu/events?combine=&tid=All&field_location_code_value=10&sort_bef_combine=field_end_date_value_ASC)__ hosted by The Ohio State University Libraries Research Commons. It assumes you already have a basic understanding of Python, including how to iterate through lists and dictionaries to extract data using a for loop. To learn basic Python concepts see [Python - Mastering the Basics](./python_basics.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e4158-c157-4037-adf8-99b37187f6a9",
   "metadata": {},
   "source": [
    "# LESSON 1\n",
    "\n",
    "# Step 1. Copyright | Terms of Use\n",
    "Before starting any webscraping or API project, you must \n",
    "\n",
    "## Review and understand the terms of use.\n",
    "- [ ] Do the terms of service include any restrictions or guidelines?\n",
    "- [ ] Are permissions/licenses needed to scrape data? If yes, have you obtained these permissions/licenses?\n",
    "- [ ] Is the information publicly available?\n",
    "- [ ] If a database, is the database protected by copyright? Or in the public domain\n",
    "\n",
    "## Fair Use \n",
    "Limited use of copyrighted materials is allowed under certain conditions for journalism, scholarship, and teaching. [Use the Resources for determining fair use](https://library.osu.edu/copyright/fair-use) to verify your project is within the scope of fair use. Contact University Libraries [Copyright Services](https://library.osu.edu/copyright) if you have any questions.\n",
    "\n",
    "## Check for robots.txt directives\n",
    "robots.txt directives limit web-scraping or web-crawling. Look for this file in the root directory of the website by adding /robots.txt to the end of the url. Respect these directives.\n",
    "\n",
    "<div class=\"accordion\" id=\"accordionExercise1\">\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex1-headingOne\"><button class=\"accordion-button fs-3\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex1-collapseOne\" aria-expanded=\"true\" aria-controls=\"ex1-collapseOne\"><img src=\"images/guidepost_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Exercise 1: Examine Copyright | Terms of Use</button></h2><div id=\"ex1-collapseOne\" class=\"accordion-collapse collapse show fs-4\" aria-labelledby=\"ex1-headingOne\" data-bs-parent=\"#accordionExercise1\"> <div class=\"accordion-body fs-4\"><ol><li>Go to the <a href=\"https://osupublicationarchives.osu.edu/\">OSU Publication Archives website</a></li><li>Where is the Copyright Notice for this resource?</li><li>What are the terms of use?</li></ol></div></div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex1-headingTwo\"><button class=\"accordion-button fs-3 collapsed\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex1-collapseTwo\" aria-expanded=\"false\" aria-controls=\"ex1-collapseTwo\"><img src=\"images/magnifying_glass_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Solution:</button></h2><div id=\"ex1-collapseTwo\" class=\"accordion-collapse collapse\" aria-labelledby=\"ex1-headingTwo\" data-bs-parent=\"#accordionExercise1\"> <div class=\"accordion-body\">The Copyright Notice for The Ohio State University's online archive of The Lantern student newspaper can be found on the bottom right of the <a href=\"https://osupublicationarchives.osu.edu/\">OSU Publication Archives website</a>. The <a href=\"https://osupublicationarchives.osu.edu/?a=p&p=terms&e=-------en-20--1--txt-txIN-------#all\">Terms of use</a> are listed separately at the bottom left.\n",
    "  </div>\n",
    "  </div>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-dismissible alert-info\">\n",
    "  <button type=\"button\" class=\"btn-close\" data-bs-dismiss=\"alert\"></button>\n",
    "  <h4 class=\"alert-heading\"><img src=\"images/document_pencil_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Note:</h4>\n",
    "  <p>The Ohio State University provides the online archives of Ohio State's student newspaper <em>The Lantern</em>, the student yearbook <em>The Makio</em>, and alumni magazines for research and educational purposes only. The Terms of Use specify that unauthorized <strong>mass</strong> downloading or scraping into any format is prohibited. For this lesson, please limit your search results to scrape no more than 100 records.</p>\n",
    "</div>\n",
    "\n",
    "# Step 2. Is an API available?\n",
    "APIs can simplify data collection from a website by returning structured data such as JSON or XML. Examples of APIs include:\n",
    "\n",
    "- [PubMed eUtilities](https://www.ncbi.nlm.nih.gov/books/NBK25500/)\n",
    "- [Elsevier APIs](https://dev.elsevier.com/)\n",
    "- [Spotify Web API](https://developer.spotify.com/documentation/web-api)\n",
    "\n",
    "To determine if an API is available, try searching for the name of the website and \"API\" or \"documentation.\" If an API is available, read the terms of use and consider factors like rate limits, costs, and access restrictions. \n",
    "\n",
    "If an API is not available, that's okay. Data collection might be a bit more complex, but always remember to respect copyright and terms of use.\n",
    "\n",
    "<div class=\"card border-primary mb-3 p-1\" >\n",
    "  <div class=\"card-header\" style=\"font-size: 1.8rem;\"><img src=\"images/idea_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Tip:</div>\n",
    "  <div class=\"card-body\"><p>The <a href=\"https://osupublicationarchives.osu.edu/\">OSU Publication Archives website</a> is powered by Veridian software, enabling full-text searching of the Libraries digital collections possible. To output a search results from The Lantern to XML, simply add <code>&amp;f=XML</code> to the end of your url. See the <a href=\"https://demos.veridiansoftware.com/latest/text-correct/web/xml-interface-documentation/VeridianXMLInterface.htm\">Veridian XML Interface</a> documentation to learn more about this service.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "# Step 3. Examine search URL\n",
    "\n",
    "To identify search parameters and insert your parameters into a url, first try searching the [OSU Publication Archives website](https://osupublicationarchives.osu.edu/?a=p&p=home&e=-------en-20--1--txt-txIN-------) for `homecoming parade`. Adjust the filters on the left to show results for the decade `1970-1979`, publication `The Lantern`, and category `Article`. \n",
    "\n",
    "![](images/lantern_homecoming_parade.png){fig-alt=\"Screenshot of search result\"}\n",
    "\n",
    "Once you've set your filters, you should have **51 results**. The first 20 search results are displayed on page 1.\n",
    "\n",
    "Take a look at the search URL. As you've searched for `homecoming parade`, the decade `1970-1979`, publication `The Lantern` and category  `Article`, the Veridian software has constructed a server request and inserted your parameters into the url. \n",
    "\n",
    "![](images/lantern_search_url.png){fig-alt=\"Screenshot of search url showing the position of homecoming+parade, the publication abbreviation LTN, the tyq Article, and the decade 197-\"}\n",
    "\n",
    "Scroll to the bottom and click on page 2 to see search results 21-40. Note that the search result has changed to **r=21**.\n",
    "\n",
    "![](images/search_results_2.png){fig-alt=\"Screenshot of url, r now equals 21\"}\n",
    "\n",
    "Return to page 1 of your search, scroll to the right end of your search URL and add the characters `&f=XML` to the end of the string. By adding `&f=XML` to the request parameters, the server returns structured XML output.\n",
    "\n",
    "![](images/search_results_3.png){fig-alt=\"Screenshot of url with &f=XML added to the end of the url\"}\n",
    "\n",
    "![](images/lantern_xml.png){fig-alt=\"Screenshot of XML output\"}\n",
    "\n",
    "# Step 4. Inspect the elements\n",
    "\n",
    "XML and HTML are tree-structured documents. When you request a search URL, it retrieves an HTML or XML page from a server. The browser then downloads the page into local memory and parses the HTML or XML for display.\n",
    "\n",
    "The [Document Object Model (DOM)](https://en.wikipedia.org/wiki/Document_Object_Model) respresents the overall tree-structure of the XML or HTML document. For example, in the XML document shown in Step 2 above:\n",
    "- `VeridianXMLResponse` represents the document node.\n",
    "- All XML elements within `VeridianXMLResponse` are element nodes. \n",
    "- There is some HTML present in the `SearchResultSnippetHTML` node.\n",
    "- There are no XML attribute nodes, but there are HTML attribute nodes in the `SearchResultSnippetHTML` node.\n",
    "- Text between the XML elements are text nodes.\n",
    "\n",
    "The tree is hierachically structured and each tree branch ends with a node. Each node contains objects, and nodes can be nested within nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce64225",
   "metadata": {},
   "source": [
    "# Step 5. Identify Python libraries for project\n",
    "To gather XML and HTML data from websites and APIs, you'll need several Python libraries. Some libraries handle web server requests and responses, while others parse the retrieved content. Libraries like Pandas and CSV are used to store and output results as .csv files.\n",
    "\n",
    "## requests\n",
    "The [requests](https://requests.readthedocs.io/en/latest/) library retrieves HTML or XML documents from a server and processes the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"https://library.osu.edu\" #INSERT URL HERE\n",
    "response=requests.get(url)\n",
    "text=response.text # This returns the response content as text\n",
    "bytes=response.content  # This returns the response content as bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d881b3d4",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "__[BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/)__ parses HTML and XML documents, helping you search for and extract elements from the DOM. The first argument is the content to be parsed, and the second specifies the parsing library to use.\n",
    "\n",
    "- `html.parser` The default HTML parser\n",
    "- `lxml` a faster parser with more features* \n",
    "- `xml` parses XML\n",
    "- `html5lib` for HTML5 parsing*\n",
    "\n",
    "ðŸ”— Additional keyword arguments (**kwargs) are available. See the [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae428fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"https://library.osu.edu\" #INSERT URL HERE\n",
    "response=requests.get(url).content\n",
    "soup=BeautifulSoup(response, 'xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0515c",
   "metadata": {},
   "source": [
    "Other Python libraries for parsing include:\n",
    "\n",
    "- [lxml.html](https://lxml.de/lxmlhtml.html)\n",
    "- [pyQuery](https://www.pyquery.org/pyquery-core-functions/)\n",
    "- [Selenium](https://www.selenium.dev/documentation/)\n",
    "\n",
    "Each library has its strengths and weaknesses. To learn more about different parsing tools read Anish Chapagain's [Hands-On Web Scraping with Python, 2nd edition](https://search.library.osu.edu/permalink/01OHIOLINK_OSU/rr4vai/alma991085522232508507) \n",
    "\n",
    "*Verify that `lxml` or `html5lib` is installed in your Anaconda environment before using.\n",
    "\n",
    "## csv\n",
    "The __[csv](https://docs.python.org/3/library/csv.html)__ module both writes and reads .csv data.\n",
    "\n",
    "**Sample workflow**\n",
    "\n",
    "1. `import csv`\n",
    "2. Create an empty list named dataset\n",
    "3. Assign .csv headers to a list named columns\n",
    "4. Define the writeto_csv function to write results to a .csv file\n",
    "5. Gather variables\n",
    "6. For each row of data, append a list of variables following the order of the .csv headers to the dataset list.\n",
    "7. Use the writeto_csv function to write results to a .csv file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8240ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#####     STEP 1 - CREATE EMPTY DATASET AND DEFINE CSV HEADINGS     ##### \n",
    "dataSet=[]\n",
    "columns=['name','pet','age','profession'] # for CSV headings\n",
    "\n",
    "#####     STEP 2 - DEFINE FUNCTION TO WRITE RESULTS TO CSV FILE     #####\n",
    "\n",
    "def writeto_csv(data,filename,columns):\n",
    "    with open(filename,'w+',newline='',encoding=\"UTF-8\") as file:\n",
    "        writer = csv.DictWriter(file,fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer = csv.writer(file)\n",
    "        for element in data:\n",
    "            writer.writerows([element])\n",
    "\n",
    "\n",
    "name=\"Stanley\"\n",
    "pet=\"dog\"\n",
    "age=8\n",
    "profession=\"chipmunk control\"\n",
    "\n",
    "dataSet.append([name, pet, age, profession])\n",
    "\n",
    "writeto_csv(dataSet,'data/pets.csv',columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91d235",
   "metadata": {},
   "source": [
    "## time.sleep()\n",
    "Most APIs limit the number of records you can request per second. __[time.sleep( )](https://docs.python.org/3/library/time.html#time.sleep)__ suspends your program for a specified number of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78dae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65623aeb-97d3-4692-8089-1951ee36b7e9",
   "metadata": {},
   "source": [
    "## datetime\n",
    "It is good practice to include a `last_updated` column in any dataset you've created after gathering HTML or XML data. The __[datetime](https://docs.python.org/3/library/datetime.html#datetime.date.today)__ module can be used to identify the date you last ran your Python program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "last_updated=today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edcb8c-ec05-46b9-9d73-a41701581146",
   "metadata": {},
   "source": [
    "# Step 6. Write and test code\n",
    "\n",
    "After examining the copyright and terms of use for the website, determining the availability of an API, examining the search URL,  inspecting the XML elements, and identifying suitable Python libraries for the project, the final step is to write the code. It is recommended to use an interactive development environment (IDE) such as [Visual Studio Code](https://youtu.be/1kKTYsQdaPw?si=JbZb6Byg8iRTSS0u) or [Spyder](https://docs.spyder-ide.org/current/index.html) for this task. IDEs provide quick feedback as you iteratively create your code, explore your data, and more. \n",
    "\n",
    "<div class=\"accordion\" id=\"accordionExercise2\">\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex-2headingOne\"><button class=\"accordion-button fs-3\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex2-collapseOne\" aria-expanded=\"true\" aria-controls=\"ex2-collapseOne\"><img src=\"images/guidepost_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Exercise 2: Retrieve XML</button></h2><div id=\"ex2-collapseOne\" class=\"accordion-collapse collapse show fs-4\" aria-labelledby=\"ex-2headingOne\" data-bs-parent=\"#accordionExercise2\"><div class=\"accordion-body fs-4\"><ol><li>Search the <a href=\"https://osupublicationarchives.osu.edu/\">OSU Publication Archives website</a> for `homecoming parade`.</li><li>Adjust the filters on the left to show results for the decade `1970-1979`, publication `The Lantern`, and category `Article` and output the search results to `&f=XML` to the search url.</li><li>Write a Python script to retrieve the XML output from the server and process the response.</li></ol></div></div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex2-headingTwo\"><button class=\"accordion-button fs-3 collapsed\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex2-collapseTwo\" aria-expanded=\"false\" aria-controls=\"ex2-collapseTwo\"><img src=\"images/magnifying_glass_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Solution:</button></h2><div id=\"ex2-collapseTwo\" class=\"accordion-collapse collapse\" aria-labelledby=\"ex2-headingTwo\" data-bs-parent=\"#accordionExercise2\"> <div class=\"accordion-body\"> \n",
    "\n",
    "```python\n",
    "import requests\n",
    "url=\"https://osupublicationarchives.osu.edu/?a=q&r=1&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\n",
    "response=requests.get(url).content\n",
    "```\n",
    "  </div>\n",
    "  </div>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9f253",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup\n",
    "Various methods and approaches may be used to gather and extract XML elements using BeautifulSoup. First we ask BeautifulSoup to  __[search the tree](https://beautiful-soup-4.readthedocs.io/en/latest/#find-all)__ to find the element nodes we identified in Step 4. Helpful methods include:\n",
    "\n",
    "### .find_all( )\n",
    "Gathers all instances of a tag (i.e. element) and returns a response object. Each instance of the tag is then examined using a for loop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a465e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all(name, attrs, recursive, string, limit, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b5b80",
   "metadata": {},
   "source": [
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"https://osupublicationarchives.osu.edu/?a=q&r=1&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\n",
    "response=requests.get(url).content\n",
    "soup=BeautifulSoup(response, 'xml')\n",
    "logical_sections = soup.find_all(\"LogicalSection\")\n",
    "for each_section in logical_sections:\n",
    "    result_number = each_section.SearchResultNumber.string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484633cf",
   "metadata": {},
   "source": [
    "**Note** that `each_section.SearchResultNumber` in the example above is equivalent to `each_section.find_all(\"SearchResultNumber\")`. Appending `.string` to `each_section.SearchResultNumber` extracts the text between the tags for `SearchResultNumber`. \n",
    "\n",
    "<div class=\"card border-primary mb-3 p-1\" >\n",
    "  <div class=\"card-header\" style=\"font-size: 1.8rem;\"><img src=\"images/idea_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Tip:</div>\n",
    "  <div class=\"card-body\"><img src=\"images/microsoft_copilot_icon.svg\" alt=\"\" aria-hidden=\"true\" style=\"width:48px;height:48px;float:right;margin-left:10px;\"><p>Learning to navigate tags with BeautifulSoup can be difficult at first. Try using Copilot in tandem with <a href=\"https://beautiful-soup-4.readthedocs.io/en/latest\">BeautifulSoup's</a> documentation to help you both identify and understand how to apply useful methods for your project.</p><p><strong>Example:</strong> Ask Copilot what is the difference between <span class=\"text-primary\">tag.string</span> and <span class=\"text-primary\">tag.text</span> in BeautifulSoup.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "### .find( )\n",
    "Gathers the first instance of a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca606b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "find(name, attrs, recursive, string, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98069fda",
   "metadata": {},
   "source": [
    "### .find_next( ) and .find_all_next( )\n",
    "Gathers the following instance of a named tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_next(name, attrs, string, **kwargs)\n",
    "find_all_next(name, attrs, string, limit, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d947e",
   "metadata": {},
   "source": [
    "### attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be822b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name[attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62335c-8915-4024-9c95-c9361bc3da8a",
   "metadata": {},
   "source": [
    "Tags can have any number of __[attributes](https://beautiful-soup-4.readthedocs.io/en/latest/#attributes)__. \n",
    "\n",
    "**Example**:\\\n",
    "`href` is an attribute of an `<a href=\"url\">` tag. To find an `href attribute value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a5cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3570a-a275-4c78-8a2b-88e6e4dd3155",
   "metadata": {},
   "source": [
    "<div class=\"accordion\" id=\"ex3-accordionExercise3\">\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex3-headingOne\"><button class=\"accordion-button fs-3\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex3-collapseOne\" aria-expanded=\"true\" aria-controls=\"ex3-collapseOne\"><img src=\"images/guidepost_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Exercise 3: Gather and extract elements</button></h2><div id=\"ex3-collapseOne\" class=\"accordion-collapse collapse show fs-4\" aria-labelledby=\"ex3-headingOne\" data-bs-parent=\"#ex3-accordionExercise3\"><div class=\"accordion-body fs-4\"><ol><li>1. Examine the XML structure and identify the nodes for the following elements:<ul><li>unique_id</li><li>article_title</li><li>article_type</li><li>document_date</li></ul></li><li>Write a Python script that uses csv and BeautifulSoup to gather and store these elements. Focus on the first 20 search results.</li></ol></div></div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex3-headingTwo\"><button class=\"accordion-button fs-3 collapsed\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex3-collapseTwo\" aria-expanded=\"false\" aria-controls=\"ex3-collapseTwo\"><img src=\"images/magnifying_glass_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Solution:</button></h2><div id=\"ex3-collapseTwo\" class=\"accordion-collapse collapse\" aria-labelledby=\"ex3-headingTwo\" data-bs-parent=\"#ex3-accordionExercise3\"> <div class=\"accordion-body\"> \n",
    "\n",
    "```python\n",
    "# 1. Import libraries\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 2. Create a function to write search results to csv\n",
    "def writeto_csv(data, filename, columns):\n",
    "    with open(filename, 'w+', newline='', encoding=\"UTF-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer = csv.writer(file)\n",
    "        for element in data:\n",
    "            writer.writerows([element])\n",
    "\n",
    "\n",
    "\n",
    "# 3. Define headers for final dataset\n",
    "columns=['result_number','unique_id','artile_title','article_type','document_date']\n",
    "\n",
    "#4. Create empty dataset to store each row of data for csv file\n",
    "dataset = []\n",
    "\n",
    "#5. Retrieve the XML\n",
    "url=\"https://osupublicationarchives.osu.edu/?a=q&r=1&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\n",
    "response=requests.get(url).content\n",
    "\n",
    "#6. Parse the content with Beautiful Soup \n",
    "soup = BeautifulSoup(response, 'xml')    \n",
    "logical_sections = soup.find_all(\"LogicalSection\")\n",
    "for each_section in logical_sections:\n",
    "    result_number = each_section.SearchResultNumber.string\n",
    "    unique_id = each_section.find(\"LogicalSectionID\").string\n",
    "    article_title = each_section.find(\"LogicalSectionTitle\").string\n",
    "    article_type = each_section.find(\"LogicalSectionType\").string\n",
    "    document_date = each_section.find(\"DocumentDate\").string\n",
    "    dataset.append([result_number,unique_id,article_title,article_type,document_date]) #adds a list of variables for each section to the dataset\n",
    "\n",
    "#7. Use the writeto_csv function defined in #2 to create the final .csv file with each row  \n",
    "writeto_csv(dataset,'data/lantern_results.csv',columns)     #uses the function defined in #2 to create the final .csv file with each row  \n",
    "\n",
    "```\n",
    "  </div>\n",
    "  </div>\n",
    "  </div>\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaeac4e",
   "metadata": {},
   "source": [
    "<div class=\"accordion\" id=\"accordionExercise4\">\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex4-headingOne\"><button class=\"accordion-button fs-3\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex4-collapseOne\" aria-expanded=\"true\" aria-controls=\"ex4-collapseOne\"><img src=\"images/guidepost_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Exercise 4: Modify code</button></h2><div id=\"ex4-collapseOne\" class=\"accordion-collapse collapse show fs-4\" aria-labelledby=\"ex4-headingOne\" data-bs-parent=\"#accordionExercise4\"> <div class=\"accordion-body fs-4\">Modify your code to gather search results 1-51. </div></div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex4-headingTwo\"><button class=\"accordion-button fs-3 collapsed\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex4-collapseTwo\" aria-expanded=\"false\" aria-controls=\"ex4-collapseTwo\"><img src=\"images/magnifying_glass_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Solution:</button></h2><div id=\"ex4-collapseTwo\" class=\"accordion-collapse collapse\" aria-labelledby=\"ex4-headingTwo\" data-bs-parent=\"#accordionExercise4\"> <div class=\"accordion-body\">\n",
    "\n",
    "```python\n",
    "# 1. Import libraries\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 2. Create a function to write search results to csv\n",
    "def writeto_csv(data, filename, columns):\n",
    "    with open(filename, 'w+', newline='', encoding=\"UTF-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer = csv.writer(file)\n",
    "        for element in data:\n",
    "            writer.writerows([element])\n",
    "\n",
    "# 3. Define headers for final dataset\n",
    "columns=['result_number','unique_id','article_title','article_type','document_date']\n",
    "\n",
    "#4. Create empty dataset to store each row of data for csv file\n",
    "dataset = []\n",
    "\n",
    "#5. Retrieve the XML\n",
    "## modify the url to increment r= by 20. Move response variable to the for loop below.\n",
    "url1=\"https://osupublicationarchives.osu.edu/?a=q&r=\"\n",
    "url2=\"&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\n",
    "\n",
    "#6. Create a for loop to search each page of results. Use range(start, stop, step) to ask\n",
    "# Python to increment i by 20 for each loop. The first SearchResultNumber (i.e. start) is 1. The last\n",
    "# SearchResultNumber (i.e. stop) is 51.  Construct a new url for the response variable.\n",
    "# Parse the response content with Beautiful Soup.\n",
    "for i in range(1,51,20):\n",
    "    response=requests.get(url1+str(i)+url2).content \n",
    "    soup = BeautifulSoup(response, 'xml')    \n",
    "    logical_section = soup.find_all(\"LogicalSection\")\n",
    "    for each_section in logical_section:\n",
    "        result_number = each_section.SearchResultNumber.string\n",
    "        unique_id = each_section.find(\"LogicalSectionID\").string\n",
    "        article_title = each_section.find(\"LogicalSectionTitle\").string\n",
    "        article_type = each_section.find(\"LogicalSectionType\").string\n",
    "        document_date = each_section.find(\"DocumentDate\").string\n",
    "        dataset.append([result_number,unique_id,article_title,article_type,document_date]) #adds a list of variables for each section to the dataset\n",
    "\n",
    "#7. Use the writeto_csv function defined in #2 to create the final .csv file with each row  \n",
    "writeto_csv(dataset,'data/lantern_results.csv',columns)     #uses the function defined in #2 to create the final .csv file with each row  \n",
    "\n",
    "```\n",
    "\n",
    "  </div>\n",
    "  </div>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2a456-cdf2-429c-9bf8-1873f433e0a7",
   "metadata": {},
   "source": [
    "<div class=\"accordion\" id=\"accordionExercise5\">\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex5-headingOne\"><button class=\"accordion-button fs-3\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex5-collapseOne\" aria-expanded=\"true\" aria-controls=\"ex5-collapseOne\"><img src=\"images/guidepost_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Exercise 5: Gather article text</button></h2><div id=\"ex5-collapseOne\" class=\"accordion-collapse collapse show fs-4\" aria-labelledby=\"ex5-headingOne\" data-bs-parent=\"#accordionExercise5\"><div class=\"accordion-body fs-4\"><p>The default search output provide structured bibliographic metadata for each homecoming parade article. How might you modify your code to gather the publication_text for each article?</p></div></div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"accordion-item\"><h2 class=\"accordion-header\" id=\"ex5-headingTwo\"><button class=\"accordion-button fs-3 collapsed\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#ex5-collapseTwo\" aria-expanded=\"false\" aria-controls=\"ex5-collapseTwo\"><img src=\"images/magnifying_glass_standard_icon.png\" alt=\"\" aria-hidden=\"true\" style=\"height: 3rem; vertical-align: middle; margin-right: 0.5rem;\">Solution:</button></h2><div id=\"ex5-collapseTwo\" class=\"accordion-collapse collapse\" aria-labelledby=\"ex5-headingTwo\" data-bs-parent=\"#accordionExercise5\"> <div class=\"accordion-body\"> \n",
    "\n",
    "```python\n",
    "# 1. Import libraries\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 2. Create a function to write search results to csv\n",
    "def writeto_csv(data, filename, columns):\n",
    "    with open(filename, 'w+', newline='', encoding=\"UTF-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer = csv.writer(file)\n",
    "        for element in data:\n",
    "            writer.writerows([element])\n",
    "\n",
    "# 3. Define headers for final dataset\n",
    "columns=['result_number','unique_id','artile_title','article_type','document_date']\n",
    "\n",
    "#4. Create empty dataset to store each row of data for csv file\n",
    "dataset = []\n",
    "\n",
    "#5. Retrieve the XML\n",
    "## modify the url to increment r= by 20. Move response variable to the for loop below.\n",
    "url1=\"https://osupublicationarchives.osu.edu/?a=q&r=\"\n",
    "url2=\"&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML\"\n",
    "\n",
    "#6. Create a for loop to search each page of results. Use range(start, stop, step) to ask\n",
    "# Python to increment i by 20 for each loop. The first SearchResultNumber (i.e. start) is 1. The last\n",
    "# SearchResultNumber (i.e. stop) is 51.  Construct a new url for the response variable.\n",
    "# Parse the response content with Beautiful Soup.\n",
    "for i in range(1,51,20):\n",
    "    response=requests.get(url1+str(i)+url2).content \n",
    "    soup = BeautifulSoup(response, 'xml')    \n",
    "    logical_section = soup.find_all(\"LogicalSection\")\n",
    "    for each_section in logical_section:\n",
    "        result_number = each_section.SearchResultNumber.string\n",
    "        unique_id = each_section.find(\"LogicalSectionID\").string\n",
    "        article_title = each_section.find(\"LogicalSectionTitle\").string\n",
    "        article_type = each_section.find(\"LogicalSectionType\").string\n",
    "        document_date = each_section.find(\"DocumentDate\").string\n",
    "        dataset.append([result_number,unique_id,article_title,article_type,document_date]) #adds a list of variables for each section to the dataset\n",
    "\n",
    "#7. Use the writeto_csv function defined in #2 to create the final .csv file with each row  \n",
    "writeto_csv(dataset,'data/lantern_results.csv',columns)     #uses the function defined in #2 to create the final .csv file with each row  \n",
    "\n",
    "#8. Go back to your original search HTML by removing &f=XML from your search url and click on article #1: \"Whose homecoming?\" \n",
    "# Note the unique_id LTN19781023-01.2.24 for the article and the position of this unique_id in the search url. Try using the url without \n",
    "# the parameters listed after the unique_id. 'https://osupublicationarchives.osu.edu/?a=d&d=LTN19781023-01.2.24' Append &f=XML to the\n",
    "# end of this url to retrieve XML for this publication from the server. Note that publication_text is present in the node\n",
    "# LogicalSectionTextHTML.\n",
    "\n",
    "# Now use a for loop to iterate through the urls for each unique_id listed in your dataset variable to gather the publication_text. But\n",
    "# first, create a new list of column headers and a new dataset variable to pass into your writeto_csv function. This will allow you to output\n",
    "# your results to a new .csv file.\n",
    "\n",
    "columns2=['unique_id','article_title','article_type','publication_date','publication_text']\n",
    "dataset2=[]\n",
    "\n",
    "#sample_url='https://osupublicationarchives.osu.edu/?a=d&d=LTN19781023-01.2.24&f=XML'\n",
    "base_url='https://osupublicationarchives.osu.edu/?a=d&d='\n",
    "\n",
    "for each_list in dataset:\n",
    "    unique_id=each_list[1]\n",
    "    article_title=each_list[2]\n",
    "    article_type=each_list[3]\n",
    "    publication_date=each_list[4]\n",
    "    url=base_url+unique_id+'&f=XML'\n",
    "    xml=requests.get(url).content\n",
    "    soup=BeautifulSoup(xml,'xml')\n",
    "    publication_text=soup.find(\"LogicalSectionTextHTML\").text\n",
    "\n",
    "    \n",
    "    dataset2.append([unique_id,article_title,article_type,publication_date,publication_text])\n",
    "\n",
    "writeto_csv(dataset2,'data/lantern_text.csv',columns2)\n",
    "```\n",
    "  </div>\n",
    "  </div>\n",
    "  </div>\n",
    "  \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
